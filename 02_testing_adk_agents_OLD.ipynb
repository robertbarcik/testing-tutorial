{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing ADK Agents: From Simple LLMs to Agentic Behavior\n",
    "\n",
    "**Course:** LLM and Agent Testing - Lesson 2  \n",
    "**Domain:** IT Support/Services Company  \n",
    "**Environment:** Google Colab\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Build testable agents using Google's ADK (Agent Development Kit)\n",
    "2. Test agent tool selection (which tools the agent chooses to call)\n",
    "3. Test tool parameters (accuracy of extracted information)\n",
    "4. Test multi-step reasoning (tool call sequences)\n",
    "5. Test edge cases (invalid inputs, ambiguous requests)\n",
    "6. Apply pytest patterns to agent behavior testing\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Context: Why Agent Testing is Different\n",
    "\n",
    "### Quick Recap: Notebook 01\n",
    "\n",
    "In the previous lesson, you learned to:\n",
    "- âœ… Write automated tests with pytest\n",
    "- âœ… Test LLM text outputs for factual correctness\n",
    "- âœ… Validate structured outputs with Pydantic\n",
    "- âœ… Use parameterized tests\n",
    "\n",
    "### What's Different with Agents?\n",
    "\n",
    "**Simple LLM (Notebook 01):**\n",
    "```\n",
    "User: \"What port does SSH use?\"\n",
    "LLM: \"Port 22\"\n",
    "Test: Check if response contains \"22\" âœ…\n",
    "```\n",
    "\n",
    "**Agent with Tools (This Notebook):**\n",
    "```\n",
    "User: \"What's the status of ticket #5678?\"\n",
    "Agent: Thinks... I need to look up this ticket\n",
    "       Calls: lookup_ticket(\"5678\")\n",
    "       Tool returns: {ticket_id: 5678, status: \"In Progress\", ...}\n",
    "       Agent: \"Ticket #5678 is currently In Progress...\"\n",
    "       \n",
    "Test: Did agent call the right tool? âœ…\n",
    "Test: Did agent extract correct ticket ID? âœ…\n",
    "Test: Did agent use the tool results properly? âœ…\n",
    "```\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Simple LLM Testing | Agent Testing |\n",
    "|-------------------|---------------|\n",
    "| Test final text output | Test tool selection & parameters |\n",
    "| Single-step response | Multi-step reasoning |\n",
    "| Stateless | Stateful (tool results affect next steps) |\n",
    "| Straightforward assertions | Test tool call sequences |\n",
    "\n",
    "### What We're Building Today\n",
    "\n",
    "An **IT Support Agent** with these tools:\n",
    "- ðŸŽ« `lookup_ticket(ticket_id)` - Retrieve ticket details\n",
    "- ðŸ“š `search_knowledge_base(query)` - Find help articles\n",
    "- ðŸ” `check_system_status(service)` - Check if systems are up\n",
    "\n",
    "**Testing scenarios:**\n",
    "- User asks about a ticket â†’ Agent calls `lookup_ticket` with correct ID\n",
    "- User has a problem â†’ Agent searches KB for solution\n",
    "- User asks multi-step question â†’ Agent calls multiple tools in order\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q google-adk litellm openai python-dotenv nest-asyncio deprecated google-genai pytest pytest-html pydantic"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport os\nfrom openai import OpenAI\nimport pytest\nfrom pydantic import BaseModel, Field\nimport json\nfrom typing import List, Optional, Dict, Any\nimport nest_asyncio\n\n# Enable nested event loops (required for Colab)\nnest_asyncio.apply()\n\n# Core ADK imports\nfrom google.adk.agents import LlmAgent  # For creating AI agents\nfrom google.adk.runners import Runner  # For executing agent interactions\nfrom google.adk.sessions import InMemorySessionService  # For managing conversation sessions\nfrom google.adk.models.lite_llm import LiteLlm  # For using OpenAI and other LLM providers\n\n# Google AI SDK for content formatting\nfrom google.genai import types\n\nprint(\"âœ… All imports successful!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Key Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API key\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"âœ… API key loaded from Colab secrets\")\n",
    "except:\n",
    "    from getpass import getpass\n",
    "    print(\"ðŸ’¡ To use Colab secrets: Go to ðŸ”‘ (left sidebar) â†’ Add new secret â†’ Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"âŒ ERROR: No API key provided!\")\n",
    "\n",
    "print(\"âœ… Authentication configured!\")\n",
    "\n",
    "# Model configuration\n",
    "OPENAI_MODEL = \"gpt-5-nano\"\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a Testable IT Support Agent\n",
    "\n",
    "Let's build a simple IT support agent with three tools. These tools will return mock data for fast testing.\n",
    "\n",
    "### Understanding ADK Tool Structure\n",
    "\n",
    "An ADK tool consists of:\n",
    "1. **Function** - The actual code that runs\n",
    "2. **Tool definition** - Describes the function to the agent\n",
    "3. **Registration** - Adding the tool to the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 1: Lookup Ticket\n",
    "def lookup_ticket(ticket_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Look up details for a support ticket.\n",
    "    \n",
    "    Args:\n",
    "        ticket_id: The ticket ID to look up (e.g., \"5678\")\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with ticket details\n",
    "    \"\"\"\n",
    "    # Mock data for testing - in real system, would query database\n",
    "    mock_tickets = {\n",
    "        \"5678\": {\n",
    "            \"ticket_id\": \"5678\",\n",
    "            \"status\": \"In Progress\",\n",
    "            \"priority\": \"High\",\n",
    "            \"user\": \"Alice Johnson\",\n",
    "            \"issue\": \"Cannot access email\",\n",
    "            \"assigned_to\": \"Tech Support Team\"\n",
    "        },\n",
    "        \"1234\": {\n",
    "            \"ticket_id\": \"1234\",\n",
    "            \"status\": \"Resolved\",\n",
    "            \"priority\": \"Medium\",\n",
    "            \"user\": \"Bob Smith\",\n",
    "            \"issue\": \"Printer not working\",\n",
    "            \"assigned_to\": \"Hardware Team\"\n",
    "        },\n",
    "        \"9999\": {\n",
    "            \"ticket_id\": \"9999\",\n",
    "            \"status\": \"Open\",\n",
    "            \"priority\": \"Critical\",\n",
    "            \"user\": \"Charlie Brown\",\n",
    "            \"issue\": \"Server down\",\n",
    "            \"assigned_to\": \"Infrastructure Team\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if ticket_id in mock_tickets:\n",
    "        return mock_tickets[ticket_id]\n",
    "    else:\n",
    "        return {\"error\": f\"Ticket {ticket_id} not found\"}\n",
    "\n",
    "\n",
    "# Tool 2: Search Knowledge Base\n",
    "def search_knowledge_base(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Search the IT knowledge base for help articles.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query (e.g., \"how to reset password\")\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with search results\n",
    "    \"\"\"\n",
    "    # Mock knowledge base articles\n",
    "    mock_kb = {\n",
    "        \"password\": [\n",
    "            {\"title\": \"How to Reset Your Password\", \"article_id\": \"KB001\"},\n",
    "            {\"title\": \"Password Requirements\", \"article_id\": \"KB002\"}\n",
    "        ],\n",
    "        \"email\": [\n",
    "            {\"title\": \"Troubleshooting Email Access\", \"article_id\": \"KB010\"},\n",
    "            {\"title\": \"Email Configuration Guide\", \"article_id\": \"KB011\"}\n",
    "        ],\n",
    "        \"vpn\": [\n",
    "            {\"title\": \"VPN Setup Instructions\", \"article_id\": \"KB020\"},\n",
    "            {\"title\": \"VPN Connection Issues\", \"article_id\": \"KB021\"}\n",
    "        ],\n",
    "        \"printer\": [\n",
    "            {\"title\": \"Printer Offline Solutions\", \"article_id\": \"KB030\"},\n",
    "            {\"title\": \"How to Install Printer Drivers\", \"article_id\": \"KB031\"}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    results = []\n",
    "    \n",
    "    for keyword, articles in mock_kb.items():\n",
    "        if keyword in query_lower:\n",
    "            results.extend(articles)\n",
    "    \n",
    "    if results:\n",
    "        return {\"query\": query, \"results\": results, \"count\": len(results)}\n",
    "    else:\n",
    "        return {\"query\": query, \"results\": [], \"count\": 0}\n",
    "\n",
    "\n",
    "# Tool 3: Check System Status\n",
    "def check_system_status(service_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Check the operational status of a service or system.\n",
    "    \n",
    "    Args:\n",
    "        service_name: Name of the service (e.g., \"email\", \"vpn\", \"database\")\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with service status\n",
    "    \"\"\"\n",
    "    # Mock system status\n",
    "    mock_status = {\n",
    "        \"email\": {\"service\": \"email\", \"status\": \"operational\", \"uptime\": \"99.9%\"},\n",
    "        \"vpn\": {\"service\": \"vpn\", \"status\": \"operational\", \"uptime\": \"100%\"},\n",
    "        \"database\": {\"service\": \"database\", \"status\": \"degraded\", \"uptime\": \"95.2%\"},\n",
    "        \"file_server\": {\"service\": \"file_server\", \"status\": \"down\", \"uptime\": \"0%\"},\n",
    "        \"web_portal\": {\"service\": \"web_portal\", \"status\": \"operational\", \"uptime\": \"99.5%\"}\n",
    "    }\n",
    "    \n",
    "    service_lower = service_name.lower().replace(\" \", \"_\")\n",
    "    \n",
    "    if service_lower in mock_status:\n",
    "        return mock_status[service_lower]\n",
    "    else:\n",
    "        return {\"service\": service_name, \"status\": \"unknown\"}\n",
    "\n",
    "\n",
    "print(\"âœ… Tools defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Creating the ADK Agent\n\nNow we'll create an ADK agent and register our tools.\n\n**Note:** In ADK, you can pass Python functions directly as tools. The agent will automatically understand them based on their docstrings and type hints!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create the LiteLlm model instance for OpenAI\n# This allows ADK to work with OpenAI's API\nllm_model = LiteLlm(\n    model=f\"openai/{OPENAI_MODEL}\",  # Format: provider/model\n    api_key=OPENAI_API_KEY\n)\n\n# Create the IT Support Agent using LlmAgent\n# In ADK, custom functions can be passed directly as tools\n# NOTE: Agent name must be a valid Python identifier (no spaces, starts with letter/underscore)\nit_support_agent = LlmAgent(\n    name=\"it_support_agent\",  # Valid identifier: lowercase with underscores\n    model=llm_model,\n    description=\"An IT support agent that helps users with tickets, knowledge base searches, and system status checks\",\n    instruction=\"\"\"You are an IT support agent. Help users with their IT issues by:\n    1. Looking up ticket information when asked about specific tickets\n    2. Searching the knowledge base for solutions to problems\n    3. Checking system status when users report service issues\n    \n    Always use the appropriate tools to get accurate information.\"\"\",\n    tools=[lookup_ticket, search_knowledge_base, check_system_status]\n)\n\nprint(\"âœ… IT Support Agent created!\")\nprint(f\"Agent has {len(it_support_agent.tools)} tools available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function for Testing\n",
    "\n",
    "We need a way to run the agent and capture tool calls for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's first test if our agent can successfully call tools\n# We'll add some debug output to see what's happening\n\nimport asyncio\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nasync def test_agent_with_debug(user_message: str):\n    \"\"\"Test the agent and show detailed debug information.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Testing: {user_message}\")\n    print(f\"{'='*60}\")\n    \n    # Create session service\n    session_service = InMemorySessionService()\n    \n    # Create session\n    session_id = \"debug_session\"\n    user_id = \"debug_user\"\n    \n    await session_service.create_session(\n        app_name=\"it_support_test\",\n        user_id=user_id,\n        session_id=session_id,\n        state={}\n    )\n    \n    # Create runner\n    runner = Runner(\n        app_name=\"it_support_test\",\n        agent=it_support_agent,\n        session_service=session_service\n    )\n    \n    # Format message\n    content = types.Content(\n        role='user',\n        parts=[types.Part(text=user_message)]\n    )\n    \n    # Run and collect all events\n    events = runner.run_async(\n        user_id=user_id,\n        session_id=session_id,\n        new_message=content\n    )\n    \n    print(\"\\nðŸ“Š Events received:\")\n    event_count = 0\n    tool_uses = []\n    final_response = None\n    \n    async for event in events:\n        event_count += 1\n        print(f\"\\nEvent {event_count}:\")\n        print(f\"  Type: {type(event).__name__}\")\n        print(f\"  Has tool_use: {hasattr(event, 'tool_use')}\")\n        print(f\"  Is final: {event.is_final_response() if hasattr(event, 'is_final_response') else 'N/A'}\")\n        \n        # Try to extract tool use information\n        if hasattr(event, 'tool_use') and event.tool_use:\n            print(f\"  âœ… Tool use detected!\")\n            for tool in event.tool_use:\n                print(f\"    - Tool: {tool.name if hasattr(tool, 'name') else tool}\")\n                tool_uses.append(tool)\n        \n        # Get final response\n        if hasattr(event, 'is_final_response') and event.is_final_response():\n            final_response = event.content.parts[0].text\n            print(f\"  âœ… Final response: {final_response[:100]}...\")\n    \n    print(f\"\\nðŸ“ˆ Summary:\")\n    print(f\"  Total events: {event_count}\")\n    print(f\"  Tools called: {len(tool_uses)}\")\n    print(f\"  Final response length: {len(final_response) if final_response else 0} chars\")\n    \n    return {\n        'tool_count': len(tool_uses),\n        'tools': tool_uses,\n        'response': final_response\n    }\n\n# Test with a ticket lookup query\ntry:\n    loop = asyncio.get_running_loop()\n    result = asyncio.run(test_agent_with_debug(\"What's the status of ticket 5678?\"))\nexcept RuntimeError:\n    result = asyncio.run(test_agent_with_debug(\"What's the status of ticket 5678?\"))\n\nprint(f\"\\nðŸŽ¯ Result:\")\nprint(f\"  Tool count: {result['tool_count']}\")\nprint(f\"  Response: {result['response']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Tool Selection\n",
    "\n",
    "The first thing to test: **Does the agent choose the right tool?**\n",
    "\n",
    "This is fundamental - if the agent picks the wrong tool, nothing else matters!\n",
    "\n",
    "### Pattern: Test Tool Selection\n",
    "\n",
    "```python\n",
    "1. Give agent a task that requires a specific tool\n",
    "2. Run the agent\n",
    "3. Assert that the expected tool was called\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile test_agent_tool_selection.py\n# test_agent_tool_selection.py\n# Tests for agent tool selection\n\nimport os\nfrom openai import OpenAI\nimport pytest\nimport nest_asyncio\n\n# Enable nested event loops\nnest_asyncio.apply()\n\n# ADK imports\nfrom google.adk.agents import LlmAgent\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.models.lite_llm import LiteLlm\n\n# Import our tool functions\nfrom tools import lookup_ticket, search_knowledge_base, check_system_status\nfrom helpers import run_agent_and_capture_tools, create_it_support_agent\n\n# Initialize agent (we'll use a fixture in real tests)\nagent = create_it_support_agent()\n\n\ndef test_agent_calls_ticket_lookup_tool():\n    \"\"\"\n    Test: When user asks about a ticket, agent should call lookup_ticket tool.\n    \"\"\"\n    user_message = \"Can you check the status of ticket 5678?\"\n    \n    result = run_agent_and_capture_tools(agent, user_message)\n    \n    # Assert that at least one tool was called\n    assert result['tool_count'] > 0, \"Agent should have called at least one tool\"\n    \n    # Assert that lookup_ticket was called\n    tool_names = [tc['name'] for tc in result['tool_calls']]\n    assert 'lookup_ticket' in tool_names, f\"Expected 'lookup_ticket' to be called, but got: {tool_names}\"\n\n\ndef test_agent_calls_knowledge_base_tool():\n    \"\"\"\n    Test: When user has a problem, agent should search knowledge base.\n    \"\"\"\n    user_message = \"How do I reset my password?\"\n    \n    result = run_agent_and_capture_tools(agent, user_message)\n    \n    # Assert that search_knowledge_base was called\n    tool_names = [tc['name'] for tc in result['tool_calls']]\n    assert 'search_knowledge_base' in tool_names, \\\n        f\"Expected 'search_knowledge_base' to be called, but got: {tool_names}\"\n\n\ndef test_agent_calls_system_status_tool():\n    \"\"\"\n    Test: When user asks about system status, agent should check status.\n    \"\"\"\n    user_message = \"Is the email service working?\"\n    \n    result = run_agent_and_capture_tools(agent, user_message)\n    \n    # Assert that check_system_status was called\n    tool_names = [tc['name'] for tc in result['tool_calls']]\n    assert 'check_system_status' in tool_names, \\\n        f\"Expected 'check_system_status' to be called, but got: {tool_names}\"\n\n\ndef test_agent_chooses_correct_tool_for_ticket_vs_kb():\n    \"\"\"\n    Test: Agent distinguishes between ticket lookup and KB search.\n    \"\"\"\n    # Scenario 1: Ticket lookup\n    result1 = run_agent_and_capture_tools(agent, \"Show me ticket 1234\")\n    tool_names1 = [tc['name'] for tc in result1['tool_calls']]\n    \n    # Scenario 2: KB search\n    result2 = run_agent_and_capture_tools(agent, \"How to fix printer offline error?\")\n    tool_names2 = [tc['name'] for tc in result2['tool_calls']]\n    \n    # Assert correct tool selection\n    assert 'lookup_ticket' in tool_names1, \"Should use lookup_ticket for ticket queries\"\n    assert 'search_knowledge_base' in tool_names2, \"Should use KB search for how-to questions\"\n\n\nprint(\"âœ… Test file created: test_agent_tool_selection.py\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: Testing Behavior, Not Text\n",
    "\n",
    "Notice what we're testing:\n",
    "- âœ… **We test**: Which tool was called\n",
    "- âŒ **We don't test**: The exact text of the response\n",
    "\n",
    "Why? Because:\n",
    "1. Tool calls are **deterministic** (agent logic)\n",
    "2. Text responses are **variable** (natural language)\n",
    "3. Tool calls prove the agent **understood** the task\n",
    "4. Tool calls are what **actually matter** for functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Tool Parameters\n",
    "\n",
    "Choosing the right tool is good. But did the agent extract the correct parameters?\n",
    "\n",
    "**Example:**\n",
    "- User: \"Check ticket 5678\"\n",
    "- Agent calls: `lookup_ticket(\"5678\")` âœ…\n",
    "- Agent calls: `lookup_ticket(\"1234\")` âŒ Wrong ticket!\n",
    "\n",
    "### Pattern: Test Parameter Extraction\n",
    "\n",
    "```python\n",
    "1. Give agent a task with specific information\n",
    "2. Run the agent\n",
    "3. Assert the tool was called with correct parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_agent_parameters.py\n",
    "# test_agent_parameters.py\n",
    "# Tests for agent parameter extraction\n",
    "\n",
    "import os\n",
    "import pytest\n",
    "from helpers import run_agent_and_capture_tools, create_it_support_agent\n",
    "\n",
    "agent = create_it_support_agent()\n",
    "\n",
    "\n",
    "def test_agent_extracts_ticket_id():\n",
    "    \"\"\"\n",
    "    Test: Agent correctly extracts ticket ID from user message.\n",
    "    \"\"\"\n",
    "    user_message = \"What's the status of ticket 5678?\"\n",
    "    \n",
    "    result = run_agent_and_capture_tools(agent, user_message)\n",
    "    \n",
    "    # Find the lookup_ticket call\n",
    "    ticket_calls = [tc for tc in result['tool_calls'] if tc['name'] == 'lookup_ticket']\n",
    "    assert len(ticket_calls) > 0, \"Agent should have called lookup_ticket\"\n",
    "    \n",
    "    # Check the ticket_id parameter\n",
    "    ticket_id = ticket_calls[0]['parameters'].get('ticket_id')\n",
    "    assert ticket_id == \"5678\", f\"Expected ticket_id '5678', but got: {ticket_id}\"\n",
    "\n",
    "\n",
    "def test_agent_extracts_search_query():\n",
    "    \"\"\"\n",
    "    Test: Agent correctly extracts and formats search query.\n",
    "    \"\"\"\n",
    "    user_message = \"I need help with VPN connection issues\"\n",
    "    \n",
    "    result = run_agent_and_capture_tools(agent, user_message)\n",
    "    \n",
    "    # Find the KB search call\n",
    "    kb_calls = [tc for tc in result['tool_calls'] if tc['name'] == 'search_knowledge_base']\n",
    "    assert len(kb_calls) > 0, \"Agent should have called search_knowledge_base\"\n",
    "    \n",
    "    # Check that query contains relevant keywords\n",
    "    query = kb_calls[0]['parameters'].get('query', '').lower()\n",
    "    assert 'vpn' in query, f\"Query should contain 'vpn', but got: {query}\"\n",
    "\n",
    "\n",
    "def test_agent_extracts_service_name():\n",
    "    \"\"\"\n",
    "    Test: Agent correctly identifies service name from user query.\n",
    "    \"\"\"\n",
    "    user_message = \"Is the email service operational right now?\"\n",
    "    \n",
    "    result = run_agent_and_capture_tools(agent, user_message)\n",
    "    \n",
    "    # Find the status check call\n",
    "    status_calls = [tc for tc in result['tool_calls'] if tc['name'] == 'check_system_status']\n",
    "    assert len(status_calls) > 0, \"Agent should have called check_system_status\"\n",
    "    \n",
    "    # Check the service name\n",
    "    service = status_calls[0]['parameters'].get('service_name', '').lower()\n",
    "    assert 'email' in service, f\"Service name should contain 'email', but got: {service}\"\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"ticket_id\", [\"5678\", \"1234\", \"9999\"])\n",
    "def test_agent_extracts_various_ticket_ids(ticket_id):\n",
    "    \"\"\"\n",
    "    Test: Agent correctly extracts different ticket ID formats.\n",
    "    This is a parameterized test - runs once for each ticket_id!\n",
    "    \"\"\"\n",
    "    user_message = f\"Look up ticket {ticket_id}\"\n",
    "    \n",
    "    result = run_agent_and_capture_tools(agent, user_message)\n",
    "    \n",
    "    # Find lookup_ticket call\n",
    "    ticket_calls = [tc for tc in result['tool_calls'] if tc['name'] == 'lookup_ticket']\n",
    "    assert len(ticket_calls) > 0, \"Agent should have called lookup_ticket\"\n",
    "    \n",
    "    # Verify correct ticket ID\n",
    "    extracted_id = ticket_calls[0]['parameters'].get('ticket_id')\n",
    "    assert extracted_id == ticket_id, \\\n",
    "        f\"Expected ticket_id '{ticket_id}', but got: {extracted_id}\"\n",
    "\n",
    "\n",
    "print(\"âœ… Test file created: test_agent_parameters.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Parameter Testing Matters\n",
    "\n",
    "Imagine these scenarios:\n",
    "\n",
    "**Scenario 1: Correct Parameters** âœ…\n",
    "```\n",
    "User: \"Check ticket 5678\"\n",
    "Agent: lookup_ticket(\"5678\") â†’ Returns correct ticket\n",
    "User: Happy! Gets the right information\n",
    "```\n",
    "\n",
    "**Scenario 2: Wrong Parameters** âŒ\n",
    "```\n",
    "User: \"Check ticket 5678\"\n",
    "Agent: lookup_ticket(\"1234\") â†’ Returns wrong ticket\n",
    "User: Confused! Gets incorrect information\n",
    "```\n",
    "\n",
    "**Testing parameters ensures data integrity!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing Multi-Step Reasoning\n",
    "\n",
    "Real-world agent tasks often require multiple steps:\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "User: \"Check ticket 5678 and find solutions for the issue\"\n",
    "\n",
    "Step 1: Agent calls lookup_ticket(\"5678\")\n",
    "        Returns: {issue: \"Cannot access email\"}\n",
    "        \n",
    "Step 2: Agent calls search_knowledge_base(\"email access\")\n",
    "        Returns: [KB articles about email]\n",
    "        \n",
    "Step 3: Agent synthesizes information and responds\n",
    "```\n",
    "\n",
    "### Pattern: Test Tool Call Sequences\n",
    "\n",
    "```python\n",
    "1. Give agent a multi-step task\n",
    "2. Run the agent\n",
    "3. Assert on tool count\n",
    "4. Assert on tool call order\n",
    "5. Assert on parameter correctness across calls\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_agent_multistep.py\n",
    "# test_agent_multistep.py\n",
    "# Tests for multi-step agent reasoning\n",
    "\n",
    "import os\n",
    "import pytest\n",
    "from helpers import run_agent_and_capture_tools, create_it_support_agent\n",
    "\n",
    "agent = create_it_support_agent()\n",
    "\n",
    "\n",
    "def test_agent_multistep_ticket_then_kb():\n",
    "    \"\"\"\n",
    "    Test: Agent performs multi-step reasoning.\n",
    "    1. Looks up ticket to understand the issue\n",
    "    2. Searches KB for solutions\n",
    "    \"\"\"\n",
    "    user_message = \"Check ticket 5678 and help me find solutions for the issue\"\n",
    "    \n",
    "    result = run_agent_and_capture_tools(agent, user_message)\n",
    "    \n",
    "    # Assert multiple tools were called\n",
    "    assert result['tool_count'] >= 2, \\\n",
    "        f\"Expected at least 2 tool calls, but got: {result['tool_count']}\"\n",
    "    \n",
    "    # Get tool names in order\n",
    "    tool_sequence = [tc['name'] for tc in result['tool_calls']]\n",
    "    \n",
    "    # Assert that lookup_ticket was called\n",
    "    assert 'lookup_ticket' in tool_sequence, \"Agent should look up the ticket\"\n",
    "    \n",
    "    # Assert that search_knowledge_base was called\n",
    "    assert 'search_knowledge_base' in tool_sequence, \"Agent should search KB for solutions\"\n",
    "    \n",
    "    # Assert correct order: ticket lookup should come before KB search\n",
    "    ticket_index = tool_sequence.index('lookup_ticket')\n",
    "    kb_index = tool_sequence.index('search_knowledge_base')\n",
    "    assert ticket_index < kb_index, \\\n",
    "        f\"Agent should look up ticket before searching KB, but order was: {tool_sequence}\"\n",
    "\n",
    "\n",
    "def test_agent_multistep_status_then_kb():\n",
    "    \"\"\"\n",
    "    Test: Agent checks system status, then searches for known issues.\n",
    "    \"\"\"\n",
    "    user_message = \"The database seems slow. Check its status and find troubleshooting guides.\"\n",
    "    \n",
    "    result = run_agent_and_capture_tools(agent, user_message)\n",
    "    \n",
    "    # Assert multiple tools were called\n",
    "    assert result['tool_count'] >= 2, \\\n",
    "        f\"Expected at least 2 tool calls for this complex request\"\n",
    "    \n",
    "    tool_names = [tc['name'] for tc in result['tool_calls']]\n",
    "    \n",
    "    # Should check status and search KB\n",
    "    assert 'check_system_status' in tool_names, \"Should check database status\"\n",
    "    assert 'search_knowledge_base' in tool_names, \"Should search for troubleshooting guides\"\n",
    "\n",
    "\n",
    "def test_agent_single_step_when_appropriate():\n",
    "    \"\"\"\n",
    "    Test: Agent uses single tool when that's all that's needed.\n",
    "    Not every query requires multiple steps!\n",
    "    \"\"\"\n",
    "    user_message = \"What's the status of ticket 1234?\"\n",
    "    \n",
    "    result = run_agent_and_capture_tools(agent, user_message)\n",
    "    \n",
    "    # For this simple query, should only need lookup_ticket\n",
    "    assert result['tool_count'] == 1, \\\n",
    "        f\"Simple ticket lookup should use 1 tool, but used: {result['tool_count']}\"\n",
    "    \n",
    "    assert result['tool_calls'][0]['name'] == 'lookup_ticket', \\\n",
    "        \"Should use lookup_ticket for ticket status query\"\n",
    "\n",
    "\n",
    "print(\"âœ… Test file created: test_agent_multistep.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Tool Call Order\n",
    "\n",
    "Why does order matter?\n",
    "\n",
    "**Good Order:**\n",
    "```\n",
    "1. lookup_ticket(\"5678\") â†’ Get issue: \"email access\"\n",
    "2. search_knowledge_base(\"email access\") â†’ Find relevant articles\n",
    "3. Provide informed response\n",
    "```\n",
    "\n",
    "**Bad Order:**\n",
    "```\n",
    "1. search_knowledge_base(\"unknown\") â†’ Generic results\n",
    "2. lookup_ticket(\"5678\") â†’ Too late, already gave bad advice\n",
    "```\n",
    "\n",
    "**Testing ensures logical reasoning flow!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing Edge Cases\n",
    "\n",
    "What happens when things go wrong or are unclear?\n",
    "\n",
    "**Edge cases to test:**\n",
    "1. â“ **Ambiguous requests** - \"Help me with my problem\" (what problem?)\n",
    "2. âŒ **Invalid data** - \"Check ticket XYZ\" (invalid ticket ID)\n",
    "3. ðŸ¤· **No tool needed** - \"What is IT support?\" (general question)\n",
    "4. ðŸ”€ **Multiple interpretations** - \"Check the email\" (ticket or system status?)\n",
    "\n",
    "### Why Test Edge Cases?\n",
    "\n",
    "In production:\n",
    "- Users won't always provide perfect input\n",
    "- Systems may return errors\n",
    "- Requests may be vague or ambiguous\n",
    "\n",
    "**Your agent needs to handle these gracefully!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_agent_edge_cases.py\n",
    "# test_agent_edge_cases.py\n",
    "# Tests for edge cases and error handling\n",
    "\n",
    "import os\n",
    "import pytest\n",
    "from helpers import run_agent_and_capture_tools, create_it_support_agent\n",
    "\n",
    "agent = create_it_support_agent()\n",
    "\n",
    "\n",
    "def test_agent_no_tool_needed():\n",
    "    \"\"\"\n",
    "    Test: Agent handles general questions without calling tools.\n",
    "    \"\"\"\n",
    "    user_message = \"What is IT support?\"\n",
    "    \n",
    "    result = run_agent_and_capture_tools(agent, user_message)\n",
    "    \n",
    "    # For a general question, agent shouldn't need tools\n",
    "    assert result['tool_count'] == 0, \\\n",
    "        f\"General question shouldn't require tools, but {result['tool_count']} tools were called\"\n",
    "    \n",
    "    # Should still provide a response\n",
    "    assert len(result['final_response']) > 0, \"Agent should provide a response\"\n",
    "\n",
    "\n",
    "def test_agent_handles_invalid_ticket_id():\n",
    "    \"\"\"\n",
    "    Test: Agent attempts to look up non-existent ticket.\n",
    "    The tool will return an error, agent should handle it.\n",
    "    \"\"\"\n",
    "    user_message = \"Check ticket 99999999\"\n",
    "    \n",
    "    result = run_agent_and_capture_tools(agent, user_message)\n",
    "    \n",
    "    # Agent should still try to look up the ticket\n",
    "    tool_names = [tc['name'] for tc in result['tool_calls']]\n",
    "    assert 'lookup_ticket' in tool_names, \"Agent should attempt ticket lookup\"\n",
    "    \n",
    "    # The final response should indicate the ticket wasn't found\n",
    "    response_lower = result['final_response'].lower()\n",
    "    assert 'not found' in response_lower or 'doesn\\'t exist' in response_lower or 'invalid' in response_lower, \\\n",
    "        f\"Response should indicate ticket not found, but got: {result['final_response']}\"\n",
    "\n",
    "\n",
    "def test_agent_handles_ambiguous_request():\n",
    "    \"\"\"\n",
    "    Test: Agent handles vague requests.\n",
    "    \"\"\"\n",
    "    user_message = \"I need help\"\n",
    "    \n",
    "    result = run_agent_and_capture_tools(agent, user_message)\n",
    "    \n",
    "    # Agent might call KB search to find general help\n",
    "    # OR might not call any tools and ask for clarification\n",
    "    # Both are acceptable behaviors\n",
    "    \n",
    "    # Check that agent provides SOME response\n",
    "    assert len(result['final_response']) > 0, \"Agent should respond even to vague requests\"\n",
    "    \n",
    "    # If tools were called, they should be relevant\n",
    "    if result['tool_count'] > 0:\n",
    "        tool_names = [tc['name'] for tc in result['tool_calls']]\n",
    "        # Should use KB search (not ticket lookup with no ID)\n",
    "        if 'lookup_ticket' in tool_names:\n",
    "            pytest.fail(\"Agent shouldn't call lookup_ticket without a ticket ID\")\n",
    "\n",
    "\n",
    "def test_agent_extracts_from_noisy_input():\n",
    "    \"\"\"\n",
    "    Test: Agent extracts ticket ID from messy/noisy user input.\n",
    "    \"\"\"\n",
    "    user_message = \"Hey, so like, I was wondering, could you maybe check ticket 5678 for me? Thanks!\"\n",
    "    \n",
    "    result = run_agent_and_capture_tools(agent, user_message)\n",
    "    \n",
    "    # Agent should extract the ticket ID despite the noise\n",
    "    ticket_calls = [tc for tc in result['tool_calls'] if tc['name'] == 'lookup_ticket']\n",
    "    assert len(ticket_calls) > 0, \"Agent should extract ticket ID from noisy input\"\n",
    "    \n",
    "    ticket_id = ticket_calls[0]['parameters'].get('ticket_id')\n",
    "    assert ticket_id == \"5678\", f\"Expected ticket '5678', but got: {ticket_id}\"\n",
    "\n",
    "\n",
    "def test_agent_handles_multiple_ticket_mentions():\n",
    "    \"\"\"\n",
    "    Test: User mentions multiple tickets - agent should handle appropriately.\n",
    "    \"\"\"\n",
    "    user_message = \"Compare tickets 5678 and 1234\"\n",
    "    \n",
    "    result = run_agent_and_capture_tools(agent, user_message)\n",
    "    \n",
    "    # Agent should look up both tickets\n",
    "    ticket_calls = [tc for tc in result['tool_calls'] if tc['name'] == 'lookup_ticket']\n",
    "    assert len(ticket_calls) >= 2, \\\n",
    "        f\"Agent should look up both tickets, but made {len(ticket_calls)} lookup calls\"\n",
    "    \n",
    "    # Check that both ticket IDs were used\n",
    "    ticket_ids = [tc['parameters'].get('ticket_id') for tc in ticket_calls]\n",
    "    assert '5678' in ticket_ids and '1234' in ticket_ids, \\\n",
    "        f\"Agent should look up both ticket IDs, but looked up: {ticket_ids}\"\n",
    "\n",
    "\n",
    "print(\"âœ… Test file created: test_agent_edge_cases.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Case Testing Strategy\n",
    "\n",
    "When testing edge cases, consider:\n",
    "\n",
    "1. **Invalid inputs** - What if data is malformed?\n",
    "2. **Missing information** - What if user doesn't provide required details?\n",
    "3. **Ambiguity** - What if request could mean multiple things?\n",
    "4. **Error conditions** - What if tools fail or return errors?\n",
    "5. **Boundary conditions** - What about extreme values or edge values?\n",
    "\n",
    "**Good agents degrade gracefully, not catastrophically!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running All Agent Tests\n",
    "\n",
    "Let's see all our tests in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# First, let's create the helper modules that our test files import\n# This organizes our code better\n\n# Create tools.py with our tool functions\ntools_code = '''\"\"\"\nIT Support Tools for ADK Agent Testing\nContains mock tool functions for ticket lookup, KB search, and system status checks.\n\"\"\"\n\ndef lookup_ticket(ticket_id: str) -> dict:\n    \"\"\"\n    Look up details for a support ticket.\n    \n    Args:\n        ticket_id: The ticket ID to look up (e.g., \"5678\")\n    \n    Returns:\n        Dictionary with ticket details\n    \"\"\"\n    # Mock data for testing - in real system, would query database\n    mock_tickets = {\n        \"5678\": {\n            \"ticket_id\": \"5678\",\n            \"status\": \"In Progress\",\n            \"priority\": \"High\",\n            \"user\": \"Alice Johnson\",\n            \"issue\": \"Cannot access email\",\n            \"assigned_to\": \"Tech Support Team\"\n        },\n        \"1234\": {\n            \"ticket_id\": \"1234\",\n            \"status\": \"Resolved\",\n            \"priority\": \"Medium\",\n            \"user\": \"Bob Smith\",\n            \"issue\": \"Printer not working\",\n            \"assigned_to\": \"Hardware Team\"\n        },\n        \"9999\": {\n            \"ticket_id\": \"9999\",\n            \"status\": \"Open\",\n            \"priority\": \"Critical\",\n            \"user\": \"Charlie Brown\",\n            \"issue\": \"Server down\",\n            \"assigned_to\": \"Infrastructure Team\"\n        }\n    }\n    \n    if ticket_id in mock_tickets:\n        return mock_tickets[ticket_id]\n    else:\n        return {\"error\": f\"Ticket {ticket_id} not found\"}\n\n\ndef search_knowledge_base(query: str) -> dict:\n    \"\"\"\n    Search the IT knowledge base for help articles.\n    \n    Args:\n        query: Search query (e.g., \"how to reset password\")\n    \n    Returns:\n        Dictionary with search results\n    \"\"\"\n    # Mock knowledge base articles\n    mock_kb = {\n        \"password\": [\n            {\"title\": \"How to Reset Your Password\", \"article_id\": \"KB001\"},\n            {\"title\": \"Password Requirements\", \"article_id\": \"KB002\"}\n        ],\n        \"email\": [\n            {\"title\": \"Troubleshooting Email Access\", \"article_id\": \"KB010\"},\n            {\"title\": \"Email Configuration Guide\", \"article_id\": \"KB011\"}\n        ],\n        \"vpn\": [\n            {\"title\": \"VPN Setup Instructions\", \"article_id\": \"KB020\"},\n            {\"title\": \"VPN Connection Issues\", \"article_id\": \"KB021\"}\n        ],\n        \"printer\": [\n            {\"title\": \"Printer Offline Solutions\", \"article_id\": \"KB030\"},\n            {\"title\": \"How to Install Printer Drivers\", \"article_id\": \"KB031\"}\n        ]\n    }\n    \n    query_lower = query.lower()\n    results = []\n    \n    for keyword, articles in mock_kb.items():\n        if keyword in query_lower:\n            results.extend(articles)\n    \n    if results:\n        return {\"query\": query, \"results\": results, \"count\": len(results)}\n    else:\n        return {\"query\": query, \"results\": [], \"count\": 0}\n\n\ndef check_system_status(service_name: str) -> dict:\n    \"\"\"\n    Check the operational status of a service or system.\n    \n    Args:\n        service_name: Name of the service (e.g., \"email\", \"vpn\", \"database\")\n    \n    Returns:\n        Dictionary with service status\n    \"\"\"\n    # Mock system status\n    mock_status = {\n        \"email\": {\"service\": \"email\", \"status\": \"operational\", \"uptime\": \"99.9%\"},\n        \"vpn\": {\"service\": \"vpn\", \"status\": \"operational\", \"uptime\": \"100%\"},\n        \"database\": {\"service\": \"database\", \"status\": \"degraded\", \"uptime\": \"95.2%\"},\n        \"file_server\": {\"service\": \"file_server\", \"status\": \"down\", \"uptime\": \"0%\"},\n        \"web_portal\": {\"service\": \"web_portal\", \"status\": \"operational\", \"uptime\": \"99.5%\"}\n    }\n    \n    service_lower = service_name.lower().replace(\" \", \"_\")\n    \n    if service_lower in mock_status:\n        return mock_status[service_lower]\n    else:\n        return {\"service\": service_name, \"status\": \"unknown\"}\n'''\n\n# Write tools.py\nwith open('tools.py', 'w') as f:\n    f.write(tools_code)\n\nprint(\"âœ… Created tools.py\")\n\n# Create helpers.py with helper functions\nhelpers_code = '''\"\"\"\nHelper functions for ADK agent testing\n\"\"\"\nimport os\nimport asyncio\nimport nest_asyncio\nfrom typing import Dict, Any\n\nfrom google.adk.agents import LlmAgent\nfrom google.adk.runners import Runner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.models.lite_llm import LiteLlm\nfrom google.genai import types\n\nfrom tools import lookup_ticket, search_knowledge_base, check_system_status\n\n# Enable nested event loops for Colab\nnest_asyncio.apply()\n\n\ndef create_it_support_agent():\n    \"\"\"Create an IT support agent for testing.\"\"\"\n    llm_model = LiteLlm(\n        model=f\"openai/{os.environ.get('OPENAI_MODEL', 'gpt-5-nano')}\",\n        api_key=os.environ.get('OPENAI_API_KEY')\n    )\n    \n    agent = LlmAgent(\n        name=\"it_support_agent\",\n        model=llm_model,\n        description=\"An IT support agent that helps users with tickets, knowledge base searches, and system status checks\",\n        instruction=\"\"\"You are an IT support agent. Help users with their IT issues by:\n        1. Looking up ticket information when asked about specific tickets\n        2. Searching the knowledge base for solutions to problems\n        3. Checking system status when users report service issues\n        \n        Always use the appropriate tools to get accurate information.\"\"\",\n        tools=[lookup_ticket, search_knowledge_base, check_system_status]\n    )\n    \n    return agent\n\n\ndef run_agent_and_capture_tools(agent: LlmAgent, user_message: str, session_id: str = \"test_session\", user_id: str = \"test_user\") -> Dict[str, Any]:\n    \"\"\"\n    Run the agent and capture tool calls for testing.\n    \n    Args:\n        agent: The ADK agent to run\n        user_message: User's message to the agent\n        session_id: Session identifier for this conversation\n        user_id: User identifier\n    \n    Returns:\n        Dictionary with:\n        - final_response: Agent's final text response\n        - tool_calls: List of tool calls made\n        - tool_count: Number of tools called\n    \"\"\"\n    \n    async def run_async():\n        # Create a session service for managing conversation state\n        session_service = InMemorySessionService()\n        \n        # Create the session\n        await session_service.create_session(\n            app_name=\"it_support_test\",\n            user_id=user_id,\n            session_id=session_id,\n            state={}\n        )\n        \n        # Create a runner to execute the agent\n        runner = Runner(\n            app_name=\"it_support_test\",\n            agent=agent,\n            session_service=session_service\n        )\n        \n        # Format the user message in the required Content structure\n        content = types.Content(\n            role='user',\n            parts=[types.Part(text=user_message)]\n        )\n        \n        # Run the agent and collect events\n        events = runner.run_async(\n            user_id=user_id,\n            session_id=session_id,\n            new_message=content\n        )\n        \n        # Extract tool calls and final response from events\n        tool_calls = []\n        final_response = \"\"\n        \n        async for event in events:\n            # Check for tool calls in the event\n            if hasattr(event, 'tool_use') and event.tool_use:\n                for tool_use in event.tool_use:\n                    tool_calls.append({\n                        'name': tool_use.name if hasattr(tool_use, 'name') else str(tool_use),\n                        'parameters': tool_use.input if hasattr(tool_use, 'input') else {}\n                    })\n            \n            # Get the final response\n            if event.is_final_response():\n                final_response = event.content.parts[0].text\n        \n        return {\n            'final_response': final_response,\n            'tool_calls': tool_calls,\n            'tool_count': len(tool_calls)\n        }\n    \n    # Run the async function\n    try:\n        loop = asyncio.get_running_loop()\n        return asyncio.run(run_async())\n    except RuntimeError:\n        return asyncio.run(run_async())\n'''\n\n# Write helpers.py\nwith open('helpers.py', 'w') as f:\n    f.write(helpers_code)\n\nprint(\"âœ… Created helpers.py\")\nprint(\"\\\\nâœ… Helper modules created successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all agent tests\n",
    "!pytest test_agent_tool_selection.py test_agent_parameters.py test_agent_multistep.py test_agent_edge_cases.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Student Exercises ðŸŽ“\n",
    "\n",
    "Now it's your turn! Apply what you've learned about testing agents.\n",
    "\n",
    "### Exercise 1: Add a New Tool and Test It\n",
    "\n",
    "Create a new tool `restart_service(service_name: str)` that simulates restarting an IT service.\n",
    "\n",
    "**Requirements:**\n",
    "1. Write the tool function\n",
    "2. Add it to the agent\n",
    "3. Write 2 tests:\n",
    "   - Test that agent calls restart_service when asked to restart\n",
    "   - Test that agent extracts correct service name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "def restart_service(service_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    TODO: Implement this function\n",
    "    Should return a dict with restart status\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# TODO: Add to agent and write tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Test Tool Call Order\n",
    "\n",
    "Write a test for this scenario:\n",
    "\"Check ticket #1234 and restart the affected service\"\n",
    "\n",
    "**Assert:**\n",
    "1. Agent calls `lookup_ticket` first\n",
    "2. Agent calls `restart_service` second\n",
    "3. Service name matches the ticket's issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your test here\n",
    "\n",
    "def test_ticket_lookup_then_restart():\n",
    "    \"\"\"\n",
    "    TODO: Test multi-step flow\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Test Unclear Ticket Information\n",
    "\n",
    "Users don't always provide clear ticket IDs. Test these cases:\n",
    "- \"Check my ticket\" (no ID provided)\n",
    "- \"Ticket ABC123\" (invalid format)\n",
    "- \"Check tickets 1, 2, and 3\" (multiple tickets)\n",
    "\n",
    "What should the agent do in each case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your tests here\n",
    "\n",
    "def test_no_ticket_id_provided():\n",
    "    \"\"\"TODO: Test agent behavior when no ticket ID given\"\"\"\n",
    "    pass\n",
    "\n",
    "def test_invalid_ticket_format():\n",
    "    \"\"\"TODO: Test agent behavior with invalid ticket format\"\"\"\n",
    "    pass\n",
    "\n",
    "def test_multiple_tickets():\n",
    "    \"\"\"TODO: Test agent handling multiple ticket IDs\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Parameterized Parameter Testing\n",
    "\n",
    "Use `@pytest.mark.parametrize` to test parameter extraction for 5 different ticket IDs in various formats:\n",
    "- \"5678\"\n",
    "- \"#1234\"\n",
    "- \"ticket 9999\"\n",
    "- \"TICKET-5555\"\n",
    "- \"id: 7777\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile exercise_4.py\n",
    "# Exercise 4: Parameterized parameter extraction test\n",
    "\n",
    "import pytest\n",
    "from helpers import run_agent_and_capture_tools, create_it_support_agent\n",
    "\n",
    "agent = create_it_support_agent()\n",
    "\n",
    "# TODO: Write parameterized test\n",
    "# @pytest.mark.parametrize(...)\n",
    "# def test_extract_ticket_id_formats(...):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Create a Failing Test\n",
    "\n",
    "Write a test that you EXPECT to fail. Then explain:\n",
    "1. Why it fails\n",
    "2. Is it a bug in the agent or a test problem?\n",
    "3. How would you fix it?\n",
    "\n",
    "**Example failing scenarios:**\n",
    "- Agent calls wrong tool\n",
    "- Agent extracts wrong parameter\n",
    "- Agent doesn't handle edge case\n",
    "- Test is too strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Your failing test and explanation\n",
    "\n",
    "def test_that_will_fail():\n",
    "    \"\"\"\n",
    "    TODO: Write a test that fails\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# TODO: Write explanation in markdown cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your explanation here:**\n",
    "\n",
    "<!-- \n",
    "TODO: Explain:\n",
    "- What test did you write?\n",
    "- Why does it fail?\n",
    "- Is it agent bug or test bug?\n",
    "- How to fix?\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Build a Mini Test Suite\n",
    "\n",
    "Create a comprehensive test suite (10+ tests) for your IT support agent.\n",
    "\n",
    "**Must include:**\n",
    "- 3 tool selection tests (one per tool)\n",
    "- 3 parameter extraction tests\n",
    "- 2 multi-step tests\n",
    "- 2 edge case tests\n",
    "- At least 1 parameterized test\n",
    "\n",
    "**Bonus points for:**\n",
    "- Clear test names\n",
    "- Good assertions with helpful messages\n",
    "- Testing realistic scenarios\n",
    "- Well-organized test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile exercise_6_test_suite.py\n",
    "# Exercise 6: Comprehensive test suite\n",
    "\n",
    "import os\n",
    "import pytest\n",
    "from helpers import run_agent_and_capture_tools, create_it_support_agent\n",
    "\n",
    "agent = create_it_support_agent()\n",
    "\n",
    "# TODO: Write your 10+ tests here\n",
    "# Organize them into sections:\n",
    "# - Tool Selection Tests\n",
    "# - Parameter Extraction Tests\n",
    "# - Multi-Step Tests\n",
    "# - Edge Case Tests\n",
    "\n",
    "# Example structure:\n",
    "# def test_1_tool_selection_ticket():\n",
    "#     pass\n",
    "#\n",
    "# def test_2_tool_selection_kb():\n",
    "#     pass\n",
    "# \n",
    "# ... etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your test suite\n",
    "!pytest exercise_6_test_suite.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices for Agent Testing\n",
    "\n",
    "### âœ… DO:\n",
    "\n",
    "1. **Test behavior, not text** - Focus on tool calls and parameters\n",
    "2. **Test tool selection first** - Ensure agent picks the right tool\n",
    "3. **Test parameter accuracy** - Verify extracted information is correct\n",
    "4. **Test tool call sequences** - Multi-step reasoning matters\n",
    "5. **Test edge cases** - Invalid input, missing data, ambiguity\n",
    "6. **Use mock data** - Fast tests with fake/mock tool responses\n",
    "7. **Test happy path AND failures** - Both success and error cases\n",
    "8. **Write descriptive test names** - `test_agent_extracts_ticket_id()` not `test_1()`\n",
    "\n",
    "### âŒ DON'T:\n",
    "\n",
    "1. **Don't test only final text** - Tool calls are more important\n",
    "2. **Don't expect exact tool sequences** - Some variation is OK\n",
    "3. **Don't skip edge cases** - That's where bugs hide\n",
    "4. **Don't use real external services** - Slow and unreliable\n",
    "5. **Don't test too many things in one test** - Keep tests focused\n",
    "\n",
    "### Testing Hierarchy\n",
    "\n",
    "**Priority 1: Critical Functionality**\n",
    "- Does agent call the right tool?\n",
    "- Does agent extract correct parameters?\n",
    "\n",
    "**Priority 2: Complex Behavior**\n",
    "- Multi-step reasoning\n",
    "- Tool call ordering\n",
    "\n",
    "**Priority 3: Edge Cases**\n",
    "- Invalid inputs\n",
    "- Error handling\n",
    "- Ambiguous requests\n",
    "\n",
    "**Priority 4: Output Quality**\n",
    "- Response helpfulness\n",
    "- Text clarity\n",
    "- (This is for next lesson: LLM-as-judge!)\n",
    "\n",
    "### Agent Testing vs LLM Testing\n",
    "\n",
    "| Aspect | LLM Testing (Notebook 01) | Agent Testing (This Notebook) |\n",
    "|--------|---------------------------|-------------------------------|\n",
    "| **What to test** | Text outputs | Tool calls & parameters |\n",
    "| **Assertions** | String contains, JSON structure | Tool names, parameter values |\n",
    "| **Complexity** | Single response | Multi-step sequences |\n",
    "| **Determinism** | Medium (LLM variability) | High (tool logic) |\n",
    "| **Focus** | Content correctness | Behavior correctness |\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Use mock data** - Don't hit real APIs in tests\n",
    "2. **Parallel test execution** - pytest can run tests in parallel\n",
    "3. **Cache agent initialization** - Use pytest fixtures\n",
    "4. **Test at the right level** - Don't test LLM internals\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "**Pitfall 1: Over-testing text**\n",
    "```python\n",
    "âŒ assert \"The ticket status is In Progress\" == response\n",
    "âœ… assert 'lookup_ticket' in tool_calls\n",
    "```\n",
    "\n",
    "**Pitfall 2: Brittle sequence assertions**\n",
    "```python\n",
    "âŒ assert tool_sequence == ['tool1', 'tool2', 'tool3']  # Too strict\n",
    "âœ… assert 'tool1' in tool_sequence and 'tool2' in tool_sequence  # Flexible\n",
    "```\n",
    "\n",
    "**Pitfall 3: Not testing edge cases**\n",
    "```python\n",
    "âŒ Only test: \"Check ticket 5678\"\n",
    "âœ… Also test: \"Check ticket XYZ\", \"Check ticket\", \"Check tickets 1, 2, 3\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways & Next Steps\n",
    "\n",
    "### ðŸŽ‰ What You've Learned\n",
    "\n",
    "1. **Agents are different** - Test behavior (tool calls) not just text\n",
    "2. **Tool selection matters** - Right tool = right functionality\n",
    "3. **Parameters must be accurate** - Wrong parameters = wrong results\n",
    "4. **Multi-step reasoning is testable** - Assert on sequences and order\n",
    "5. **Edge cases reveal bugs** - Test invalid, ambiguous, and error cases\n",
    "6. **ADK makes testing easier** - Structured tool calls are inspectable\n",
    "\n",
    "### ðŸš€ What You Can Do Now\n",
    "\n",
    "- âœ… Build testable agents with ADK\n",
    "- âœ… Test tool selection and parameters\n",
    "- âœ… Test multi-step agent reasoning\n",
    "- âœ… Test edge cases and error handling\n",
    "- âœ… Apply pytest patterns to agent testing\n",
    "- âœ… Distinguish between agent testing and LLM testing\n",
    "\n",
    "### ðŸ“š Testing Levels Completed\n",
    "\n",
    "**âœ… Lesson 1:** Simple LLM Testing\n",
    "- Text outputs\n",
    "- Factual correctness\n",
    "- Structured output validation\n",
    "\n",
    "**âœ… Lesson 2:** Agent Testing (This Lesson)\n",
    "- Tool selection\n",
    "- Parameter extraction\n",
    "- Multi-step reasoning\n",
    "- Edge cases\n",
    "\n",
    "**ðŸ”œ Lesson 3:** Advanced Testing (Coming Next)\n",
    "- LLM-as-judge for subjective quality\n",
    "- Testing response helpfulness\n",
    "- Testing conversation flow\n",
    "- Integration testing\n",
    "\n",
    "### ðŸ’¡ Real-World Application\n",
    "\n",
    "You've learned skills applicable to:\n",
    "- ðŸŽ« Customer support agents\n",
    "- ðŸ“Š Data analysis agents\n",
    "- ðŸ”§ DevOps automation agents\n",
    "- ðŸ“ Content generation agents\n",
    "- ðŸ” Research and retrieval agents\n",
    "\n",
    "### ðŸ¤” Discussion Questions\n",
    "\n",
    "1. When should you test tool calls vs final text output?\n",
    "2. How do you decide if agent behavior is \"correct\" for ambiguous requests?\n",
    "3. What's the tradeoff between strict and flexible test assertions?\n",
    "4. How would you test an agent that uses 10+ tools?\n",
    "5. Should you test tool call order strictly or flexibly? When does each make sense?\n",
    "\n",
    "### ðŸ› ï¸ Next Steps\n",
    "\n",
    "1. **Complete all exercises** - Hands-on practice is essential\n",
    "2. **Build your own agent** - Apply these patterns to a new domain\n",
    "3. **Expand your test suite** - Aim for 80%+ coverage of behaviors\n",
    "4. **Test in production scenarios** - Use real user queries as test cases\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Additional Resources\n",
    "\n",
    "- [Google ADK Documentation](https://ai.google.dev/adk)\n",
    "- [Pytest Documentation](https://docs.pytest.org/)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/)\n",
    "- [Agent Testing Best Practices](https://example.com/agent-testing)\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work! You've completed Lesson 2: Testing ADK Agents** ðŸŽ“\n",
    "\n",
    "You can now test both simple LLMs and complex agents with tools. Next, we'll learn advanced techniques for testing subjective quality using LLM-as-judge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}