{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0nLDqRq_g6Y"
   },
   "source": [
    "# Building a Complete Evaluation Pipeline\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Design structured evaluation datasets for IT support agents\n",
    "2. Build a reusable `AgentEvaluator` class\n",
    "3. Combine multiple test types (assertions, tool calls, LLM judges)\n",
    "4. Generate comprehensive evaluation reports\n",
    "5. Visualize evaluation results\n",
    "6. Use evaluation for iterative agent improvement\n",
    "7. Apply production-ready evaluation patterns\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Context: Why Build an Evaluation Pipeline?\n",
    "\n",
    "### Quick Recap: What You've Learned\n",
    "\n",
    "**Notebook 01:** Pytest basics\n",
    "- ‚úÖ Automated testing with pytest\n",
    "- ‚úÖ Assertion-based tests for factual correctness\n",
    "- ‚úÖ Parameterized tests\n",
    "\n",
    "**Notebook 02:** Agent testing\n",
    "- ‚úÖ Testing tool selection and parameters\n",
    "- ‚úÖ Multi-step reasoning validation\n",
    "- ‚úÖ Edge case handling\n",
    "\n",
    "**Notebook 03:** LLM-as-judge\n",
    "- ‚úÖ Evaluating subjective quality\n",
    "- ‚úÖ Multi-dimension evaluation\n",
    "- ‚úÖ Batch evaluation\n",
    "\n",
    "### The Problem: Testing at Scale\n",
    "\n",
    "You now know THREE testing approaches... but:\n",
    "\n",
    "‚ùì **How do you test 100+ different IT support scenarios?**  \n",
    "‚ùì **How do you track quality over time as you improve your agent?**  \n",
    "‚ùì **How do you prove your agent got better after changes?**  \n",
    "‚ùì **How do you organize different test types efficiently?**\n",
    "\n",
    "### The Solution: Evaluation Pipeline\n",
    "\n",
    "**An evaluation pipeline is a systematic, reusable framework for testing at scale.**\n",
    "\n",
    "```\n",
    "Evaluation Dataset ‚Üí AgentEvaluator ‚Üí Report + Visualizations\n",
    "     (Test Cases)      (Runs Tests)    (Shows Results)\n",
    "```\n",
    "\n",
    "**Key components:**\n",
    "1. **Structured dataset** - Test cases in consistent format\n",
    "2. **Unified evaluator** - Runs all test types\n",
    "3. **Comprehensive reports** - Metrics, failures, insights\n",
    "4. **Visualizations** - Charts to understand performance\n",
    "5. **Iteration workflow** - Compare before/after improvements\n",
    "\n",
    "### Today's Focus: IT Support Agent Evaluation\n",
    "\n",
    "We'll build a complete evaluation pipeline for an **IT Support Helpdesk Agent**:\n",
    "\n",
    "**What we'll test:**\n",
    "- üìö Technical knowledge (assertions)\n",
    "- üîß Tool usage (tool call validation)\n",
    "- üí¨ Response quality (LLM judges)\n",
    "- üéØ Helpfulness and professionalism\n",
    "\n",
    "**What we'll build:**\n",
    "- Reusable `AgentEvaluator` class\n",
    "- 10-case evaluation dataset\n",
    "- Automated report generation\n",
    "- Visualization dashboard\n",
    "- Improvement tracking system\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivgQsRqw_g6a"
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmgZ-cW3_g6a"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai pydantic>=2.11.0 pandas matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLZHs5aZ_g6b"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "# OpenAI and Pydantic\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data analysis and visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcpV9cuG_g6b"
   },
   "source": [
    "### API Key Setup\n",
    "\n",
    "Configure your OpenAI API key for the evaluation pipeline.\n",
    "\n",
    "**üí∞ Cost Note:** We'll use `gpt-5-nano` model. Running the full evaluation pipeline will cost approximately ‚Ç¨0.10-0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhGv3KEg_g6b"
   },
   "outputs": [],
   "source": [
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
    "print(f\"ü§ñ Selected Model: {OPENAI_MODEL}\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvYsfTGM_g6c"
   },
   "source": [
    "## 2. Evaluation Dataset Structure\n",
    "\n",
    "An evaluation dataset is a collection of test cases in a structured format.\n",
    "\n",
    "### Test Case Format\n",
    "\n",
    "Each test case is a dictionary with:\n",
    "- **id**: Unique identifier (e.g., \"TC001\")\n",
    "- **input**: User query or scenario\n",
    "- **test_type**: One of `assertion`, `tool_call`, or `llm_judge`\n",
    "- **expected**: Expected outcome (format depends on test_type)\n",
    "- **criteria**: Evaluation criteria (mainly for llm_judge)\n",
    "- **description**: Human-readable description\n",
    "\n",
    "### Why This Structure?\n",
    "\n",
    "‚úÖ **Consistent format** - Easy to process programmatically  \n",
    "‚úÖ **Flexible** - Supports different test types  \n",
    "‚úÖ **Maintainable** - Easy to add/modify test cases  \n",
    "‚úÖ **Version control friendly** - Can track changes over time  \n",
    "‚úÖ **Scalable** - Grows from 10 to 1000+ test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpDOV1Sd_g6c"
   },
   "outputs": [],
   "source": [
    "# Create evaluation dataset for IT support agent\n",
    "EVALUATION_DATASET = [\n",
    "    # ========================================\n",
    "    # ASSERTION-BASED TESTS (Technical Knowledge)\n",
    "    # ========================================\n",
    "    {\n",
    "        \"id\": \"TC001\",\n",
    "        \"description\": \"Technical knowledge: SSH port\",\n",
    "        \"input\": \"What port does SSH use? Answer with just the number.\",\n",
    "        \"test_type\": \"assertion\",\n",
    "        \"expected\": {\"contains\": \"22\"},\n",
    "        \"criteria\": None\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC002\",\n",
    "        \"description\": \"Technical knowledge: HTTPS port\",\n",
    "        \"input\": \"What port does HTTPS use? Answer with just the number.\",\n",
    "        \"test_type\": \"assertion\",\n",
    "        \"expected\": {\"contains\": \"443\"},\n",
    "        \"criteria\": None\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC003\",\n",
    "        \"description\": \"Technical knowledge: DNS definition\",\n",
    "        \"input\": \"What does DNS stand for? Answer in one sentence.\",\n",
    "        \"test_type\": \"assertion\",\n",
    "        \"expected\": {\"contains\": [\"domain\", \"name\", \"system\"]},\n",
    "        \"criteria\": None\n",
    "    },\n",
    "\n",
    "    # ========================================\n",
    "    # TOOL CALL TESTS (Agent Behavior)\n",
    "    # ========================================\n",
    "    {\n",
    "        \"id\": \"TC004\",\n",
    "        \"description\": \"Tool call: Ticket lookup\",\n",
    "        \"input\": \"Can you check the status of ticket #5678?\",\n",
    "        \"test_type\": \"tool_call\",\n",
    "        \"expected\": {\n",
    "            \"tool_name\": \"lookup_ticket\",\n",
    "            \"parameter\": {\"key\": \"ticket_id\", \"value\": \"5678\"}\n",
    "        },\n",
    "        \"criteria\": None\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC005\",\n",
    "        \"description\": \"Tool call: Knowledge base search\",\n",
    "        \"input\": \"How do I reset my password?\",\n",
    "        \"test_type\": \"tool_call\",\n",
    "        \"expected\": {\n",
    "            \"tool_name\": \"search_knowledge_base\",\n",
    "            \"parameter\": {\"key\": \"query\", \"contains\": \"password\"}\n",
    "        },\n",
    "        \"criteria\": None\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC006\",\n",
    "        \"description\": \"Tool call: System status check\",\n",
    "        \"input\": \"Is the email service working?\",\n",
    "        \"test_type\": \"tool_call\",\n",
    "        \"expected\": {\n",
    "            \"tool_name\": \"check_system_status\",\n",
    "            \"parameter\": {\"key\": \"service_name\", \"contains\": \"email\"}\n",
    "        },\n",
    "        \"criteria\": None\n",
    "    },\n",
    "\n",
    "    # ========================================\n",
    "    # LLM-AS-JUDGE TESTS (Response Quality)\n",
    "    # ========================================\n",
    "    {\n",
    "        \"id\": \"TC007\",\n",
    "        \"description\": \"Judge: Helpfulness of troubleshooting advice\",\n",
    "        \"input\": \"My computer won't start. What should I do?\",\n",
    "        \"test_type\": \"llm_judge\",\n",
    "        \"expected\": {\"min_score\": 3},\n",
    "        \"criteria\": \"Helpfulness: Does the response provide actionable troubleshooting steps?\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC008\",\n",
    "        \"description\": \"Judge: Clarity of technical explanation\",\n",
    "        \"input\": \"What's the difference between RAM and storage?\",\n",
    "        \"test_type\": \"llm_judge\",\n",
    "        \"expected\": {\"min_score\": 3},\n",
    "        \"criteria\": \"Clarity: Is the explanation clear and easy to understand for a non-technical user?\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC009\",\n",
    "        \"description\": \"Judge: Professionalism and tone\",\n",
    "        \"input\": \"I've been waiting for 2 hours and nobody helped me!\",\n",
    "        \"test_type\": \"llm_judge\",\n",
    "        \"expected\": {\"min_score\": 4},\n",
    "        \"criteria\": \"Professionalism: Does the response handle the frustrated customer professionally and empathetically?\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC010\",\n",
    "        \"description\": \"Judge: Completeness of response\",\n",
    "        \"input\": \"How do I set up VPN on my work laptop?\",\n",
    "        \"test_type\": \"llm_judge\",\n",
    "        \"expected\": {\"min_score\": 3},\n",
    "        \"criteria\": \"Completeness: Does the response provide comprehensive instructions or indicate where to find them?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Evaluation dataset created with {len(EVALUATION_DATASET)} test cases\")\n",
    "print(f\"\\nüìä Test type breakdown:\")\n",
    "print(f\"  - Assertion tests: {sum(1 for tc in EVALUATION_DATASET if tc['test_type'] == 'assertion')}\")\n",
    "print(f\"  - Tool call tests: {sum(1 for tc in EVALUATION_DATASET if tc['test_type'] == 'tool_call')}\")\n",
    "print(f\"  - LLM judge tests: {sum(1 for tc in EVALUATION_DATASET if tc['test_type'] == 'llm_judge')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCLwpRPr_g6c"
   },
   "source": [
    "### View Sample Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4LwlO0q_g6d"
   },
   "outputs": [],
   "source": [
    "# Display one example of each test type\n",
    "print(\"üìã EXAMPLE TEST CASES:\\n\")\n",
    "\n",
    "for test_type in [\"assertion\", \"tool_call\", \"llm_judge\"]:\n",
    "    example = next(tc for tc in EVALUATION_DATASET if tc['test_type'] == test_type)\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test Type: {test_type.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ID: {example['id']}\")\n",
    "    print(f\"Description: {example['description']}\")\n",
    "    print(f\"Input: {example['input']}\")\n",
    "    print(f\"Expected: {example['expected']}\")\n",
    "    if example['criteria']:\n",
    "        print(f\"Criteria: {example['criteria']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uppI-3o_g6d"
   },
   "source": [
    "## 3. Building the AgentEvaluator Class\n",
    "\n",
    "The `AgentEvaluator` class is the heart of our evaluation pipeline.\n",
    "\n",
    "### Class Design\n",
    "\n",
    "**Core methods:**\n",
    "- `__init__(self, agent_function)` - Initialize with agent to test\n",
    "- `evaluate_case(self, test_case)` - Evaluate single test case\n",
    "- `evaluate_all(self, dataset)` - Evaluate entire dataset\n",
    "- `generate_report(self)` - Create comprehensive report\n",
    "\n",
    "**Handles three test types:**\n",
    "1. **Assertion** - String contains checks\n",
    "2. **Tool call** - Mock tool validation (simplified for this notebook)\n",
    "3. **LLM judge** - Quality evaluation\n",
    "\n",
    "### Result Format\n",
    "\n",
    "Each evaluation produces:\n",
    "```python\n",
    "{\n",
    "    'test_id': 'TC001',\n",
    "    'passed': True/False,\n",
    "    'score': 0-5 (for judge tests),\n",
    "    'reasoning': 'Why it passed/failed',\n",
    "    'duration': 1.23,  # seconds\n",
    "    'agent_output': 'The actual response...'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghZ4Po2I_g6d"
   },
   "outputs": [],
   "source": [
    "# Pydantic model for LLM judge evaluation\n",
    "class JudgeEvaluation(BaseModel):\n",
    "    \"\"\"Structured evaluation from LLM judge.\"\"\"\n",
    "    score: int = Field(..., ge=1, le=5, description=\"Score from 1 (worst) to 5 (best)\")\n",
    "    reasoning: str = Field(..., description=\"Explanation for the score\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Result from evaluating a single test case.\"\"\"\n",
    "    test_id: str\n",
    "    test_type: str\n",
    "    description: str\n",
    "    passed: bool\n",
    "    score: Optional[int] = None  # Only for llm_judge tests\n",
    "    reasoning: str = \"\"\n",
    "    duration: float = 0.0\n",
    "    agent_output: str = \"\"\n",
    "\n",
    "\n",
    "class AgentEvaluator:\n",
    "    \"\"\"\n",
    "    Unified evaluation class for IT support agents.\n",
    "\n",
    "    Supports three test types:\n",
    "    - assertion: String containment checks\n",
    "    - tool_call: Tool selection and parameter validation\n",
    "    - llm_judge: Quality evaluation using LLM-as-judge\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agent_function, tools=None):\n",
    "        \"\"\"\n",
    "        Initialize evaluator.\n",
    "\n",
    "        Args:\n",
    "            agent_function: Function that takes user input and returns response\n",
    "            tools: Optional dict of tool functions (for tool_call tests)\n",
    "        \"\"\"\n",
    "        self.agent_function = agent_function\n",
    "        self.tools = tools or {}\n",
    "        self.results = []\n",
    "        self.client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "    def evaluate_case(self, test_case: Dict) -> EvaluationResult:\n",
    "        \"\"\"\n",
    "        Evaluate a single test case.\n",
    "\n",
    "        Args:\n",
    "            test_case: Test case dictionary from evaluation dataset\n",
    "\n",
    "        Returns:\n",
    "            EvaluationResult with pass/fail status and details\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Get agent response\n",
    "        agent_output = self.agent_function(test_case['input'])\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        # Route to appropriate evaluator based on test type\n",
    "        if test_case['test_type'] == 'assertion':\n",
    "            result = self._evaluate_assertion(test_case, agent_output)\n",
    "        elif test_case['test_type'] == 'tool_call':\n",
    "            result = self._evaluate_tool_call(test_case, agent_output)\n",
    "        elif test_case['test_type'] == 'llm_judge':\n",
    "            result = self._evaluate_with_judge(test_case, agent_output)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown test type: {test_case['test_type']}\")\n",
    "\n",
    "        # Set common fields\n",
    "        result.test_id = test_case['id']\n",
    "        result.test_type = test_case['test_type']\n",
    "        result.description = test_case['description']\n",
    "        result.duration = duration\n",
    "        result.agent_output = agent_output\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _evaluate_assertion(self, test_case: Dict, agent_output: str) -> EvaluationResult:\n",
    "        \"\"\"Evaluate assertion-based test.\"\"\"\n",
    "        expected = test_case['expected']\n",
    "        output_lower = agent_output.lower()\n",
    "\n",
    "        if 'contains' in expected:\n",
    "            contains_value = expected['contains']\n",
    "\n",
    "            # Handle list of required strings\n",
    "            if isinstance(contains_value, list):\n",
    "                missing = [s for s in contains_value if s.lower() not in output_lower]\n",
    "                passed = len(missing) == 0\n",
    "                reasoning = f\"Expected all of {contains_value}. \" + \\\n",
    "                           (f\"Missing: {missing}\" if not passed else \"All found.\")\n",
    "            else:\n",
    "                # Single string\n",
    "                passed = str(contains_value).lower() in output_lower\n",
    "                reasoning = f\"Expected '{contains_value}' in response. \" + \\\n",
    "                           (\"Found.\" if passed else \"Not found.\")\n",
    "        else:\n",
    "            passed = False\n",
    "            reasoning = \"Unknown assertion format\"\n",
    "\n",
    "        return EvaluationResult(\n",
    "            test_id=\"\",  # Will be set by evaluate_case\n",
    "            test_type=\"assertion\",\n",
    "            description=\"\",\n",
    "            passed=passed,\n",
    "            reasoning=reasoning\n",
    "        )\n",
    "\n",
    "    def _evaluate_tool_call(self, test_case: Dict, agent_output: str) -> EvaluationResult:\n",
    "        \"\"\"\n",
    "        Evaluate tool call test.\n",
    "\n",
    "        NOTE: This is a simplified version that checks if the agent's response\n",
    "        mentions the expected tool or action. In a full ADK implementation,\n",
    "        you would check actual tool calls from the agent.\n",
    "        \"\"\"\n",
    "        expected = test_case['expected']\n",
    "        tool_name = expected['tool_name']\n",
    "        output_lower = agent_output.lower()\n",
    "\n",
    "        # Check if response indicates the tool would be used\n",
    "        # This is a proxy for actual tool call checking\n",
    "        tool_keywords = {\n",
    "            'lookup_ticket': ['ticket', 'check', 'status', 'lookup'],\n",
    "            'search_knowledge_base': ['search', 'knowledge', 'article', 'guide'],\n",
    "            'check_system_status': ['status', 'service', 'check', 'working']\n",
    "        }\n",
    "\n",
    "        keywords = tool_keywords.get(tool_name, [])\n",
    "        mentions_tool = any(kw in output_lower for kw in keywords)\n",
    "\n",
    "        # Check parameter if specified\n",
    "        param_check = True\n",
    "        param_reasoning = \"\"\n",
    "        if 'parameter' in expected:\n",
    "            param = expected['parameter']\n",
    "            if 'value' in param:\n",
    "                param_check = str(param['value']) in agent_output\n",
    "                param_reasoning = f\" Parameter '{param['key']}={param['value']}' \" + \\\n",
    "                                 (\"found\" if param_check else \"not found\")\n",
    "            elif 'contains' in param:\n",
    "                param_check = param['contains'].lower() in output_lower\n",
    "                param_reasoning = f\" Parameter contains '{param['contains']}': \" + \\\n",
    "                                 (\"yes\" if param_check else \"no\")\n",
    "\n",
    "        passed = mentions_tool and param_check\n",
    "        reasoning = f\"Expected tool '{tool_name}' to be used. \" + \\\n",
    "                   (\"Indicators found.\" if mentions_tool else \"No indicators found.\") + \\\n",
    "                   param_reasoning\n",
    "\n",
    "        return EvaluationResult(\n",
    "            test_id=\"\",\n",
    "            test_type=\"tool_call\",\n",
    "            description=\"\",\n",
    "            passed=passed,\n",
    "            reasoning=reasoning\n",
    "        )\n",
    "\n",
    "    def _evaluate_with_judge(self, test_case: Dict, agent_output: str) -> EvaluationResult:\n",
    "        \"\"\"Evaluate using LLM-as-judge.\"\"\"\n",
    "        judge_prompt = f\"\"\"\n",
    "You are an expert evaluator of IT support responses.\n",
    "\n",
    "USER QUERY:\n",
    "{test_case['input']}\n",
    "\n",
    "AGENT RESPONSE:\n",
    "{agent_output}\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "{test_case['criteria']}\n",
    "\n",
    "Evaluate the agent's response based on the criteria above.\n",
    "Provide your evaluation as JSON:\n",
    "{{\n",
    "  \"score\": <integer from 1-5, where 1=very poor, 5=excellent>,\n",
    "  \"reasoning\": \"<detailed explanation>\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Call judge LLM\n",
    "            response = self.client.responses.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                input=judge_prompt\n",
    "            )\n",
    "\n",
    "            # Parse and validate\n",
    "            result_json = json.loads(response.output_text)\n",
    "            evaluation = JudgeEvaluation(**result_json)\n",
    "\n",
    "            # Check if score meets threshold\n",
    "            min_score = test_case['expected'].get('min_score', 3)\n",
    "            passed = evaluation.score >= min_score\n",
    "\n",
    "            return EvaluationResult(\n",
    "                test_id=\"\",\n",
    "                test_type=\"llm_judge\",\n",
    "                description=\"\",\n",
    "                passed=passed,\n",
    "                score=evaluation.score,\n",
    "                reasoning=f\"Score: {evaluation.score}/5 (threshold: {min_score}). {evaluation.reasoning}\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return EvaluationResult(\n",
    "                test_id=\"\",\n",
    "                test_type=\"llm_judge\",\n",
    "                description=\"\",\n",
    "                passed=False,\n",
    "                reasoning=f\"Judge evaluation failed: {str(e)}\"\n",
    "            )\n",
    "\n",
    "    def evaluate_all(self, dataset: List[Dict]) -> List[EvaluationResult]:\n",
    "        \"\"\"\n",
    "        Evaluate all test cases in dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: List of test case dictionaries\n",
    "\n",
    "        Returns:\n",
    "            List of EvaluationResult objects\n",
    "        \"\"\"\n",
    "        self.results = []\n",
    "\n",
    "        print(f\"üöÄ Starting evaluation of {len(dataset)} test cases...\\n\")\n",
    "\n",
    "        for i, test_case in enumerate(dataset, 1):\n",
    "            print(f\"[{i}/{len(dataset)}] Evaluating {test_case['id']}: {test_case['description'][:50]}...\")\n",
    "\n",
    "            result = self.evaluate_case(test_case)\n",
    "            self.results.append(result)\n",
    "\n",
    "            # Show result\n",
    "            status = \"‚úÖ PASS\" if result.passed else \"‚ùå FAIL\"\n",
    "            print(f\"        {status} ({result.duration:.2f}s)\")\n",
    "            if not result.passed:\n",
    "                print(f\"        Reason: {result.reasoning[:80]}...\")\n",
    "            print()\n",
    "\n",
    "        print(f\"‚úÖ Evaluation complete!\\n\")\n",
    "        return self.results\n",
    "\n",
    "    def generate_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate comprehensive evaluation report.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with metrics and analysis\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            return {\"error\": \"No evaluation results available\"}\n",
    "\n",
    "        total_cases = len(self.results)\n",
    "        passed_cases = sum(1 for r in self.results if r.passed)\n",
    "        failed_cases = total_cases - passed_cases\n",
    "        pass_rate = (passed_cases / total_cases) * 100\n",
    "\n",
    "        # Calculate average score for judge tests\n",
    "        judge_results = [r for r in self.results if r.score is not None]\n",
    "        avg_score = sum(r.score for r in judge_results) / len(judge_results) if judge_results else None\n",
    "\n",
    "        # Average duration\n",
    "        avg_duration = sum(r.duration for r in self.results) / total_cases\n",
    "\n",
    "        # Group by test type\n",
    "        by_type = {}\n",
    "        for result in self.results:\n",
    "            if result.test_type not in by_type:\n",
    "                by_type[result.test_type] = {'total': 0, 'passed': 0}\n",
    "            by_type[result.test_type]['total'] += 1\n",
    "            if result.passed:\n",
    "                by_type[result.test_type]['passed'] += 1\n",
    "\n",
    "        # Failed test details\n",
    "        failures = [\n",
    "            {\n",
    "                'test_id': r.test_id,\n",
    "                'description': r.description,\n",
    "                'reasoning': r.reasoning\n",
    "            }\n",
    "            for r in self.results if not r.passed\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_cases': total_cases,\n",
    "            'passed': passed_cases,\n",
    "            'failed': failed_cases,\n",
    "            'pass_rate': pass_rate,\n",
    "            'average_score': avg_score,\n",
    "            'average_duration': avg_duration,\n",
    "            'by_test_type': by_type,\n",
    "            'failures': failures\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ AgentEvaluator class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOORoBFp_g6e"
   },
   "source": [
    "## 4. Creating the IT Support Agent\n",
    "\n",
    "Let's create a simple IT support agent that we'll evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xW1Amfxq_g6e"
   },
   "outputs": [],
   "source": [
    "def it_support_agent(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    IT Support agent - provides technical support assistance.\n",
    "\n",
    "    Args:\n",
    "        user_query: User's question or issue\n",
    "\n",
    "    Returns:\n",
    "        Agent's response\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "You are a helpful IT support agent for a company helpdesk.\n",
    "\n",
    "Your role:\n",
    "- Answer technical questions clearly\n",
    "- Provide troubleshooting steps\n",
    "- Be professional and empathetic\n",
    "- Check ticket status when asked\n",
    "- Search knowledge base for solutions\n",
    "- Check system status when relevant\n",
    "\n",
    "Always be concise but helpful.\n",
    "\"\"\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        input=f\"System: {system_prompt}\\n\\nUser: {user_query}\\n\\nAssistant:\"\n",
    "    )\n",
    "\n",
    "    return response.output_text\n",
    "\n",
    "# Test the agent\n",
    "test_response = it_support_agent(\"What port does SSH use?\")\n",
    "print(\"ü§ñ Agent test:\")\n",
    "print(f\"Q: What port does SSH use?\")\n",
    "print(f\"A: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3m3T6Voe_g6e"
   },
   "source": [
    "## 5. Running Your First Evaluation\n",
    "\n",
    "Now let's evaluate our IT support agent against the full dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mglNS9Bi_g6e"
   },
   "outputs": [],
   "source": [
    "# Create evaluator\n",
    "evaluator = AgentEvaluator(it_support_agent)\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.evaluate_all(EVALUATION_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpwBRJ-z_g6f"
   },
   "source": [
    "## 6. Evaluation Report Generation\n",
    "\n",
    "Let's generate and display a comprehensive report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XUiNi_4_g6f"
   },
   "outputs": [],
   "source": [
    "# Generate report\n",
    "report = evaluator.generate_report()\n",
    "\n",
    "# Display report\n",
    "print(\"=\"*60)\n",
    "print(\"üìä EVALUATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÖ Timestamp: {report['timestamp']}\")\n",
    "print(f\"\\nüìà Overall Metrics:\")\n",
    "print(f\"  Total Test Cases: {report['total_cases']}\")\n",
    "print(f\"  ‚úÖ Passed: {report['passed']}\")\n",
    "print(f\"  ‚ùå Failed: {report['failed']}\")\n",
    "print(f\"  üìä Pass Rate: {report['pass_rate']:.1f}%\")\n",
    "\n",
    "if report['average_score'] is not None:\n",
    "    print(f\"  ‚≠ê Average Score (Judge tests): {report['average_score']:.2f}/5\")\n",
    "\n",
    "print(f\"  ‚è±Ô∏è  Average Duration: {report['average_duration']:.2f}s\")\n",
    "\n",
    "print(f\"\\nüìã Results by Test Type:\")\n",
    "for test_type, stats in report['by_test_type'].items():\n",
    "    pass_rate = (stats['passed'] / stats['total']) * 100\n",
    "    print(f\"  {test_type:15s}: {stats['passed']}/{stats['total']} passed ({pass_rate:.1f}%)\")\n",
    "\n",
    "if report['failures']:\n",
    "    print(f\"\\n‚ùå Failed Tests:\")\n",
    "    for failure in report['failures']:\n",
    "        print(f\"\\n  üö© {failure['test_id']}: {failure['description']}\")\n",
    "        print(f\"     Reason: {failure['reasoning'][:100]}...\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ All tests passed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovJ-H44c_g6f"
   },
   "source": [
    "### Convert Results to DataFrame for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGaqfxDx_g6f"
   },
   "outputs": [],
   "source": [
    "# Convert results to pandas DataFrame\n",
    "results_df = pd.DataFrame([asdict(r) for r in evaluator.results])\n",
    "\n",
    "# Display DataFrame (rendered as table in Jupyter/Colab)\n",
    "print(\"üìä Results DataFrame:\")\n",
    "display(results_df[['test_id', 'test_type', 'passed', 'score', 'duration']])\n",
    "\n",
    "print(f\"\\nüìà Summary Statistics:\")\n",
    "display(results_df.groupby('test_type')['passed'].agg(['count', 'sum', 'mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d3_oyqA_g6g"
   },
   "source": [
    "## 7. Visualizing Results\n",
    "\n",
    "Let's create visualizations to better understand our evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U5KAuKg_g6g"
   },
   "outputs": [],
   "source": [
    "def visualize_results(evaluator: AgentEvaluator):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization dashboard.\n",
    "\n",
    "    Generates 4 charts:\n",
    "    1. Score distribution (for judge tests)\n",
    "    2. Pass/fail rate (pie chart)\n",
    "    3. Response duration by test\n",
    "    4. Pass rate by test type\n",
    "    \"\"\"\n",
    "    results_df = pd.DataFrame([asdict(r) for r in evaluator.results])\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('IT Support Agent Evaluation Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Score Distribution (for judge tests)\n",
    "    ax1 = axes[0, 0]\n",
    "    judge_scores = results_df[results_df['score'].notna()]['score']\n",
    "    if len(judge_scores) > 0:\n",
    "        ax1.hist(judge_scores, bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5],\n",
    "                edgecolor='black', color='skyblue', alpha=0.7)\n",
    "        ax1.set_xlabel('Score', fontsize=12)\n",
    "        ax1.set_ylabel('Frequency', fontsize=12)\n",
    "        ax1.set_title('Score Distribution (LLM Judge Tests)', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xticks([1, 2, 3, 4, 5])\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'No judge test scores', ha='center', va='center', fontsize=12)\n",
    "        ax1.set_title('Score Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # 2. Pass/Fail Rate (pie chart)\n",
    "    ax2 = axes[0, 1]\n",
    "    passed_count = results_df['passed'].sum()\n",
    "    failed_count = len(results_df) - passed_count\n",
    "    colors = ['#90EE90', '#FFB6C6']\n",
    "    ax2.pie([passed_count, failed_count], labels=['Passed', 'Failed'],\n",
    "           autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    ax2.set_title('Overall Pass/Fail Rate', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # 3. Response Duration by Test\n",
    "    ax3 = axes[1, 0]\n",
    "    test_ids = results_df['test_id']\n",
    "    durations = results_df['duration']\n",
    "    colors_duration = ['green' if p else 'red' for p in results_df['passed']]\n",
    "    ax3.bar(range(len(test_ids)), durations, color=colors_duration, alpha=0.6, edgecolor='black')\n",
    "    ax3.set_xlabel('Test Case', fontsize=12)\n",
    "    ax3.set_ylabel('Duration (seconds)', fontsize=12)\n",
    "    ax3.set_title('Response Duration by Test', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xticks(range(len(test_ids)))\n",
    "    ax3.set_xticklabels(test_ids, rotation=45, ha='right')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Legend\n",
    "    green_patch = mpatches.Patch(color='green', alpha=0.6, label='Passed')\n",
    "    red_patch = mpatches.Patch(color='red', alpha=0.6, label='Failed')\n",
    "    ax3.legend(handles=[green_patch, red_patch], loc='upper right')\n",
    "\n",
    "    # 4. Pass Rate by Test Type\n",
    "    ax4 = axes[1, 1]\n",
    "    type_stats = results_df.groupby('test_type')['passed'].agg(['sum', 'count'])\n",
    "    type_stats['pass_rate'] = (type_stats['sum'] / type_stats['count']) * 100\n",
    "\n",
    "    test_types = type_stats.index\n",
    "    pass_rates = type_stats['pass_rate']\n",
    "    colors_type = ['green' if pr >= 70 else 'orange' if pr >= 50 else 'red'\n",
    "                   for pr in pass_rates]\n",
    "\n",
    "    ax4.bar(test_types, pass_rates, color=colors_type, alpha=0.6, edgecolor='black')\n",
    "    ax4.set_xlabel('Test Type', fontsize=12)\n",
    "    ax4.set_ylabel('Pass Rate (%)', fontsize=12)\n",
    "    ax4.set_title('Pass Rate by Test Type', fontsize=12, fontweight='bold', pad=20)\n",
    "    ax4.set_ylim(0, 110)  # Extended to 110 to give space for percentage labels\n",
    "    ax4.axhline(y=70, color='gray', linestyle='--', alpha=0.5, label='70% threshold')\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    ax4.legend()\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, (test_type, rate) in enumerate(zip(test_types, pass_rates)):\n",
    "        ax4.text(i, rate + 2, f'{rate:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations\n",
    "visualize_results(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3sd7PT9_g6g"
   },
   "source": [
    "## 8. Iterative Improvement Workflow\n",
    "\n",
    "One of the most powerful uses of an evaluation pipeline is **iterative improvement**.\n",
    "\n",
    "### The Improvement Loop\n",
    "\n",
    "```\n",
    "1. Run evaluation ‚Üí Get baseline metrics\n",
    "2. Analyze failures ‚Üí Identify patterns\n",
    "3. Improve agent ‚Üí Modify prompt/logic\n",
    "4. Re-run evaluation ‚Üí Get new metrics\n",
    "5. Compare ‚Üí Quantify improvement\n",
    "6. Repeat\n",
    "```\n",
    "\n",
    "Let's demonstrate this workflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJH_8ZLd_g6g"
   },
   "source": [
    "### Step 1: Baseline (Already Done)\n",
    "\n",
    "We've already run our baseline evaluation above. Let's save those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34wyNWFR_g6g"
   },
   "outputs": [],
   "source": [
    "# Save baseline results\n",
    "baseline_report = evaluator.generate_report()\n",
    "\n",
    "print(\"üìä Baseline Results:\")\n",
    "print(f\"  Pass Rate: {baseline_report['pass_rate']:.1f}%\")\n",
    "print(f\"  Passed: {baseline_report['passed']}/{baseline_report['total_cases']}\")\n",
    "if baseline_report['average_score']:\n",
    "    print(f\"  Avg Score: {baseline_report['average_score']:.2f}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l0UZ6Tg_g6g"
   },
   "source": [
    "### Step 2: Analyze Failures\n",
    "\n",
    "Look at what failed and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kUsmZtG_g6g"
   },
   "outputs": [],
   "source": [
    "if baseline_report['failures']:\n",
    "    print(\"üîç Analyzing Failures:\\n\")\n",
    "    for failure in baseline_report['failures']:\n",
    "        print(f\"‚ùå {failure['test_id']}: {failure['description']}\")\n",
    "        print(f\"   Reason: {failure['reasoning'][:150]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚úÖ No failures to analyze - agent performing perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlouFxkp_g6g"
   },
   "source": [
    "### Step 3: Improve the Agent\n",
    "\n",
    "Based on failures, let's create an improved version of our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DB3LPr4_g6h"
   },
   "outputs": [],
   "source": [
    "def improved_it_support_agent(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Improved IT Support agent with better instructions.\n",
    "\n",
    "    Improvements:\n",
    "    - More explicit about being helpful and thorough\n",
    "    - Better handling of frustrated customers\n",
    "    - Clearer troubleshooting steps\n",
    "    - More professional tone guidance\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "You are a helpful IT support agent for a company helpdesk.\n",
    "\n",
    "Your role:\n",
    "- Answer technical questions with clear, accurate information\n",
    "- Provide step-by-step troubleshooting instructions\n",
    "- Be professional, empathetic, and patient - especially with frustrated users\n",
    "- When checking tickets, mention you're looking it up\n",
    "- When searching for solutions, indicate you're searching the knowledge base\n",
    "- When checking system status, mention you're verifying service status\n",
    "\n",
    "Guidelines:\n",
    "- Always acknowledge the user's concern\n",
    "- Provide actionable steps\n",
    "- Be concise but thorough\n",
    "- Use simple language for technical concepts\n",
    "- Show empathy for frustrated customers\n",
    "\n",
    "Examples:\n",
    "- \"Let me check the status of ticket #5678 for you...\"\n",
    "- \"I'll search our knowledge base for password reset instructions...\"\n",
    "- \"Let me verify the email service status...\"\n",
    "\"\"\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        input=f\"System: {system_prompt}\\n\\nUser: {user_query}\\n\\nAssistant:\"\n",
    "    )\n",
    "\n",
    "    return response.output_text\n",
    "\n",
    "print(\"‚úÖ Improved agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQ8hiy_y_g6h"
   },
   "source": [
    "### Step 4: Re-run Evaluation\n",
    "\n",
    "Evaluate the improved agent on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwaLmiRS_g6h"
   },
   "outputs": [],
   "source": [
    "# Create new evaluator with improved agent\n",
    "improved_evaluator = AgentEvaluator(improved_it_support_agent)\n",
    "\n",
    "# Run evaluation\n",
    "improved_results = improved_evaluator.evaluate_all(EVALUATION_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AsFbhCN_g6h"
   },
   "source": [
    "### Step 5: Compare Results\n",
    "\n",
    "Compare baseline vs improved metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNGda1i2_g6h"
   },
   "outputs": [],
   "source": [
    "# Generate improved report\n",
    "improved_report = improved_evaluator.generate_report()\n",
    "\n",
    "# Comparison\n",
    "print(\"=\"*70)\n",
    "print(\"üìä BEFORE vs AFTER COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Baseline':<20} {'Improved':<20} {'Change'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Pass rate\n",
    "baseline_pass_rate = baseline_report['pass_rate']\n",
    "improved_pass_rate = improved_report['pass_rate']\n",
    "pass_rate_change = improved_pass_rate - baseline_pass_rate\n",
    "print(f\"{'Pass Rate':<30} {baseline_pass_rate:>6.1f}%{'':<13} {improved_pass_rate:>6.1f}%{'':<13} {pass_rate_change:+.1f}%\")\n",
    "\n",
    "# Passed tests\n",
    "print(f\"{'Tests Passed':<30} {baseline_report['passed']}/{baseline_report['total_cases']}{'':<16} \"\n",
    "      f\"{improved_report['passed']}/{improved_report['total_cases']}{'':<16} \"\n",
    "      f\"{improved_report['passed'] - baseline_report['passed']:+d}\")\n",
    "\n",
    "# Average score (if available)\n",
    "if baseline_report['average_score'] and improved_report['average_score']:\n",
    "    score_change = improved_report['average_score'] - baseline_report['average_score']\n",
    "    print(f\"{'Avg Score (Judge)':<30} {baseline_report['average_score']:>6.2f}/5{'':<12} \"\n",
    "          f\"{improved_report['average_score']:>6.2f}/5{'':<12} {score_change:+.2f}\")\n",
    "\n",
    "# Duration\n",
    "duration_change = improved_report['average_duration'] - baseline_report['average_duration']\n",
    "print(f\"{'Avg Duration':<30} {baseline_report['average_duration']:>6.2f}s{'':<12} \"\n",
    "      f\"{improved_report['average_duration']:>6.2f}s{'':<12} {duration_change:+.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Summary\n",
    "if improved_pass_rate > baseline_pass_rate:\n",
    "    print(f\"\\n‚úÖ IMPROVEMENT! Pass rate increased by {pass_rate_change:.1f}%\")\n",
    "    print(f\"   {improved_report['passed'] - baseline_report['passed']} more tests passing\")\n",
    "elif improved_pass_rate < baseline_pass_rate:\n",
    "    print(f\"\\n‚ö†Ô∏è  REGRESSION! Pass rate decreased by {abs(pass_rate_change):.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n‚û°Ô∏è  NO CHANGE in pass rate\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6RvrTim_g6h"
   },
   "source": [
    "### Visualize the Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-hM56ZV_g6h"
   },
   "outputs": [],
   "source": [
    "# Side-by-side comparison chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Baseline vs Improved Agent Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Chart 1: Pass Rate Comparison\n",
    "ax1 = axes[0]\n",
    "versions = ['Baseline', 'Improved']\n",
    "pass_rates = [baseline_report['pass_rate'], improved_report['pass_rate']]\n",
    "colors = ['#FFB6C6', '#90EE90']\n",
    "\n",
    "bars = ax1.bar(versions, pass_rates, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Pass Rate (%)', fontsize=12)\n",
    "ax1.set_title('Pass Rate Comparison', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.axhline(y=70, color='gray', linestyle='--', alpha=0.5, label='70% target')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, (version, rate) in enumerate(zip(versions, pass_rates)):\n",
    "    ax1.text(i, rate + 2, f'{rate:.1f}%', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Chart 2: Test Results Breakdown\n",
    "ax2 = axes[1]\n",
    "x = [0, 1]\n",
    "width = 0.35\n",
    "\n",
    "passed_baseline = baseline_report['passed']\n",
    "failed_baseline = baseline_report['failed']\n",
    "passed_improved = improved_report['passed']\n",
    "failed_improved = improved_report['failed']\n",
    "\n",
    "ax2.bar([i - width/2 for i in x], [passed_baseline, passed_improved],\n",
    "        width, label='Passed', color='#90EE90', alpha=0.7, edgecolor='black')\n",
    "ax2.bar([i + width/2 for i in x], [failed_baseline, failed_improved],\n",
    "        width, label='Failed', color='#FFB6C6', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax2.set_ylabel('Number of Tests', fontsize=12)\n",
    "ax2.set_title('Test Results Breakdown', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(versions)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzBOi_g7_g6h"
   },
   "source": [
    "## 9. Advanced: Exporting and Tracking Results\n",
    "\n",
    "In production, you'll want to track evaluation results over time.\n",
    "\n",
    "\n",
    "\n",
    "- üìà **Monitor trends** - Is quality improving or degrading?\n",
    "- üêõ **Detect regressions** - Did a change break something?\n",
    "- üìä **Prove ROI** - Show quantitative improvements\n",
    "- üéØ **Set benchmarks** - Track progress toward goals\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Save results to CSV** - Easy to analyze in spreadsheets\n",
    "2. **Version control** - Track evaluation dataset changes\n",
    "3. **Timestamp everything** - Know when each evaluation ran\n",
    "4. **Tag versions** - Link to agent/model versions\n",
    "5. **Automate** - Run evals in CI/CD pipeline\n",
    "\n",
    "### Example: Export to CSV (Theoretical)\n",
    "\n",
    "```python\n",
    "# In a real project, you would do:\n",
    "results_df = pd.DataFrame([asdict(r) for r in evaluator.results])\n",
    "results_df['timestamp'] = datetime.now()\n",
    "results_df['agent_version'] = 'v1.2.3'\n",
    "results_df.to_csv(f'eval_results_{datetime.now():%Y%m%d_%H%M%S}.csv', index=False)\n",
    "\n",
    "# Load and compare multiple runs:\n",
    "df1 = pd.read_csv('eval_results_20250101.csv')\n",
    "df2 = pd.read_csv('eval_results_20250201.csv')\n",
    "comparison = pd.merge(df1, df2, on='test_id', suffixes=('_jan', '_feb'))\n",
    "```\n",
    "\n",
    "### Continuous Monitoring Dashboard (Conceptual)\n",
    "\n",
    "In production, you might build:\n",
    "- **Daily automated evals** - Run tests nightly\n",
    "- **Slack notifications** - Alert on regressions\n",
    "- **Grafana dashboard** - Visualize trends\n",
    "- **Threshold alerts** - Warn if pass rate drops below 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5gGOSXv_g6h"
   },
   "source": [
    "## 10. Best Practices for Evaluation Pipelines\n",
    "\n",
    "### 1. Dataset Management\n",
    "\n",
    "‚úÖ **DO:**\n",
    "- Keep evaluation dataset in version control\n",
    "- Start small (10-20 cases), grow over time\n",
    "- Add new cases when you find edge cases\n",
    "- Balance different test types\n",
    "- Document why each test exists\n",
    "\n",
    "‚ùå **DON'T:**\n",
    "- Hardcode test data in code\n",
    "- Remove failing tests just to boost metrics\n",
    "- Only test happy paths\n",
    "- Create redundant test cases\n",
    "\n",
    "### 2. Evaluation Frequency\n",
    "\n",
    "**When to run evaluations:**\n",
    "- ‚úÖ Before every deployment\n",
    "- ‚úÖ After every prompt change\n",
    "- ‚úÖ After model upgrades\n",
    "- ‚úÖ Nightly (automated)\n",
    "- ‚úÖ When adding new features\n",
    "\n",
    "### 3. Thresholds and Goals\n",
    "\n",
    "**Set clear targets:**\n",
    "- Overall pass rate: 80%+\n",
    "- Critical tests: 100%\n",
    "- Judge tests: Average score 4+\n",
    "- Response time: <2s per test\n",
    "\n",
    "### 4. Test Type Balance\n",
    "\n",
    "**Recommended mix:**\n",
    "- 30% Assertion tests (fast, deterministic)\n",
    "- 30% Tool/behavior tests (validate agent logic)\n",
    "- 40% Judge tests (quality and subjective measures)\n",
    "\n",
    "### 5. Iteration Strategy\n",
    "\n",
    "**Improvement workflow:**\n",
    "1. Run baseline evaluation\n",
    "2. Fix the WORST failures first\n",
    "3. Don't try to fix everything at once\n",
    "4. Re-evaluate after each change\n",
    "5. Watch for regressions\n",
    "\n",
    "### 6. Cost Management\n",
    "\n",
    "**LLM-as-judge is expensive:**\n",
    "- Each judge test = 2 API calls (agent + judge)\n",
    "- 100 tests with 40 judges = ~140 API calls\n",
    "- Use gpt-5-nano for cost efficiency\n",
    "- Cache agent responses when possible\n",
    "- Run full suite less frequently\n",
    "\n",
    "### 7. Documentation\n",
    "\n",
    "**Document everything:**\n",
    "- What each test validates\n",
    "- Why thresholds are set at specific values\n",
    "- How to interpret results\n",
    "- Who to notify on failures\n",
    "- How to add new tests\n",
    "\n",
    "### 8. Team Integration\n",
    "\n",
    "**Make evals part of your workflow:**\n",
    "- Run in CI/CD pipeline\n",
    "- Review results in PR reviews\n",
    "- Share reports with stakeholders\n",
    "- Celebrate improvements!\n",
    "\n",
    "### 9. Common Pitfalls to Avoid\n",
    "\n",
    "‚ö†Ô∏è **\"Gaming\" the metrics:**\n",
    "- Don't remove failing tests to boost pass rate\n",
    "- Don't lower thresholds without good reason\n",
    "- Don't cherry-pick which tests to include\n",
    "\n",
    "‚ö†Ô∏è **Over-fitting to eval set:**\n",
    "- Don't only optimize for eval dataset\n",
    "- Real users will have different queries\n",
    "- Keep adding new test cases\n",
    "\n",
    "‚ö†Ô∏è **Ignoring edge cases:**\n",
    "- Test error conditions\n",
    "- Test unusual inputs\n",
    "- Test adversarial queries\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
