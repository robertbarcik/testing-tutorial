{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpylwiSTvcdV"
      },
      "source": [
        "# LLM Testing: From Manual to Automated\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "1. Understand why automated testing is essential for LLM applications\n",
        "2. Convert manual tests (like those you did in ChatGPT) into automated Python tests\n",
        "3. Use pytest to organize and run test suites\n",
        "4. Write parameterized tests to test multiple scenarios efficiently\n",
        "5. Validate structured outputs using Pydantic models\n",
        "6. Interpret test results and debug failures\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Context: What You've Done Before (Manual Testing)\n",
        "\n",
        "Previously, you manually tested LLMs in ChatGPT by:\n",
        "- Asking factual questions and checking answers\n",
        "- Testing prompt sensitivity by rephrasing questions\n",
        "- Checking for bias in responses\n",
        "- Evaluating consistency across multiple queries\n",
        "\n",
        "### Why This Doesn't Scale\n",
        "\n",
        "Imagine you're building an **IT Support Chatbot** for a company helpdesk. You need to test:\n",
        "- âœ… Does it correctly identify ticket severity?\n",
        "- âœ… Does it provide accurate troubleshooting steps?\n",
        "- âœ… Does it extract ticket information correctly?\n",
        "- âœ… Is it consistent across similar queries?\n",
        "\n",
        "**Problem:** Testing these manually every time you update your prompt or model is:\n",
        "- â° Time-consuming\n",
        "- ðŸ› Error-prone\n",
        "- ðŸ“ˆ Not scalable (what about 100 test cases?)\n",
        "- ðŸ”„ Hard to reproduce\n",
        "\n",
        "### The Solution: Automated Testing\n",
        "\n",
        "**Same tests you did in ChatGPT, now in code!**\n",
        "\n",
        "Benefits:\n",
        "- ðŸš€ Run hundreds of tests in seconds\n",
        "- ðŸ” Reproducible results\n",
        "- ðŸ¤– Integrate into CI/CD pipelines\n",
        "- ðŸ“Š Generate test reports automatically\n",
        "- ðŸ›¡ï¸ Catch regressions when you change prompts or models\n",
        "\n",
        "---\n",
        "\n",
        "Let's get started! ðŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SH1nSnzvcdW"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, we'll install the required packages and set up our OpenAI API access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAFKccVVvcdX"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install openai pytest==8.3.4 pytest-html==4.1.1 pydantic>=2.11.0 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD3vNVKTvcdX"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pytest\n",
        "from pydantic import BaseModel, Field\n",
        "import json\n",
        "from typing import List, Optional\n",
        "\n",
        "print(\"âœ… All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9AQgUETvcdX"
      },
      "source": [
        "### API Key Setup\n",
        "\n",
        "You'll need an OpenAI API key to run these tests.\n",
        "\n",
        "**How to get your API key:**\n",
        "1. Go to [platform.openai.com](https://platform.openai.com)\n",
        "2. Sign in or create an account\n",
        "3. Navigate to API Keys section\n",
        "4. Create a new secret key\n",
        "5. Copy it and use it below\n",
        "\n",
        "**Two ways to provide your API key:**\n",
        "\n",
        "**Option 1: Colab Secrets (Recommended - More Secure)**\n",
        "- Click the ðŸ”‘ key icon in the left sidebar\n",
        "- Add a new secret with name: `OPENAI_API_KEY`\n",
        "- Paste your API key as the value\n",
        "- Enable \"Notebook access\" toggle\n",
        "- Run the cell below - it will automatically load from secrets\n",
        "\n",
        "**Option 2: Enter when prompted**\n",
        "- Just run the cell below\n",
        "- You'll be prompted to enter your API key\n",
        "- The key will be hidden as you type\n",
        "\n",
        "**ðŸ’° Cost Note:** We'll use the `gpt-5-nano` model, which is very cost-effective for testing. These examples will cost less than $0.01 to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w75U7cnRvcdX"
      },
      "outputs": [],
      "source": [
        "# Configure OpenAI API key\n",
        "# Method 1: Try to get API key from Colab secrets (recommended)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"âœ… API key loaded from Colab secrets\")\n",
        "except:\n",
        "    # Method 2: Manual input (fallback)\n",
        "    from getpass import getpass\n",
        "    print(\"ðŸ’¡ To use Colab secrets: Go to ðŸ”‘ (left sidebar) â†’ Add new secret â†’ Name: OPENAI_API_KEY\")\n",
        "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Set the API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# Validate that the API key is set\n",
        "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
        "    raise ValueError(\"âŒ ERROR: No API key provided!\")\n",
        "\n",
        "print(\"âœ… Authentication configured!\")\n",
        "\n",
        "# Configure which OpenAI model to use\n",
        "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
        "print(f\"ðŸ¤– Selected Model: {OPENAI_MODEL}\")\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPd7UhilvcdY"
      },
      "source": [
        "## 2. Your First Automated Test\n",
        "\n",
        "Let's start simple. Remember when you manually tested factual accuracy in ChatGPT in the Basic Testing document? You asked questions like \"What is the capital of Australia?\" and checked if the response was \"Canberra\".\n",
        "\n",
        "**Manual Test (What you did in 00_Basic_Testing.md):**\n",
        "1. Open ChatGPT\n",
        "2. Type: \"What is the capital of Australia?\"\n",
        "3. Read response\n",
        "4. Check if it says \"Canberra\" (not Sydney!)\n",
        "\n",
        "**Automated Test (What we'll do now):**\n",
        "Same thing, but in code! This means:\n",
        "- You don't have to manually type the question each time\n",
        "- The checking happens automatically\n",
        "- You can run this test hundreds of times in seconds\n",
        "- You get consistent, reproducible results\n",
        "\n",
        "\n",
        "We will create a helper function that send your question to the LLM API and returns the text response.\n",
        "\n",
        "  Think of this function as similar to a REST API client in traditional automated testing. Just as you might use `requests.post()` to call a web service and get JSON back, this function calls OpenAI's API with your prompt and receives the LLM's text response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBBKV_v2vcdY"
      },
      "outputs": [],
      "source": [
        "# Helper function to call the LLM\n",
        "def ask_llm(prompt: str, model: str = \"gpt-5-nano\") -> str:\n",
        "    \"\"\"\n",
        "    Send a prompt to the LLM and return the response.\n",
        "\n",
        "    Args:\n",
        "        prompt: The question or instruction to send to the LLM\n",
        "        model: The model to use (default: gpt-5-nano)\n",
        "\n",
        "    Returns:\n",
        "        The LLM's response as a string\n",
        "    \"\"\"\n",
        "    response = client.responses.create(\n",
        "        model=model,\n",
        "        input=prompt\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "# Test it out!\n",
        "response = ask_llm(\"What is the capital of Australia?\")\n",
        "print(f\"LLM Response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBkEU3NcvcdY"
      },
      "source": [
        "### Plain Python Test Function\n",
        "\n",
        "Now we will create a test function that automate what you previously did manually in ChatGPT: it\n",
        "- asks 'What is the capital of Australia?'\n",
        "- receives the LLM's response\n",
        "- automatically checks whether 'Canberra' appears in the answer\n",
        "\n",
        "Notice the structure follows the AAA pattern (Arrange-Act-Assert):\n",
        " - **Arrange**: We prepare our test input â€” the question string, just like\n",
        "  setting up test data in a unit test\n",
        "  - **Act**: We execute the action being tested â€” calling the LLM, similar\n",
        "  to invoking the function or API endpoint you're testing\n",
        "  - **Assert**: We verify the result meets our expectations using\n",
        "  Python's assert statement\n",
        "\n",
        "\n",
        "â— The key difference from testing traditional software is that we check for the\n",
        "   presence of the correct answer ('Canberra' in response) rather than exact\n",
        "  string equality, because LLMs generate natural language that varies in\n",
        "  phrasing. LLMs might say 'The capital is Canberra' or 'Canberra is the capital\n",
        "   city', but both contain the factually correct answer we're looking for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zNf5BRKvcdY"
      },
      "outputs": [],
      "source": [
        "def test_australia_capital_knowledge():\n",
        "    \"\"\"\n",
        "    Test: Does the LLM know what the capital of Australia is?\n",
        "    Expected: Response should contain \"Canberra\" (not Sydney!)\n",
        "\n",
        "    This is the automated version of the manual test from 00_Basic_Testing.md\n",
        "    where we tested if the LLM correctly identifies Canberra as the capital.\n",
        "    \"\"\"\n",
        "    # Arrange: Set up the test data (same question from manual testing)\n",
        "    question = \"What is the capital of Australia?\"\n",
        "\n",
        "    # Act: Call the LLM (same as typing in ChatGPT, but automated)\n",
        "    response = ask_llm(question)\n",
        "\n",
        "    # Assert: Check if the response is correct\n",
        "    # We check that \"Canberra\" appears in the response\n",
        "    assert \"Canberra\" in response, f\"Expected 'Canberra' in response, but got: {response}\"\n",
        "\n",
        "    print(\"âœ… Test passed! LLM correctly identified Canberra as the capital of Australia\")\n",
        "\n",
        "# Run the test\n",
        "test_australia_capital_knowledge()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niH1_MznvcdY"
      },
      "source": [
        "## 3. Building a Test Suite with Pytest\n",
        "\n",
        "In traditional software testing, you might organize related tests into a\n",
        "  test suite. For example, all API tests in one file, all database tests in\n",
        "  another.\n",
        "\n",
        "We'll do the same here: organize all IT support chatbot tests into one file called `test_it_support.py`. Pytest will automatically discover and\n",
        "  run all functions that start with `test_`, just like how testing frameworks\n",
        "  like JUnit or NUnit discover test methods with @Test annotations.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKqMoJGVvcdY"
      },
      "outputs": [],
      "source": [
        "%%writefile test_it_support.py\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pytest\n",
        "from pydantic import BaseModel, Field\n",
        "import json\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def ask_llm(prompt: str, model: str = \"gpt-5-nano\") -> str:\n",
        "    \"\"\"Send a prompt to the LLM and return the response.\"\"\"\n",
        "    response = client.responses.create(\n",
        "        model=model,\n",
        "        input=prompt\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "\n",
        "# Test 1: Technical Fact Testing\n",
        "def test_technical_fact():\n",
        "    \"\"\"\n",
        "    Test: Does the LLM know basic IT facts?\n",
        "    This is like asking factual questions in ChatGPT and checking the answer.\n",
        "    \"\"\"\n",
        "    question = \"What is the default SSH port? Answer with just the number.\"\n",
        "    response = ask_llm(question)\n",
        "\n",
        "    # Check if response contains the correct port\n",
        "    assert \"22\" in response, f\"Expected SSH port 22, but got: {response}\"\n",
        "\n",
        "\n",
        "# Test 2: Ticket Severity Assessment\n",
        "def test_ticket_severity_assessment():\n",
        "    \"\"\"\n",
        "    Test: Can the LLM correctly assess ticket severity?\n",
        "    Critical issues should be flagged as HIGH priority.\n",
        "    \"\"\"\n",
        "    ticket_description = \"\"\"\n",
        "    Ticket: Database server is completely down. All employees cannot access\n",
        "    customer data. Production environment is affected.\n",
        "\n",
        "    Classify this ticket's severity as: LOW, MEDIUM, or HIGH.\n",
        "    Answer with just one word.\n",
        "    \"\"\"\n",
        "\n",
        "    response = ask_llm(ticket_description)\n",
        "\n",
        "    # A complete database outage should be HIGH severity\n",
        "    assert \"HIGH\" in response.upper(), f\"Expected HIGH severity, but got: {response}\"\n",
        "\n",
        "\n",
        "# Test 3: Consistency Testing\n",
        "def test_consistency():\n",
        "    \"\"\"\n",
        "    Test: Does the LLM give consistent answers to the same question?\n",
        "    This is like asking the same question multiple times in ChatGPT.\n",
        "    \"\"\"\n",
        "    question = \"What is the purpose of a firewall in network security? Answer in one sentence.\"\n",
        "\n",
        "    # Ask the same question twice\n",
        "    response1 = ask_llm(question)\n",
        "    response2 = ask_llm(question)\n",
        "\n",
        "    # Check that both responses mention key concepts about firewalls\n",
        "    assert \"firewall\" in response1.lower(), f\"Response 1 missing 'firewall': {response1}\"\n",
        "    assert \"firewall\" in response2.lower(), f\"Response 2 missing 'firewall': {response2}\"\n",
        "\n",
        "    # Check semantic similarity (both should mention blocking/filtering/protecting)\n",
        "    keywords = [\"block\", \"filter\", \"control\", \"protect\", \"monitor\"]\n",
        "    assert any(kw in response1.lower() for kw in keywords), f\"Response 1 missing key concepts: {response1}\"\n",
        "    assert any(kw in response2.lower() for kw in keywords), f\"Response 2 missing key concepts: {response2}\"\n",
        "\n",
        "\n",
        "# Test 4: Structured Output (JSON)\n",
        "def test_structured_output_json():\n",
        "    \"\"\"\n",
        "    Test: Can the LLM extract structured information from a ticket?\n",
        "    This tests if the LLM can parse unstructured text into structured data.\n",
        "    \"\"\"\n",
        "    ticket_text = \"\"\"\n",
        "    Ticket #7823: User Sarah Chen reports that she cannot print from her laptop.\n",
        "    The printer shows as offline. Priority: Medium. Category: Hardware.\n",
        "\n",
        "    Extract the following information as JSON:\n",
        "    - ticket_id\n",
        "    - user_name\n",
        "    - issue_summary\n",
        "    - priority\n",
        "    - category\n",
        "\n",
        "    Return ONLY valid JSON, no other text.\n",
        "    \"\"\"\n",
        "\n",
        "    response = ask_llm(ticket_text)\n",
        "\n",
        "    # Parse the JSON response\n",
        "    try:\n",
        "        data = json.loads(response)\n",
        "    except json.JSONDecodeError:\n",
        "        pytest.fail(f\"Response is not valid JSON: {response}\")\n",
        "\n",
        "    # Verify all required fields are present\n",
        "    required_fields = [\"ticket_id\", \"user_name\", \"issue_summary\", \"priority\", \"category\"]\n",
        "    for field in required_fields:\n",
        "        assert field in data, f\"Missing required field: {field}\"\n",
        "\n",
        "    # Verify correctness of extracted data\n",
        "    assert \"7823\" in str(data[\"ticket_id\"]), f\"Wrong ticket_id: {data['ticket_id']}\"\n",
        "    assert \"Sarah\" in data[\"user_name\"] or \"Chen\" in data[\"user_name\"], f\"Wrong user: {data['user_name']}\"\n",
        "\n",
        "\n",
        "# Test 5: Structured Output with Pydantic Validation\n",
        "class SupportTicket(BaseModel):\n",
        "    \"\"\"Pydantic model for a support ticket.\"\"\"\n",
        "    ticket_id: str = Field(..., description=\"Unique ticket identifier\")\n",
        "    user: str = Field(..., description=\"Name of the user who reported the issue\")\n",
        "    issue: str = Field(..., description=\"Brief description of the issue\")\n",
        "    priority: str = Field(..., description=\"Priority level: LOW, MEDIUM, or HIGH\")\n",
        "    category: str = Field(..., description=\"Category of the issue\")\n",
        "\n",
        "def test_structured_output_validation():\n",
        "    \"\"\"\n",
        "    Test: Can the LLM produce output that passes strict validation?\n",
        "    We use Pydantic to ensure the output has the right structure AND data types.\n",
        "    \"\"\"\n",
        "    ticket_text = \"\"\"\n",
        "    Ticket #9234: John Smith cannot access his email. Getting \"authentication failed\" error.\n",
        "    This is blocking his work. Priority: High. Category: Email.\n",
        "\n",
        "    Extract the information as JSON with these exact fields:\n",
        "    - ticket_id (string)\n",
        "    - user (string)\n",
        "    - issue (string)\n",
        "    - priority (string: LOW, MEDIUM, or HIGH)\n",
        "    - category (string)\n",
        "\n",
        "    Return ONLY valid JSON, no other text.\n",
        "    \"\"\"\n",
        "\n",
        "    response = ask_llm(ticket_text)\n",
        "\n",
        "    # Parse JSON\n",
        "    try:\n",
        "        data = json.loads(response)\n",
        "    except json.JSONDecodeError:\n",
        "        pytest.fail(f\"Response is not valid JSON: {response}\")\n",
        "\n",
        "    # Validate with Pydantic model\n",
        "    try:\n",
        "        ticket = SupportTicket(**data)\n",
        "    except Exception as e:\n",
        "        pytest.fail(f\"Response doesn't match SupportTicket model: {e}\\nData: {data}\")\n",
        "\n",
        "    # Verify semantic correctness\n",
        "    assert \"9234\" in ticket.ticket_id, f\"Wrong ticket_id: {ticket.ticket_id}\"\n",
        "    assert \"John\" in ticket.user or \"Smith\" in ticket.user, f\"Wrong user: {ticket.user}\"\n",
        "    assert ticket.priority.upper() in [\"LOW\", \"MEDIUM\", \"HIGH\"], f\"Invalid priority: {ticket.priority}\"\n",
        "\n",
        "\n",
        "print(\"âœ… Test file created: test_it_support.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down the 5 tests we just created. Each one tests a different capability of our IT Support chatbot:\n",
        "\n",
        "**Test 1: `test_technical_fact()` â€” Basic Fact Checking**\n",
        "\n",
        "- What it tests: Can the LLM recall basic IT knowledge (SSH port number)?\n",
        "- Your chatbot needs accurate factual knowledge to help users. If it\n",
        "  can't remember that SSH uses port 22, it can't provide reliable\n",
        "  troubleshooting advice.\n",
        "- Testing approach: We ask a specific factual question\n",
        "  and check if the correct answer (\"22\") appears anywhere in the\n",
        "  response.\n",
        "\n",
        "**Test 2: `test_ticket_severity_assessment()` â€” Classification Logic**\n",
        "\n",
        "- What it tests: Can the LLM correctly evaluate the severity of an IT issue?\n",
        "- In a real helpdesk system, tickets need proper prioritization. A\n",
        "   complete database outage affecting all employees should always be classified\n",
        "   as HIGH priority, not MEDIUM or LOW.\n",
        "- Testing approach: We provide a clearly critical scenario (database down, production affected) and verify the LLM classifies it as HIGH severity.\n",
        "\n",
        "**Test 3: `test_consistency()` â€” Stability Check**\n",
        "\n",
        "- What it tests: Does the LLM give semantically consistent answers when asked\n",
        "  the same question multiple times?\n",
        "- Users expect reliable answers. If someone asks \"What is a firewall?\" twice and gets contradictory explanations, they'll lose trust in your chatbot.\n",
        "- Testing approach: We ask the same question twice and verify both responses mention core concepts (the word \"firewall\" itself, plus relevant actions like \"block\" or \"protect\"). We don't require identical wordingâ€”just consistent meaning.\n",
        "\n",
        "**Test 4: `test_structured_output_json()` â€” Data Extraction**\n",
        "\n",
        "- What it tests: Can the LLM extract structured information from unstructured\n",
        "  text and return it as valid JSON?\n",
        "- Real systems need structured data for databases, APIs, and workflows. If a user submits a ticket via email or chat, you need to extract the ticket ID, username, issue description, priority, and category into a database record.- - Testing approach: We provide a natural language ticket description and ask the LLM to extract specific fields as JSON. We then verify:\n",
        "  - (1) the response is valid JSON (parseable),\n",
        "  - (2) all required fields are present,\n",
        "  - (3) the extracted values are correct (ticket_id contains \"7823\", user_name contains \"Sarah\" or \"Chen\").\n",
        "\n",
        "**Test 5: `test_structured_output_validation()` â€” Schema Validation**\n",
        "\n",
        "- What it tests: Does the LLM's JSON output conform to a strict data schema\n",
        "  with type checking?\n",
        "- Beyond just having the right fields (Test 4), you need correct data types and valid values. Your database might only accept priority values of \"LOW\", \"MEDIUM\", or \"HIGH\"â€”if the LLM returns \"urgent\" or \"high priority\", your system will crash.\n",
        "- Testing approach: We use Pydantic (a validation library) to define exactly what a valid SupportTicket looks like.\n",
        "  - When the LLM returns JSON, we try to create a SupportTicket object from it. If validation fails, the test fails.\n",
        "  - We also verify the semantic correctness of extracted values.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UWUq831as9Rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ðŸ’¡ In traditional automated testing, you're usually testing deterministic\n",
        "  systems:\n",
        "  - Call add(2, 3) â†’ Always returns 5\n",
        "  - Send API request â†’ Always get same status code and JSON structure\n",
        "  - Query database â†’ Always get same records\n",
        "\n",
        "  With LLMs, you're testing probabilistic systems:\n",
        "  - Ask \"What is a firewall?\" â†’ Might say \"A firewall blocks unauthorized\n",
        "  traffic\" or \"Firewalls control network access\"â€”both correct, different\n",
        "  wording\n",
        "  - This is why we check for key concepts (does response contain \"firewall\" AND\n",
        "   mention \"block\"/\"protect\"?) rather than exact string matches\n",
        "\n",
        "â­ Your testing strategy must adapt: Focus on verifying correct concepts,\n",
        "  intent, and structure rather than exact output."
      ],
      "metadata": {
        "id": "j_Em4NoFuUL8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHSsIJEwvcdZ"
      },
      "source": [
        "### Run the Test Suite\n",
        "\n",
        "Now let's run all 5 tests with pytest!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMcuqfDZvcdZ"
      },
      "outputs": [],
      "source": [
        "# Run pytest with verbose output\n",
        "# -v = verbose (show each test name)\n",
        "# -s = show print statements\n",
        "!pytest test_it_support.py -v -s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_oFyjOFvcdZ"
      },
      "source": [
        "\n",
        "\n",
        "You just ran 5 automated tests! Notice how pytest:\n",
        "- Discovered all functions starting with `test_`\n",
        "- Ran each test independently\n",
        "- Showed which passed or failed\n",
        "\n",
        "**This is the same testing you did manually in ChatGPT, but now:**\n",
        "- âš¡ Takes seconds instead of minutes\n",
        "- ðŸ” Perfectly reproducible\n",
        "- ðŸ“Š Generates reports automatically\n",
        "- ðŸ¤– Can run in CI/CD pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFuenFFdvcdZ"
      },
      "source": [
        "## 4. Parameterized Testing\n",
        "\n",
        "  Imagine you want to test if your IT support chatbot knows standard network\n",
        "  ports. You could write separate test functions like this:\n",
        "\n",
        "  ```python\n",
        "  def test_http_port():\n",
        "      response = ask_llm(\"What port does HTTP use?\")\n",
        "      assert \"80\" in response\n",
        "\n",
        "  def test_https_port():\n",
        "      response = ask_llm(\"What port does HTTPS use?\")\n",
        "      assert \"443\" in response\n",
        "\n",
        "  def test_ssh_port():\n",
        "      response = ask_llm(\"What port does SSH use?\")\n",
        "      assert \"22\" in response\n",
        "\n",
        "  def test_ftp_port():\n",
        "      response = ask_llm(\"What port does FTP use?\")\n",
        "      assert \"21\" in response\n",
        "\n",
        "  def test_smtp_port():\n",
        "      response = ask_llm(\"What port does SMTP use?\")\n",
        "      assert \"25\" in response\n",
        " ```\n",
        "\n",
        "However, problems with this approach are:\n",
        "  - ðŸ” Repetitive code: Same test logic repeated 5 times\n",
        "  - ðŸ“ Hard to maintain: If you need to change the test logic, you must update\n",
        "  5 functions\n",
        "  - ðŸ˜« Tedious to expand: Adding a new port test means writing another entire\n",
        "  function\n",
        "  - ðŸ“Š Poor reporting: If tests fail, you can't easily see patterns (e.g., \"LLM\n",
        "   knows common ports but fails on less common ones\")\n",
        "\n",
        "**The Solution: Parameterized Tests**\n",
        "\n",
        "Parameterized tests let you write the test logic once and run it with multiple sets of data. Think of it like a loop, but specifically designed for testing.\n",
        "\n",
        "Traditional testing parallel:\n",
        "  - In JUnit (Java), you'd use @ParameterizedTest with `@ValueSource` or\n",
        "  `@CsvSource`\n",
        "  - In NUnit (C#), you'd use `[TestCase]` attributes\n",
        "  - In pytest (Python), you use `@pytest.mark.parametrize`\n",
        "\n",
        "**How It Works**\n",
        "\n",
        "Instead of 5 separate functions, you write ONE function with a decorator that supplies different test data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Testing Multiple IT Knowledge Questions\n",
        "\n",
        "We will write three parameterized tests. Let's break down what each test does:\n",
        "\n",
        "**Test 1: `test_port_knowledge()`** â€” Knowledge Testing at Scale\n",
        "\n",
        "  **What it tests:** Does the LLM know standard network port numbers for 5\n",
        "  common protocols?\n",
        "\n",
        "  **Why parameterize?**\n",
        "  - Testing one port proves nothing about the LLM's broader knowledge\n",
        "  - Testing 5 ports reveals if it knows common protocols consistently\n",
        "  - Easy to expand: add DNS (port 53), MySQL (3306), etc. by adding one line\n",
        "\n",
        "  **The test cases:**\n",
        "  - HTTP â†’ 80\n",
        "  - HTTPS â†’ 443\n",
        "  - SSH â†’ 22\n",
        "  - FTP â†’ 21\n",
        "  - SMTP â†’ 25\n",
        "\n",
        "**Test 2: `test_ticket_classification()`** â€” Multi-Category\n",
        "  Classification\n",
        "\n",
        "  **What it tests:** Can the LLM correctly categorize 5 different IT issues\n",
        "  into the right category (Hardware, Software, Network, Access)?\n",
        "\n",
        "  **Why parameterize?**\n",
        "  - Each IT issue has a clear expected category\n",
        "  - Tests if the LLM can distinguish between issue types (not just memorize one\n",
        "   category)\n",
        "  - Real helpdesks have dozens of categoriesâ€”this approach scales easily\n",
        "\n",
        "  **The test cases:**\n",
        "  - \"Laptop screen is cracked\" â†’ Hardware (physical device problem)\n",
        "  - \"Forgot my password\" â†’ Access (authentication issue)\n",
        "  - \"Website loading slowly for all users\" â†’ Network (connectivity problem)\n",
        "  - \"Excel keeps crashing\" â†’ Software (application issue)\n",
        "  - \"Need permission for shared folder\" â†’ Access (authorization issue)\n",
        "\n",
        "  **Why these specific examples?** They test if the LLM can differentiate\n",
        "  between similar-sounding categories (both password reset and folder\n",
        "  permissions are \"Access\" issues, even though one is authentication and the\n",
        "  other is authorization).\n",
        "\n",
        "**Test 3: `test_troubleshooting_advice()`** â€” Flexible Keyword Matching\n",
        "\n",
        "  **What it tests:** Does the LLM provide relevant troubleshooting advice that\n",
        "  mentions appropriate technical concepts?\n",
        "\n",
        "  **Why parameterize?**\n",
        "  - Each IT problem has different relevant keywords\n",
        "  - We're not testing for exact phrasing (too brittle for LLMs)\n",
        "  - We check if the advice is \"in the right ballpark\" by looking for\n",
        "  domain-relevant terms\n",
        "\n",
        "  **The test cases and their keyword lists:**\n",
        "  - \"Can't connect to WiFi\" â†’ [\"wifi\", \"wireless\", \"router\", \"modem\",\n",
        "  \"connection\"]\n",
        "  - \"Printer is offline\" â†’ [\"printer\", \"print\", \"device\", \"cable\", \"driver\"]\n",
        "  - \"Computer is very slow\" â†’ [\"slow\", \"memory\", \"cpu\", \"task\", \"process\",\n",
        "  \"resource\"]\n",
        "  - \"Email won't send\" â†’ [\"email\", \"smtp\", \"server\", \"account\", \"credential\"]\n",
        "  - \"Can't install software\" â†’ [\"install\", \"permission\", \"administrator\",\n",
        "  \"compatibility\", \"space\"]\n",
        "\n",
        "  **Important difference from Tests 1 & 2:** We check if **ANY** of the\n",
        "  keywords appear (not all of them). Why? Because good troubleshooting advice\n",
        "  might say \"check your WiFi router\" OR \"restart your modem\"â€”both are valid,\n",
        "  but they use different keywords.\n",
        "\n",
        "  **The two-part assertion:**\n",
        "  1. **Keyword check:** `assert len(found_keywords) > 0` â€” Did the advice\n",
        "  mention at least one relevant concept?\n",
        "  2. **Length check:** `assert len(response) > 20` â€” Did the LLM actually give\n",
        "  advice, or just echo the problem back?"
      ],
      "metadata": {
        "id": "5obalmOOEELP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h60j0z8nvcdZ"
      },
      "outputs": [],
      "source": [
        "%%writefile test_parameterized.py\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pytest\n",
        "\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def ask_llm(prompt: str, model: str = \"gpt-5-nano\") -> str:\n",
        "    \"\"\"Send a prompt to the LLM and return the response.\"\"\"\n",
        "    response = client.responses.create(\n",
        "        model=model,\n",
        "        input=prompt\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "\n",
        "# Parameterized test for IT knowledge\n",
        "@pytest.mark.parametrize(\"question,expected_answer\", [\n",
        "    (\"What port does HTTP use? Answer with just the number.\", \"80\"),\n",
        "    (\"What port does HTTPS use? Answer with just the number.\", \"443\"),\n",
        "    (\"What port does SSH use? Answer with just the number.\", \"22\"),\n",
        "    (\"What port does FTP use? Answer with just the number.\", \"21\"),\n",
        "    (\"What port does SMTP use? Answer with just the number.\", \"25\"),\n",
        "])\n",
        "def test_port_knowledge(question, expected_answer):\n",
        "    \"\"\"\n",
        "    Test: Does the LLM know standard network ports?\n",
        "    This ONE test function runs 5 times with different data!\n",
        "    \"\"\"\n",
        "    response = ask_llm(question)\n",
        "    assert expected_answer in response, f\"Expected '{expected_answer}' in response, got: {response}\"\n",
        "\n",
        "\n",
        "# Parameterized test for ticket classification\n",
        "@pytest.mark.parametrize(\"issue_description,expected_category\", [\n",
        "    (\"My laptop screen is cracked and won't display anything.\", \"Hardware\"),\n",
        "    (\"I forgot my password and can't log into the system.\", \"Access\"),\n",
        "    (\"The company website is loading very slowly for all users.\", \"Network\"),\n",
        "    (\"Excel keeps crashing when I try to open large files.\", \"Software\"),\n",
        "    (\"I need permission to access the shared finance folder.\", \"Access\"),\n",
        "])\n",
        "def test_ticket_classification(issue_description, expected_category):\n",
        "    \"\"\"\n",
        "    Test: Can the LLM correctly categorize different types of IT issues?\n",
        "    Categories: Hardware, Software, Network, Access\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Categorize this IT support issue into ONE of these categories:\n",
        "    Hardware, Software, Network, Access\n",
        "\n",
        "    Issue: {issue_description}\n",
        "\n",
        "    Answer with just the category name.\n",
        "    \"\"\"\n",
        "\n",
        "    response = ask_llm(prompt)\n",
        "    assert expected_category.lower() in response.lower(), \\\n",
        "        f\"Expected category '{expected_category}', but got: {response}\"\n",
        "\n",
        "\n",
        "# Parameterized test for troubleshooting advice\n",
        "# NOTE: We use multiple possible keywords since LLMs may phrase advice differently\n",
        "@pytest.mark.parametrize(\"problem,expected_keywords\", [\n",
        "    (\"User can't connect to WiFi\", [\"wifi\", \"wireless\", \"router\", \"modem\", \"connection\"]),\n",
        "    (\"Printer is offline\", [\"printer\", \"print\", \"device\", \"cable\", \"driver\"]),\n",
        "    (\"Computer is very slow\", [\"slow\", \"memory\", \"cpu\", \"task\", \"process\", \"resource\"]),\n",
        "    (\"Email won't send\", [\"email\", \"smtp\", \"server\", \"account\", \"credential\"]),\n",
        "    (\"Can't install software\", [\"install\", \"permission\", \"administrator\", \"compatibility\", \"space\"]),\n",
        "])\n",
        "def test_troubleshooting_advice(problem, expected_keywords):\n",
        "    \"\"\"\n",
        "    Test: Does the LLM provide relevant troubleshooting advice?\n",
        "    We check if the advice mentions at least ONE relevant keyword.\n",
        "    This is more flexible than checking for exact keywords!\n",
        "    \"\"\"\n",
        "    prompt = f\"Provide one troubleshooting step for this issue: {problem}\"\n",
        "    response = ask_llm(prompt)\n",
        "\n",
        "    response_lower = response.lower()\n",
        "\n",
        "    # Check if ANY of the expected keywords appear in the response\n",
        "    found_keywords = [kw for kw in expected_keywords if kw in response_lower]\n",
        "\n",
        "    assert len(found_keywords) > 0, \\\n",
        "        f\"Expected advice to mention one of {expected_keywords}, but got: {response}\"\n",
        "\n",
        "    # Also check that we got actual advice (not just echoing the problem)\n",
        "    assert len(response) > 20, f\"Response too short to be useful advice: {response}\"\n",
        "\n",
        "\n",
        "print(\"âœ… Parameterized test file created: test_parameterized.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens when pytest runs this:\n",
        "  1. Pytest reads `@pytest.mark.parametrize` decorator\n",
        "  2. It sees 5 sets of test data (5 tuples in the list)\n",
        "  3. It runs test_port_knowledge() 5 separate times, each time with different\n",
        "  values for question and expected_answer\n",
        "  4. Each run appears as a separate test case in the report\n",
        "\n"
      ],
      "metadata": {
        "id": "snnEf4OdEQOU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QML2Lo4uvcdZ"
      },
      "outputs": [],
      "source": [
        "# Run the parameterized tests\n",
        "# Notice how ONE test function becomes MANY test cases!\n",
        "!pytest test_parameterized.py -v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG4ESG1PvcdZ"
      },
      "source": [
        "\n",
        "\n",
        "Look at what just happened:\n",
        "- We wrote **3 test functions**\n",
        "- But pytest ran **15 test cases** (5 + 5 + 5)\n",
        "- Each test case appears separately in the report\n",
        "- If one case fails, others still run\n",
        "\n",
        "**Benefits:**\n",
        "- âœï¸ Less code duplication\n",
        "- ðŸ“ Easier to add new test cases (just add to the list)\n",
        "- ðŸ” Clearer which specific inputs failed\n",
        "- ðŸŽ¯ More comprehensive coverage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0RC-1zpvcdZ"
      },
      "source": [
        "## 5. Testing Structured Outputs with Pydantic\n",
        "\n",
        "In real applications, you often need LLMs to return structured data (JSON). But how do you test that the structure is correct?\n",
        "\n",
        "**Pydantic** helps you:\n",
        "1. Define the expected structure (schema)\n",
        "2. Validate that the LLM output matches the schema\n",
        "3. Get type checking and helpful error messages\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Support Ticket Model"
      ],
      "metadata": {
        "id": "umqNCsVhHdvA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAAx1dWxvcdZ"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from typing import Literal\n",
        "import json\n",
        "\n",
        "# Define the structure we expect from the LLM\n",
        "class SupportTicket(BaseModel):\n",
        "    \"\"\"A structured support ticket with all required fields.\"\"\"\n",
        "    ticket_id: str = Field(..., description=\"Ticket ID (e.g., 'TICK-1234')\")\n",
        "    user: str = Field(..., description=\"Full name of the user\")\n",
        "    email: str = Field(..., description=\"User's email address\")\n",
        "    issue: str = Field(..., description=\"Brief description of the issue\")\n",
        "    priority: Literal[\"LOW\", \"MEDIUM\", \"HIGH\"] = Field(..., description=\"Ticket priority\")\n",
        "    category: str = Field(..., description=\"Issue category\")\n",
        "    status: Literal[\"OPEN\", \"IN_PROGRESS\", \"RESOLVED\", \"CLOSED\"] = Field(default=\"OPEN\")\n",
        "\n",
        "# Example: Let's test extraction\n",
        "ticket_text = \"\"\"\n",
        "Ticket ID: TICK-5678\n",
        "From: Alice Johnson (alice.johnson@company.com)\n",
        "Issue: Cannot access VPN from home. Getting \"connection timeout\" error.\n",
        "Priority: HIGH\n",
        "Category: Network/VPN\n",
        "\n",
        "Extract this ticket information as JSON with fields: ticket_id, user, email, issue, priority, category, status.\n",
        "Return ONLY valid JSON, no other text.\n",
        "\"\"\"\n",
        "\n",
        "response = ask_llm(ticket_text)\n",
        "print(\"LLM Response:\")\n",
        "print(response)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Parse and validate\n",
        "try:\n",
        "    data = json.loads(response)\n",
        "    ticket = SupportTicket(**data)\n",
        "    print(\"âœ… Valid ticket structure!\")\n",
        "    print(f\"\\nTicket Details:\")\n",
        "    print(f\"  ID: {ticket.ticket_id}\")\n",
        "    print(f\"  User: {ticket.user}\")\n",
        "    print(f\"  Email: {ticket.email}\")\n",
        "    print(f\"  Issue: {ticket.issue}\")\n",
        "    print(f\"  Priority: {ticket.priority}\")\n",
        "    print(f\"  Category: {ticket.category}\")\n",
        "    print(f\"  Status: {ticket.status}\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"âŒ Invalid JSON: {e}\")\n",
        "except ValidationError as e:\n",
        "    print(f\"âŒ Validation failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb3prztJvcdZ"
      },
      "source": [
        "### Why Pydantic is Powerful for Testing\n",
        "\n",
        "Pydantic automatically checks:\n",
        "- âœ… All required fields are present\n",
        "- âœ… Field types are correct (string, int, etc.)\n",
        "- âœ… Values match constraints (e.g., priority must be LOW/MEDIUM/HIGH)\n",
        "- âœ… Provides clear error messages when validation fails\n",
        "\n",
        "**Without Pydantic:** You'd need to manually write checks for each field.  \n",
        "**With Pydantic:** One line (`SupportTicket(**data)`) does it all! ðŸŽ‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RHqsGFKvcdZ"
      },
      "source": [
        "## 6. Generating Test Reports\n",
        "\n",
        "Pytest can generate beautiful HTML reports showing all test results.\n",
        "\n",
        "Let's run all our tests and create a report:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHnvNi4Jvcda"
      },
      "outputs": [],
      "source": [
        "# Run all tests and generate an HTML report\n",
        "!pytest test_it_support.py test_parameterized.py -v --html=report.html --self-contained-html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsFx6g03vcda"
      },
      "source": [
        "The report is saved as `report.html`. You can download it from Colab and open it in your browser to see:\n",
        "- Summary of passed/failed tests\n",
        "- Execution time for each test\n",
        "- Detailed error messages for failures\n",
        "- Environment information\n",
        "\n",
        "This is perfect for sharing test results with your team! ðŸ“Š"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WYaBxA_vcda"
      },
      "source": [
        "## 7. Exercises ðŸŽ“\n",
        "\n",
        "Now it's your turn! Complete these exercises to practice what you've learned.\n",
        "\n",
        "### Exercise 1: Convert Manual Tests to Automated Tests\n",
        "\n",
        "Think of 3 IT support questions you manually tested in ChatGPT. Convert them to automated pytest functions.\n",
        "\n",
        "**Example topics:**\n",
        "- What causes a \"DNS not found\" error?\n",
        "- How do you reset a Windows password?\n",
        "- What's the difference between RAM and storage?\n",
        "\n",
        "Write your tests below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB0aUybBvcda"
      },
      "outputs": [],
      "source": [
        "# Exercise 1: Write your 3 tests here\n",
        "\n",
        "def test_your_question_1():\n",
        "    \"\"\"\n",
        "    TODO: Test your first IT support question\n",
        "    \"\"\"\n",
        "    pass  # Replace with your test code\n",
        "\n",
        "def test_your_question_2():\n",
        "    \"\"\"\n",
        "    TODO: Test your second IT support question\n",
        "    \"\"\"\n",
        "    pass  # Replace with your test code\n",
        "\n",
        "def test_your_question_3():\n",
        "    \"\"\"\n",
        "    TODO: Test your third IT support question\n",
        "    \"\"\"\n",
        "    pass  # Replace with your test code\n",
        "\n",
        "# Run your tests\n",
        "# !pytest -v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSULxMPnvcda"
      },
      "source": [
        "### Exercise 2: Parameterized Test for Common IT Issues\n",
        "\n",
        "Create a parameterized test that checks if the LLM provides appropriate solutions for 5 common IT problems.\n",
        "\n",
        "**Requirements:**\n",
        "- Use `@pytest.mark.parametrize`\n",
        "- Test at least 5 different IT issues\n",
        "- Check that solutions contain relevant keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmrBm5lGvcda"
      },
      "outputs": [],
      "source": [
        "%%writefile exercise_2.py\n",
        "# Exercise 2: Your parameterized test here\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pytest\n",
        "\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def ask_llm(prompt: str) -> str:\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-5-nano\",\n",
        "        input=prompt\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "# TODO: Write your parameterized test here\n",
        "# @pytest.mark.parametrize(...)\n",
        "# def test_it_solutions(...):\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1RGvlnbvcda"
      },
      "outputs": [],
      "source": [
        "# Run your parameterized test\n",
        "# !pytest exercise_2.py -v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdQ--Tltvcda"
      },
      "source": [
        "### Exercise 3: Pydantic Model for Incident Report\n",
        "\n",
        "Create a Pydantic model for an `IncidentReport` and test if the LLM can extract it correctly.\n",
        "\n",
        "**Required fields:**\n",
        "- `incident_id`: string\n",
        "- `reporter`: string\n",
        "- `timestamp`: string\n",
        "- `severity`: \"MINOR\" | \"MAJOR\" | \"CRITICAL\"\n",
        "- `affected_systems`: list of strings\n",
        "- `description`: string\n",
        "- `resolution_time_estimate`: string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvVaDYH0vcda"
      },
      "outputs": [],
      "source": [
        "# Exercise 3: Define your Pydantic model and test\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Literal\n",
        "import json\n",
        "\n",
        "# TODO: Define IncidentReport model\n",
        "class IncidentReport(BaseModel):\n",
        "    pass  # Add your fields here\n",
        "\n",
        "# TODO: Write a test that:\n",
        "# 1. Provides incident text to the LLM\n",
        "# 2. Asks LLM to extract as JSON\n",
        "# 3. Validates with your IncidentReport model\n",
        "\n",
        "def test_incident_extraction():\n",
        "    pass  # Your test code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB-v34xXvcda"
      },
      "source": [
        "### Exercise 4: Consistency Testing\n",
        "\n",
        "Test if the LLM gives consistent troubleshooting advice across 3 runs.\n",
        "\n",
        "**Requirements:**\n",
        "- Pick an IT issue (e.g., \"Computer won't start\")\n",
        "- Ask for troubleshooting steps 3 times\n",
        "- Check that all 3 responses mention the same key concepts\n",
        "- Hint: Define a set of expected keywords and check they appear in all responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUzwuqg-vcda"
      },
      "outputs": [],
      "source": [
        "# Exercise 4: Consistency test\n",
        "\n",
        "def test_troubleshooting_consistency():\n",
        "    \"\"\"\n",
        "    TODO: Test consistency of troubleshooting advice\n",
        "    \"\"\"\n",
        "    pass  # Your test code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3eT94mavcda"
      },
      "source": [
        "### Exercise 5: Intentionally Failing Tests\n",
        "\n",
        "Write 3 tests that will FAIL because they test edge cases or difficult scenarios.\n",
        "\n",
        "**Example edge cases:**\n",
        "- Ambiguous ticket descriptions\n",
        "- Mixed priority signals in text\n",
        "- Tickets with missing information\n",
        "- Very technical jargon\n",
        "\n",
        "Then, improve your prompts to make the tests pass!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baohSWTMvcda"
      },
      "outputs": [],
      "source": [
        "# Exercise 5: Write failing tests, then fix them\n",
        "\n",
        "def test_edge_case_1():\n",
        "    \"\"\"\n",
        "    TODO: Test an edge case that initially fails\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def test_edge_case_2():\n",
        "    \"\"\"\n",
        "    TODO: Test another edge case\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def test_edge_case_3():\n",
        "    \"\"\"\n",
        "    TODO: Test a third edge case\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# Step 1: Run and watch them fail\n",
        "# Step 2: Improve your prompts\n",
        "# Step 3: Run again and see them pass!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7m9O5xCvcdb"
      },
      "source": [
        "## 9. Best Practices for LLM Testing\n",
        "\n",
        "### âœ… DO:\n",
        "\n",
        "1. **Write clear, specific prompts** - Reduces ambiguity\n",
        "2. **Test one thing at a time** - Easier to debug failures\n",
        "3. **Use descriptive test names** - `test_ticket_severity()` not `test_1()`\n",
        "4. **Add helpful assertion messages** - Explain what you expected\n",
        "5. **Test edge cases** - Empty inputs, very long inputs, ambiguous cases\n",
        "6. **Use Pydantic for structured outputs** - Automatic validation\n",
        "7. **Group related tests** - Use separate test files for different features\n",
        "8. **Check for key concepts, not exact strings** - LLMs may phrase answers differently\n",
        "\n",
        "### âŒ DON'T:\n",
        "\n",
        "1. **Don't expect exact string matches** - LLMs vary in phrasing\n",
        "2. **Don't test creative tasks too strictly** - Allow for variation\n",
        "3. **Don't ignore flaky tests** - Investigate and fix them\n",
        "4. **Don't test too many things in one test** - Keep tests focused\n",
        "5. **Don't forget to test error cases** - What if API fails?\n",
        "\n",
        "### Testing Strategy\n",
        "\n",
        "For each LLM feature, test:\n",
        "1. **Happy path** - Normal, expected inputs\n",
        "2. **Edge cases** - Unusual but valid inputs\n",
        "3. **Error cases** - Invalid inputs\n",
        "4. **Consistency** - Check for key concepts across multiple runs\n",
        "5. **Performance** - Response time (if critical)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qvLaTabvcdb"
      },
      "source": [
        "## 10. Key Takeaways & Next Steps\n",
        "\n",
        "### ðŸŽ‰ What You've Learned\n",
        "\n",
        "1. **Manual â†’ Automated**: You transformed your ChatGPT testing workflow into automated Python tests\n",
        "2. **Pytest basics**: Writing tests, running test suites, reading output\n",
        "3. **Parameterization**: Testing multiple scenarios with minimal code\n",
        "4. **Structured testing**: Using Pydantic to validate LLM outputs\n",
        "5. **Best practices**: How to write maintainable, effective LLM tests\n",
        "\n",
        "\n",
        "### ðŸ’¡ Pro Tips\n",
        "\n",
        "1. **Start small**: Begin with a few critical tests, then expand\n",
        "2. **Run tests often**: Integrate into your development workflow\n",
        "3. **Track test coverage**: Aim for 80%+ coverage of critical paths\n",
        "4. **Share reports**: Use HTML reports to communicate with non-technical team members\n",
        "5. **Iterate on prompts**: Use failing tests to improve your prompts\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ Additional Resources\n",
        "\n",
        "- [Pytest Documentation](https://docs.pytest.org/)\n",
        "- [Pydantic Documentation](https://docs.pydantic.dev/)\n",
        "- [OpenAI API Documentation](https://platform.openai.com/docs/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfBc1hnkymIt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}