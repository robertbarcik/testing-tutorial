{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpylwiSTvcdV"
      },
      "source": [
        "# LLM Testing: From Manual to Automated\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "1. Understand why automated testing is essential for LLM applications\n",
        "2. Convert manual tests (like those you did in ChatGPT) into automated Python tests\n",
        "3. Use pytest to organize and run test suites\n",
        "4. Write parameterized tests to test multiple scenarios efficiently\n",
        "5. Validate structured outputs using Pydantic models\n",
        "6. Interpret test results and debug failures\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Context: What You've Done Before (Manual Testing)\n",
        "\n",
        "Previously, you manually tested LLMs in ChatGPT by:\n",
        "- Asking factual questions and checking answers\n",
        "- Testing prompt sensitivity by rephrasing questions\n",
        "- Checking for bias in responses\n",
        "- Evaluating consistency across multiple queries\n",
        "\n",
        "### Why This Doesn't Scale\n",
        "\n",
        "Imagine you're building an **IT Support Chatbot** for a company helpdesk. You need to test:\n",
        "- ‚úÖ Does it correctly identify ticket severity?\n",
        "- ‚úÖ Does it provide accurate troubleshooting steps?\n",
        "- ‚úÖ Does it extract ticket information correctly?\n",
        "- ‚úÖ Is it consistent across similar queries?\n",
        "\n",
        "**Problem:** Testing these manually every time you update your prompt or model is:\n",
        "- ‚è∞ Time-consuming\n",
        "- üêõ Error-prone\n",
        "- üìà Not scalable (what about 100 test cases?)\n",
        "- üîÑ Hard to reproduce\n",
        "\n",
        "### The Solution: Automated Testing\n",
        "\n",
        "**Same tests you did in ChatGPT, now in code!**\n",
        "\n",
        "Benefits:\n",
        "- üöÄ Run hundreds of tests in seconds\n",
        "- üîÅ Reproducible results\n",
        "- ü§ñ Integrate into CI/CD pipelines\n",
        "- üìä Generate test reports automatically\n",
        "- üõ°Ô∏è Catch regressions when you change prompts or models\n",
        "\n",
        "---\n",
        "\n",
        "Let's get started! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SH1nSnzvcdW"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, we'll install the required packages and set up our OpenAI API access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAFKccVVvcdX"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install openai pytest==8.3.4 pytest-html==4.1.1 pydantic>=2.11.0 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD3vNVKTvcdX"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pytest\n",
        "from pydantic import BaseModel, Field\n",
        "import json\n",
        "from typing import List, Optional\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9AQgUETvcdX"
      },
      "source": [
        "### API Key Setup\n",
        "\n",
        "You'll need an OpenAI API key to run these tests.\n",
        "\n",
        "**How to get your API key:**\n",
        "1. Go to [platform.openai.com](https://platform.openai.com)\n",
        "2. Sign in or create an account\n",
        "3. Navigate to API Keys section\n",
        "4. Create a new secret key\n",
        "5. Copy it and use it below\n",
        "\n",
        "**Two ways to provide your API key:**\n",
        "\n",
        "**Option 1: Colab Secrets (Recommended - More Secure)**\n",
        "- Click the üîë key icon in the left sidebar\n",
        "- Add a new secret with name: `OPENAI_API_KEY`\n",
        "- Paste your API key as the value\n",
        "- Enable \"Notebook access\" toggle\n",
        "- Run the cell below - it will automatically load from secrets\n",
        "\n",
        "**Option 2: Enter when prompted**\n",
        "- Just run the cell below\n",
        "- You'll be prompted to enter your API key\n",
        "- The key will be hidden as you type\n",
        "\n",
        "**üí∞ Cost Note:** We'll use the `gpt-5-nano` model, which is very cost-effective for testing. These examples will cost less than $0.01 to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w75U7cnRvcdX"
      },
      "outputs": [],
      "source": [
        "# Configure OpenAI API key\n",
        "# Method 1: Try to get API key from Colab secrets (recommended)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
        "except:\n",
        "    # Method 2: Manual input (fallback)\n",
        "    from getpass import getpass\n",
        "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
        "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "# Set the API key as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# Validate that the API key is set\n",
        "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
        "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
        "\n",
        "print(\"‚úÖ Authentication configured!\")\n",
        "\n",
        "# Configure which OpenAI model to use\n",
        "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
        "print(f\"ü§ñ Selected Model: {OPENAI_MODEL}\")\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPd7UhilvcdY"
      },
      "source": [
        "## 2. Your First Automated Test\n",
        "\n",
        "Let's start simple. Remember when you manually tested factual accuracy in ChatGPT in the Basic Testing document? You asked questions like \"What is the capital of Australia?\" and checked if the response was \"Canberra\".\n",
        "\n",
        "**Manual Test (What you did in 00_Basic_Testing.md):**\n",
        "1. Open ChatGPT\n",
        "2. Type: \"What is the capital of Australia?\"\n",
        "3. Read response\n",
        "4. Check if it says \"Canberra\" (not Sydney!)\n",
        "\n",
        "**Automated Test (What we'll do now):**\n",
        "Same thing, but in code! This means:\n",
        "- You don't have to manually type the question each time\n",
        "- The checking happens automatically\n",
        "- You can run this test hundreds of times in seconds\n",
        "- You get consistent, reproducible results\n",
        "\n",
        "\n",
        "We will create a helper function that send your question to the LLM API and returns the text response.\n",
        "\n",
        "  Think of this function as similar to a REST API client in traditional automated testing. Just as you might use `requests.post()` to call a web service and get JSON back, this function calls OpenAI's API with your prompt and receives the LLM's text response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBBKV_v2vcdY"
      },
      "outputs": [],
      "source": [
        "# Helper function to call the LLM\n",
        "def ask_llm(prompt: str, model: str = \"gpt-5-nano\") -> str:\n",
        "    \"\"\"\n",
        "    Send a prompt to the LLM and return the response.\n",
        "\n",
        "    Args:\n",
        "        prompt: The question or instruction to send to the LLM\n",
        "        model: The model to use (default: gpt-5-nano)\n",
        "\n",
        "    Returns:\n",
        "        The LLM's response as a string\n",
        "    \"\"\"\n",
        "    response = client.responses.create(\n",
        "        model=model,\n",
        "        input=prompt\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "# Test it out!\n",
        "response = ask_llm(\"What is the capital of Australia?\")\n",
        "print(f\"LLM Response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBkEU3NcvcdY"
      },
      "source": [
        "### Plain Python Test Function\n",
        "\n",
        "Now we will create a test function that automate what you previously did manually in ChatGPT: it\n",
        "- asks 'What is the capital of Australia?'\n",
        "- receives the LLM's response\n",
        "- automatically checks whether 'Canberra' appears in the answer\n",
        "\n",
        "Notice the structure follows the AAA pattern (Arrange-Act-Assert):\n",
        " - **Arrange**: We prepare our test input ‚Äî the question string, just like\n",
        "  setting up test data in a unit test\n",
        "  - **Act**: We execute the action being tested ‚Äî calling the LLM, similar\n",
        "  to invoking the function or API endpoint you're testing\n",
        "  - **Assert**: We verify the result meets our expectations using\n",
        "  Python's assert statement\n",
        "\n",
        "\n",
        "‚ùó The key difference from testing traditional software is that we check for the\n",
        "   presence of the correct answer ('Canberra' in response) rather than exact\n",
        "  string equality, because LLMs generate natural language that varies in\n",
        "  phrasing. LLMs might say 'The capital is Canberra' or 'Canberra is the capital\n",
        "   city', but both contain the factually correct answer we're looking for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zNf5BRKvcdY"
      },
      "outputs": [],
      "source": [
        "def test_australia_capital_knowledge():\n",
        "    \"\"\"\n",
        "    Test: Does the LLM know what the capital of Australia is?\n",
        "    Expected: Response should contain \"Canberra\" (not Sydney!)\n",
        "\n",
        "    This is the automated version of the manual test from 00_Basic_Testing.md\n",
        "    where we tested if the LLM correctly identifies Canberra as the capital.\n",
        "    \"\"\"\n",
        "    # Arrange: Set up the test data (same question from manual testing)\n",
        "    question = \"What is the capital of Australia?\"\n",
        "\n",
        "    # Act: Call the LLM (same as typing in ChatGPT, but automated)\n",
        "    response = ask_llm(question)\n",
        "\n",
        "    # Assert: Check if the response is correct\n",
        "    # We check that \"Canberra\" appears in the response\n",
        "    assert \"Canberra\" in response, f\"Expected 'Canberra' in response, but got: {response}\"\n",
        "\n",
        "    print(\"‚úÖ Test passed! LLM correctly identified Canberra as the capital of Australia\")\n",
        "\n",
        "# Run the test\n",
        "test_australia_capital_knowledge()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niH1_MznvcdY"
      },
      "source": [
        "## 3. Building a Test Suite with Pytest\n",
        "\n",
        "In traditional software testing, you might organize related tests into a\n",
        "  test suite. For example, all API tests in one file, all database tests in\n",
        "  another.\n",
        "\n",
        "We'll do the same here: organize all IT support chatbot tests into one file called `test_it_support.py`. Pytest will automatically discover and\n",
        "  run all functions that start with `test_`, just like how testing frameworks\n",
        "  like JUnit or NUnit discover test methods with @Test annotations.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKqMoJGVvcdY"
      },
      "outputs": [],
      "source": [
        "%%writefile test_it_support.py\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pytest\n",
        "import json\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def ask_llm(prompt: str, model: str = \"gpt-5-nano\") -> str:\n",
        "    \"\"\"Send a prompt to the LLM and return the response.\"\"\"\n",
        "    response = client.responses.create(\n",
        "        model=model,\n",
        "        input=prompt\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "\n",
        "# Test 1: Technical Fact Testing\n",
        "def test_technical_fact():\n",
        "    \"\"\"\n",
        "    Test: Does the LLM know basic IT facts?\n",
        "    This is like asking factual questions in ChatGPT and checking the answer.\n",
        "    \"\"\"\n",
        "    question = \"What is the default SSH port? Answer with just the number.\"\n",
        "    response = ask_llm(question)\n",
        "\n",
        "    # Check if response contains the correct port\n",
        "    assert \"22\" in response, f\"Expected SSH port 22, but got: {response}\"\n",
        "\n",
        "\n",
        "# Test 2: Ticket Severity Assessment\n",
        "def test_ticket_severity_assessment():\n",
        "    \"\"\"\n",
        "    Test: Can the LLM correctly assess ticket severity?\n",
        "    Critical issues should be flagged as HIGH priority.\n",
        "    \"\"\"\n",
        "    ticket_description = \"\"\"\n",
        "    Ticket: Database server is completely down. All employees cannot access\n",
        "    customer data. Production environment is affected.\n",
        "\n",
        "    Classify this ticket's severity as: LOW, MEDIUM, or HIGH.\n",
        "    Answer with just one word.\n",
        "    \"\"\"\n",
        "\n",
        "    response = ask_llm(ticket_description)\n",
        "\n",
        "    # A complete database outage should be HIGH severity\n",
        "    assert \"HIGH\" in response.upper(), f\"Expected HIGH severity, but got: {response}\"\n",
        "\n",
        "\n",
        "# Test 3: Consistency Testing\n",
        "def test_consistency():\n",
        "    \"\"\"\n",
        "    Test: Does the LLM give consistent answers to the same question?\n",
        "    This is like asking the same question multiple times in ChatGPT.\n",
        "    \"\"\"\n",
        "    question = \"What is the purpose of a firewall in network security? Answer in one sentence.\"\n",
        "\n",
        "    # Ask the same question twice\n",
        "    response1 = ask_llm(question)\n",
        "    response2 = ask_llm(question)\n",
        "\n",
        "    # Check that both responses mention key concepts about firewalls\n",
        "    assert \"firewall\" in response1.lower(), f\"Response 1 missing 'firewall': {response1}\"\n",
        "    assert \"firewall\" in response2.lower(), f\"Response 2 missing 'firewall': {response2}\"\n",
        "\n",
        "    # Check semantic similarity (both should mention blocking/filtering/protecting)\n",
        "    keywords = [\"block\", \"filter\", \"control\", \"protect\", \"monitor\"]\n",
        "    assert any(kw in response1.lower() for kw in keywords), f\"Response 1 missing key concepts: {response1}\"\n",
        "    assert any(kw in response2.lower() for kw in keywords), f\"Response 2 missing key concepts: {response2}\"\n",
        "\n",
        "\n",
        "# Test 4: Structured Output (JSON)\n",
        "def test_structured_output_json():\n",
        "    \"\"\"\n",
        "    Test: Can the LLM extract structured information from a ticket?\n",
        "    This tests if the LLM can parse unstructured text into structured data.\n",
        "    \"\"\"\n",
        "    ticket_text = \"\"\"\n",
        "    Ticket #7823: User Sarah Chen reports that she cannot print from her laptop.\n",
        "    The printer shows as offline. Priority: Medium. Category: Hardware.\n",
        "\n",
        "    Extract the following information as JSON:\n",
        "    - ticket_id\n",
        "    - user_name\n",
        "    - issue_summary\n",
        "    - priority\n",
        "    - category\n",
        "\n",
        "    Return ONLY valid JSON, no other text.\n",
        "    \"\"\"\n",
        "\n",
        "    response = ask_llm(ticket_text)\n",
        "\n",
        "    # Parse the JSON response\n",
        "    try:\n",
        "        data = json.loads(response)\n",
        "    except json.JSONDecodeError:\n",
        "        pytest.fail(f\"Response is not valid JSON: {response}\")\n",
        "\n",
        "    # Verify all required fields are present\n",
        "    required_fields = [\"ticket_id\", \"user_name\", \"issue_summary\", \"priority\", \"category\"]\n",
        "    for field in required_fields:\n",
        "        assert field in data, f\"Missing required field: {field}\"\n",
        "\n",
        "    # Verify correctness of extracted data\n",
        "    assert \"7823\" in str(data[\"ticket_id\"]), f\"Wrong ticket_id: {data['ticket_id']}\"\n",
        "    assert \"Sarah\" in data[\"user_name\"] or \"Chen\" in data[\"user_name\"], f\"Wrong user: {data['user_name']}\"\n",
        "\n",
        "\n",
        "print(\"‚úÖ Test file created: test_it_support.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down the 4 tests we just created. Each one tests a different capability of our IT Support chatbot:\n",
        "\n",
        "**Test 1: `test_technical_fact()` ‚Äî Basic Fact Checking**\n",
        "\n",
        "- **What it tests:** Can the LLM recall basic IT knowledge (SSH port number)?\n",
        "- **Why it matters:** Your chatbot needs accurate factual knowledge to help users. If it can't remember that SSH uses port 22, it can't provide reliable troubleshooting advice.\n",
        "- **Testing approach:** We ask a specific factual question and check if the correct answer (\"22\") appears anywhere in the response.\n",
        "- **Traditional testing parallel:** Like testing if a configuration API returns the correct default port values.\n",
        "\n",
        "---\n",
        "\n",
        "**Test 2: `test_ticket_severity_assessment()` ‚Äî Classification Logic**\n",
        "\n",
        "- **What it tests:** Can the LLM correctly evaluate the severity of an IT issue?\n",
        "- **Why it matters:** In a real helpdesk system, tickets need proper prioritization. A complete database outage affecting all employees should always be classified as HIGH priority, not MEDIUM or LOW.\n",
        "- **Testing approach:** We provide a clearly critical scenario (database down, production affected) and verify the LLM classifies it as HIGH severity.\n",
        "- **Traditional testing parallel:** Like testing a business rules engine that categorizes insurance claims by risk level‚Äîyou need to verify it correctly identifies high-risk scenarios.\n",
        "\n",
        "---\n",
        "\n",
        "**Test 3: `test_consistency()` ‚Äî Stability Check**\n",
        "\n",
        "- **What it tests:** Does the LLM give semantically consistent answers when asked the same question multiple times?\n",
        "- **Why it matters:** Users expect reliable answers. If someone asks \"What is a firewall?\" twice and gets contradictory explanations, they'll lose trust in your chatbot.\n",
        "- **Testing approach:** We ask the same question twice and verify both responses mention core concepts (the word \"firewall\" itself, plus relevant actions like \"block\" or \"protect\"). We don't require identical wording‚Äîjust consistent meaning.\n",
        "- **Traditional testing parallel:** Like running the same API request 10 times and verifying you get logically equivalent results each time (same data, possibly different JSON formatting).\n",
        "\n",
        "---\n",
        "\n",
        "**Test 4: `test_structured_output_json()` ‚Äî Data Extraction**\n",
        "\n",
        "- **What it tests:** Can the LLM extract structured information from unstructured text and return it as valid JSON?\n",
        "- **Why it matters:** Real systems need structured data for databases, APIs, and workflows. If a user submits a ticket via email or chat, you need to extract the ticket ID, username, issue description, priority, and category into a database record.\n",
        "- **Testing approach:** We provide a natural language ticket description and ask the LLM to extract specific fields as JSON. We then verify:\n",
        "  - (1) the response is valid JSON (parseable),\n",
        "  - (2) all required fields are present,\n",
        "  - (3) the extracted values are correct (ticket_id contains \"7823\", user_name contains \"Sarah\" or \"Chen\").\n",
        "- **Traditional testing parallel:** Like testing a parser that extracts structured order data from customer emails‚Äîyou verify it correctly identifies order number, customer name, items, and shipping address.\n",
        "\n",
        "**Note about Test 4:** We're manually checking each field one by one. This works, but imagine doing this for 10+ fields across 20+ tests. That's a lot of repetitive code! In Section 5, we'll learn a better way to validate structured outputs using **Pydantic**."
      ],
      "metadata": {
        "id": "UWUq831as9Rg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHSsIJEwvcdZ"
      },
      "source": [
        "### Run the Test Suite\n",
        "\n",
        "Now let's run all 4 tests with pytest!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMcuqfDZvcdZ"
      },
      "outputs": [],
      "source": [
        "# Run pytest with verbose output\n",
        "# -v = verbose (show each test name)\n",
        "# -s = show print statements\n",
        "!pytest test_it_support.py -v -s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_oFyjOFvcdZ"
      },
      "source": [
        "You just ran 4 automated tests! Notice how pytest:\n",
        "- Discovered all functions starting with `test_`\n",
        "- Ran each test independently\n",
        "- Showed which passed or failed\n",
        "\n",
        "**This is the same testing you did manually in ChatGPT, but now:**\n",
        "- ‚ö° Takes seconds instead of minutes\n",
        "- üîÅ Perfectly reproducible\n",
        "- üìä Generates reports automatically\n",
        "- ü§ñ Can run in CI/CD pipelines\n",
        "\n",
        "-------\n",
        "\n",
        "**üéì Understanding Test Flakiness in LLM Testing**\n",
        "\n",
        "  **Did Test 3 fail when you ran it?** If so, congratulations, you just\n",
        "  experienced your first **flaky LLM test**!\n",
        "\n",
        "  Take a look at the failure message. You'll likely see something like:\n",
        "\n",
        "```python\n",
        "> assert \"firewall\" in response1.lower(), f\"Response 1 missing 'firewall': {response1}\"\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "  **What happened?**\n",
        "  - The LLM probably gave a **correct, accurate definition** of a firewall\n",
        "  - But it didn't use the word \"firewall\" in its response\n",
        "  - Our test checked for the word \"firewall\", so it failed\n",
        "\n",
        "  **Is this a bug in our test?**\n",
        "No! Checking for \"firewall\" is actually a **good practice** in professional\n",
        "   LLM testing. We want responses to be **grounded** in the question's terminology. If someone asks \"What is a firewall?\" we expect the answer to mention\n",
        "  \"firewall\"\n",
        "\n",
        "  **Why does this happen?**\n",
        "  - LLMs are **non-deterministic** - they generate slightly different responses\n",
        "   each time\n",
        "  - Sometimes they avoid repetition (won't say \"A firewall is a firewall\n",
        "  that...\")\n",
        "  - The same prompt can yield different (but equally correct) answers\n",
        "\n",
        "\n",
        "\n",
        "  This is a **real challenge** in production LLM systems. We can handle it in\n",
        "  several ways:\n",
        "\n",
        "  1. **Accept the flakiness** - Run the test multiple times in CI/CD\n",
        "  2. **Loosen the assertions** - Check for functional keywords instead of\n",
        "  \"firewall\"\n",
        "  3. **Improve the prompt** - Make it more likely the term appears (\"In one\n",
        "  sentence, explain: What does a **firewall** do?\")\n",
        "  4. **Use probabilistic testing** - Run test 10 times, accept 8/10 pass rate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFuenFFdvcdZ"
      },
      "source": [
        "## 4. Parameterized Testing\n",
        "\n",
        "  Imagine you want to test if your IT support chatbot knows standard network\n",
        "  ports. You could write separate test functions like this:\n",
        "\n",
        "  ```python\n",
        "  def test_http_port():\n",
        "      response = ask_llm(\"What port does HTTP use?\")\n",
        "      assert \"80\" in response\n",
        "\n",
        "  def test_https_port():\n",
        "      response = ask_llm(\"What port does HTTPS use?\")\n",
        "      assert \"443\" in response\n",
        "\n",
        "  def test_ssh_port():\n",
        "      response = ask_llm(\"What port does SSH use?\")\n",
        "      assert \"22\" in response\n",
        "\n",
        "  def test_ftp_port():\n",
        "      response = ask_llm(\"What port does FTP use?\")\n",
        "      assert \"21\" in response\n",
        "\n",
        "  def test_smtp_port():\n",
        "      response = ask_llm(\"What port does SMTP use?\")\n",
        "      assert \"25\" in response\n",
        " ```\n",
        "\n",
        "However, problems with this approach are:\n",
        "  - üîÅ Repetitive code: Same test logic repeated 5 times\n",
        "  - üìù Hard to maintain: If you need to change the test logic, you must update\n",
        "  5 functions\n",
        "  - üò´ Tedious to expand: Adding a new port test means writing another entire\n",
        "  function\n",
        "  - üìä Poor reporting: If tests fail, you can't easily see patterns (e.g., \"LLM\n",
        "   knows common ports but fails on less common ones\")\n",
        "\n",
        "**The Solution: Parameterized Tests**\n",
        "\n",
        "Parameterized tests let you write the test logic once and run it with multiple sets of data. Think of it like a loop, but specifically designed for testing.\n",
        "\n",
        "Traditional testing parallel:\n",
        "  - In JUnit (Java), you'd use @ParameterizedTest with `@ValueSource` or\n",
        "  `@CsvSource`\n",
        "  - In NUnit (C#), you'd use `[TestCase]` attributes\n",
        "  - In pytest (Python), you use `@pytest.mark.parametrize`\n",
        "\n",
        "**How It Works**\n",
        "\n",
        "Instead of 5 separate functions, you write ONE function with a decorator that supplies different test data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Testing Multiple IT Knowledge Questions\n",
        "\n",
        "We will write three parameterized tests. Let's break down what each test does:\n",
        "\n",
        "**Test 1: `test_port_knowledge()`** ‚Äî Knowledge Testing at Scale\n",
        "\n",
        "  **What it tests:** Does the LLM know standard network port numbers for 5\n",
        "  common protocols?\n",
        "\n",
        "  **Why parameterize?**\n",
        "  - Testing one port proves nothing about the LLM's broader knowledge\n",
        "  - Testing 5 ports reveals if it knows common protocols consistently\n",
        "  - Easy to expand: add DNS (port 53), MySQL (3306), etc. by adding one line\n",
        "\n",
        "  **The test cases:**\n",
        "  - HTTP ‚Üí 80\n",
        "  - HTTPS ‚Üí 443\n",
        "  - SSH ‚Üí 22\n",
        "  - FTP ‚Üí 21\n",
        "  - SMTP ‚Üí 25\n",
        "\n",
        "**Test 2: `test_ticket_classification()`** ‚Äî Multi-Category\n",
        "  Classification\n",
        "\n",
        "  **What it tests:** Can the LLM correctly categorize 5 different IT issues\n",
        "  into the right category (Hardware, Software, Network, Access)?\n",
        "\n",
        "  **Why parameterize?**\n",
        "  - Each IT issue has a clear expected category\n",
        "  - Tests if the LLM can distinguish between issue types (not just memorize one\n",
        "   category)\n",
        "  - Real helpdesks have dozens of categories‚Äîthis approach scales easily\n",
        "\n",
        "  **The test cases:**\n",
        "  - \"Laptop screen is cracked\" ‚Üí Hardware (physical device problem)\n",
        "  - \"Forgot my password\" ‚Üí Access (authentication issue)\n",
        "  - \"Website loading slowly for all users\" ‚Üí Network (connectivity problem)\n",
        "  - \"Excel keeps crashing\" ‚Üí Software (application issue)\n",
        "  - \"Need permission for shared folder\" ‚Üí Access (authorization issue)\n",
        "\n",
        "  **Why these specific examples?** They test if the LLM can differentiate\n",
        "  between similar-sounding categories (both password reset and folder\n",
        "  permissions are \"Access\" issues, even though one is authentication and the\n",
        "  other is authorization).\n",
        "\n",
        "**Test 3: `test_troubleshooting_advice()`** ‚Äî Flexible Keyword Matching\n",
        "\n",
        "  **What it tests:** Does the LLM provide relevant troubleshooting advice that\n",
        "  mentions appropriate technical concepts?\n",
        "\n",
        "  **Why parameterize?**\n",
        "  - Each IT problem has different relevant keywords\n",
        "  - We're not testing for exact phrasing (too brittle for LLMs)\n",
        "  - We check if the advice is \"in the right ballpark\" by looking for\n",
        "  domain-relevant terms\n",
        "\n",
        "  **The test cases and their keyword lists:**\n",
        "  - \"Can't connect to WiFi\" ‚Üí [\"wifi\", \"wireless\", \"router\", \"modem\",\n",
        "  \"connection\"]\n",
        "  - \"Printer is offline\" ‚Üí [\"printer\", \"print\", \"device\", \"cable\", \"driver\"]\n",
        "  - \"Computer is very slow\" ‚Üí [\"slow\", \"memory\", \"cpu\", \"task\", \"process\",\n",
        "  \"resource\"]\n",
        "  - \"Email won't send\" ‚Üí [\"email\", \"smtp\", \"server\", \"account\", \"credential\"]\n",
        "  - \"Can't install software\" ‚Üí [\"install\", \"permission\", \"administrator\",\n",
        "  \"compatibility\", \"space\"]\n",
        "\n",
        "  **Important difference from Tests 1 & 2:** We check if **ANY** of the\n",
        "  keywords appear (not all of them). Why? Because good troubleshooting advice\n",
        "  might say \"check your WiFi router\" OR \"restart your modem\"‚Äîboth are valid,\n",
        "  but they use different keywords.\n",
        "\n",
        "  **The two-part assertion:**\n",
        "  1. **Keyword check:** `assert len(found_keywords) > 0` ‚Äî Did the advice\n",
        "  mention at least one relevant concept?\n",
        "  2. **Length check:** `assert len(response) > 20` ‚Äî Did the LLM actually give\n",
        "  advice, or just echo the problem back?"
      ],
      "metadata": {
        "id": "5obalmOOEELP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h60j0z8nvcdZ"
      },
      "outputs": [],
      "source": [
        "%%writefile test_parameterized.py\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pytest\n",
        "\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def ask_llm(prompt: str, model: str = \"gpt-5-nano\") -> str:\n",
        "    \"\"\"Send a prompt to the LLM and return the response.\"\"\"\n",
        "    response = client.responses.create(\n",
        "        model=model,\n",
        "        input=prompt\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "\n",
        "# Parameterized test for IT knowledge\n",
        "@pytest.mark.parametrize(\"question,expected_answer\", [\n",
        "    (\"What port does HTTP use? Answer with just the number.\", \"80\"),\n",
        "    (\"What port does HTTPS use? Answer with just the number.\", \"443\"),\n",
        "    (\"What port does SSH use? Answer with just the number.\", \"22\"),\n",
        "    (\"What port does FTP use? Answer with just the number.\", \"21\"),\n",
        "    (\"What port does SMTP use? Answer with just the number.\", \"25\"),\n",
        "])\n",
        "def test_port_knowledge(question, expected_answer):\n",
        "    \"\"\"\n",
        "    Test: Does the LLM know standard network ports?\n",
        "    This ONE test function runs 5 times with different data!\n",
        "    \"\"\"\n",
        "    response = ask_llm(question)\n",
        "    assert expected_answer in response, f\"Expected '{expected_answer}' in response, got: {response}\"\n",
        "\n",
        "\n",
        "# Parameterized test for ticket classification\n",
        "@pytest.mark.parametrize(\"issue_description,expected_category\", [\n",
        "    (\"My laptop screen is cracked and won't display anything.\", \"Hardware\"),\n",
        "    (\"I forgot my password and can't log into the system.\", \"Access\"),\n",
        "    (\"The company website is loading very slowly for all users.\", \"Network\"),\n",
        "    (\"Excel keeps crashing when I try to open large files.\", \"Software\"),\n",
        "    (\"I need permission to access the shared finance folder.\", \"Access\"),\n",
        "])\n",
        "def test_ticket_classification(issue_description, expected_category):\n",
        "    \"\"\"\n",
        "    Test: Can the LLM correctly categorize different types of IT issues?\n",
        "    Categories: Hardware, Software, Network, Access\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Categorize this IT support issue into ONE of these categories:\n",
        "    Hardware, Software, Network, Access\n",
        "\n",
        "    Issue: {issue_description}\n",
        "\n",
        "    Answer with just the category name.\n",
        "    \"\"\"\n",
        "\n",
        "    response = ask_llm(prompt)\n",
        "    assert expected_category.lower() in response.lower(), \\\n",
        "        f\"Expected category '{expected_category}', but got: {response}\"\n",
        "\n",
        "\n",
        "# Parameterized test for troubleshooting advice\n",
        "# NOTE: We use multiple possible keywords since LLMs may phrase advice differently\n",
        "@pytest.mark.parametrize(\"problem,expected_keywords\", [\n",
        "    (\"User can't connect to WiFi\", [\"wifi\", \"wireless\", \"router\", \"modem\", \"connection\"]),\n",
        "    (\"Printer is offline\", [\"printer\", \"print\", \"device\", \"cable\", \"driver\"]),\n",
        "    (\"Computer is very slow\", [\"slow\", \"memory\", \"cpu\", \"task\", \"process\", \"resource\"]),\n",
        "    (\"Email won't send\", [\"email\", \"smtp\", \"server\", \"account\", \"credential\"]),\n",
        "    (\"Can't install software\", [\"install\", \"permission\", \"administrator\", \"compatibility\", \"space\"]),\n",
        "])\n",
        "def test_troubleshooting_advice(problem, expected_keywords):\n",
        "    \"\"\"\n",
        "    Test: Does the LLM provide relevant troubleshooting advice?\n",
        "    We check if the advice mentions at least ONE relevant keyword.\n",
        "    This is more flexible than checking for exact keywords!\n",
        "    \"\"\"\n",
        "    prompt = f\"Provide one troubleshooting step for this issue: {problem}\"\n",
        "    response = ask_llm(prompt)\n",
        "\n",
        "    response_lower = response.lower()\n",
        "\n",
        "    # Check if ANY of the expected keywords appear in the response\n",
        "    found_keywords = [kw for kw in expected_keywords if kw in response_lower]\n",
        "\n",
        "    assert len(found_keywords) > 0, \\\n",
        "        f\"Expected advice to mention one of {expected_keywords}, but got: {response}\"\n",
        "\n",
        "    # Also check that we got actual advice (not just echoing the problem)\n",
        "    assert len(response) > 20, f\"Response too short to be useful advice: {response}\"\n",
        "\n",
        "\n",
        "print(\"‚úÖ Parameterized test file created: test_parameterized.py\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens when pytest runs this:\n",
        "  1. Pytest reads `@pytest.mark.parametrize` decorator\n",
        "  2. It sees 5 sets of test data (5 tuples in the list)\n",
        "  3. It runs test_port_knowledge() 5 separate times, each time with different\n",
        "  values for question and expected_answer\n",
        "  4. Each run appears as a separate test case in the report\n",
        "\n"
      ],
      "metadata": {
        "id": "snnEf4OdEQOU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QML2Lo4uvcdZ"
      },
      "outputs": [],
      "source": [
        "# Run the parameterized tests\n",
        "# Notice how ONE test function becomes MANY test cases!\n",
        "!pytest test_parameterized.py -v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG4ESG1PvcdZ"
      },
      "source": [
        "\n",
        "\n",
        "Look at what just happened:\n",
        "- We wrote **3 test functions**\n",
        "- But pytest ran **15 test cases** (5 + 5 + 5)\n",
        "- Each test case appears separately in the report\n",
        "- If one case fails, others still run\n",
        "\n",
        "**Benefits:**\n",
        "- ‚úçÔ∏è Less code duplication\n",
        "- üìù Easier to add new test cases (just add to the list)\n",
        "- üîç Clearer which specific inputs failed\n",
        "- üéØ More comprehensive coverage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0RC-1zpvcdZ"
      },
      "source": [
        "## 5. Testing Structured Outputs with Pydantic\n",
        "\n",
        "### Remember Test 4 from Section 3?\n",
        "\n",
        "In that test, we validated JSON output manually:\n",
        "\n",
        "```python\n",
        "# We had to check EVERYTHING by hand:\n",
        "required_fields = [\"ticket_id\", \"user_name\", \"issue_summary\", \"priority\", \"category\"]\n",
        "for field in required_fields:\n",
        "    assert field in data, f\"Missing required field: {field}\"\n",
        "\n",
        "assert \"7823\" in str(data[\"ticket_id\"])\n",
        "assert \"Sarah\" in data[\"user_name\"] or \"Chen\" in data[\"user_name\"]\n",
        "```\n",
        "\n",
        "**That works, but imagine doing this for:**\n",
        "- 10 fields instead of 5\n",
        "- 20 different test functions\n",
        "- Complex constraints (email format, value ranges, enums like \"LOW\"/\"MEDIUM\"/\"HIGH\")\n",
        "\n",
        "You'd write HUNDREDS of lines of repetitive validation code. There's a better way that we will show you in this section.\n",
        "\n",
        "---\n",
        "\n",
        "Also, we've mostly tested LLMs that return **natural language text**:\n",
        "- \"The capital of Australia is Canberra\"\n",
        "- \"SSH uses port 22\"\n",
        "- \"This ticket should be classified as HIGH priority\"\n",
        "\n",
        "But real-world applications need **structured data** for:\n",
        "- üíæ **Saving to databases** (INSERT INTO tickets VALUES ...)\n",
        "- üîÑ **API integrations** (sending JSON to other systems)\n",
        "- üìä **Data processing** (filtering, sorting, aggregating)\n",
        "- üéØ **Workflow automation** (if priority == \"HIGH\", send alert)\n",
        "\n",
        "**The problem:** LLMs are creative and inconsistent. Ask them to extract a ticket as JSON, and you might get:\n",
        "\n",
        "**Good output:**\n",
        "```json\n",
        "{\"ticket_id\": \"TICK-5678\", \"priority\": \"HIGH\"}\n",
        "```\n",
        "\n",
        "**Bad outputs:**\n",
        "```json\n",
        "{\"ticketID\": \"5678\", \"Priority\": \"high\"}  // Wrong field names, wrong case\n",
        "{ticket_id: \"TICK-5678\"}                   // Missing quotes (invalid JSON)\n",
        "{\"ticket_id\": 5678}                        // Wrong data type (number not string)\n",
        "{\"ticket_id\": \"TICK-5678\", \"priority\": \"urgent\"}  // Invalid value (not LOW/MEDIUM/HIGH)\n",
        "```\n",
        "\n",
        "**In traditional testing:** When you test a REST API, you know the JSON structure is controlled by the backend code. It's consistent and predictable.\n",
        "\n",
        "**In LLM testing:** The LLM generates the JSON on-the-fly. You must validate that:\n",
        "1. It's valid JSON (parseable)\n",
        "2. All required fields are present\n",
        "3. Field names match exactly what you expect\n",
        "4. Data types are correct (string vs number vs boolean)\n",
        "5. Values meet constraints (e.g., priority must be \"LOW\", \"MEDIUM\", or \"HIGH\")\n",
        "\n",
        "---\n",
        "\n",
        "### The Solution is: Pydantic for Schema Validation\n",
        "\n",
        "**Pydantic** is a Python library that provides **data validation using Python type hints**. Think of it as:\n",
        "- **JSON Schema** for Python\n",
        "- **TypeScript interfaces** with runtime validation\n",
        "- **Database schema constraints** applied to Python objects\n",
        "\n",
        "\n",
        "Pydantic automatically checks:\n",
        "- ‚úÖ All required fields are present\n",
        "- ‚úÖ Field types are correct (string, int, etc.)\n",
        "- ‚úÖ Values match constraints (e.g., priority must be LOW/MEDIUM/HIGH)\n",
        "- ‚úÖ Provides clear error messages when validation fails"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Pydantic Works: A Complete Example\n",
        "\n",
        "Let's see Pydantic in action with a support ticket model.\n",
        "\n",
        "This model has 7 fields that validate different aspects of the extracted ticket data:\n",
        "\n",
        "  1. **ticket_id**: str ‚Äî Validates the ticket has a unique identifier (must be\n",
        "  text to preserve prefixes like \"TICK-\" and leading zeros)\n",
        "  2. **user**: str ‚Äî Ensures the reporter's name is extracted (required field, must\n",
        "   be text)\n",
        "  3. **email**: str ‚Äî Checks that a contact email address is present (required for\n",
        "  follow-up communication)\n",
        "  4. **issue**: str ‚Äî Validates that the problem description was captured (required\n",
        "   field containing the issue details)\n",
        "  5. **priority**: Literal[\"LOW\", \"MEDIUM\", \"HIGH\"] ‚Äî Enforces strict priority\n",
        "  values matching database constraints (rejects invalid values like \"urgent\" or\n",
        "   \"high priority\")\n",
        "  6. **category**: str ‚Äî Ensures the ticket is categorized for routing to the\n",
        "  correct support team\n",
        "  7. **status**: Literal[\"OPEN\", \"IN_PROGRESS\", \"RESOLVED\", \"CLOSED\"] ‚Äî Validates\n",
        "  ticket status with automatic default to \"OPEN\" if not provided by the LLM"
      ],
      "metadata": {
        "id": "umqNCsVhHdvA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAAx1dWxvcdZ"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from typing import Literal\n",
        "import json\n",
        "\n",
        "# Define the structure we expect from the LLM\n",
        "class SupportTicket(BaseModel):\n",
        "    \"\"\"A structured support ticket with all required fields.\"\"\"\n",
        "    ticket_id: str = Field(..., description=\"Ticket ID (e.g., 'TICK-1234')\")\n",
        "    user: str = Field(..., description=\"Full name of the user\")\n",
        "    email: str = Field(..., description=\"User's email address\")\n",
        "    issue: str = Field(..., description=\"Brief description of the issue\")\n",
        "    priority: Literal[\"LOW\", \"MEDIUM\", \"HIGH\"] = Field(..., description=\"Ticket priority\")\n",
        "    category: str = Field(..., description=\"Issue category\")\n",
        "    status: Literal[\"OPEN\", \"IN_PROGRESS\", \"RESOLVED\", \"CLOSED\"] = Field(default=\"OPEN\")\n",
        "\n",
        "# Example: Let's test extraction\n",
        "ticket_text = \"\"\"\n",
        "Ticket ID: TICK-5678\n",
        "From: Alice Johnson (alice.johnson@company.com)\n",
        "Issue: Cannot access VPN from home. Getting \"connection timeout\" error.\n",
        "Priority: HIGH\n",
        "Category: Network/VPN\n",
        "\n",
        "Extract this ticket information as JSON with fields: ticket_id, user, email, issue, priority, category, status.\n",
        "Return ONLY valid JSON, no other text.\n",
        "\"\"\"\n",
        "\n",
        "response = ask_llm(ticket_text)\n",
        "print(\"LLM Response:\")\n",
        "print(response)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Parse and validate\n",
        "try:\n",
        "    data = json.loads(response)\n",
        "    ticket = SupportTicket(**data)\n",
        "    print(\"‚úÖ Valid ticket structure!\")\n",
        "    print(f\"\\nTicket Details:\")\n",
        "    print(f\"  ID: {ticket.ticket_id}\")\n",
        "    print(f\"  User: {ticket.user}\")\n",
        "    print(f\"  Email: {ticket.email}\")\n",
        "    print(f\"  Issue: {ticket.issue}\")\n",
        "    print(f\"  Priority: {ticket.priority}\")\n",
        "    print(f\"  Category: {ticket.category}\")\n",
        "    print(f\"  Status: {ticket.status}\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"‚ùå Invalid JSON: {e}\")\n",
        "except ValidationError as e:\n",
        "    print(f\"‚ùå Validation failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What Happened During Validation\n",
        "\n",
        "When we call `SupportTicket(**data)`, Pydantic performed this validation sequence:\n",
        "\n",
        "```\n",
        "1. Check presence ‚Üí Are all required fields present? (...fields)\n",
        "                   ‚Üì Pass\n",
        "2. Check types ‚Üí Is ticket_id a string? Is priority a string?\n",
        "                   ‚Üì Pass\n",
        "3. Check constraints ‚Üí Is priority one of [\"LOW\", \"MEDIUM\", \"HIGH\"]?\n",
        "                   ‚Üì Pass\n",
        "4. Apply defaults ‚Üí If status missing, set to \"OPEN\"\n",
        "                   ‚Üì Pass\n",
        "5. Create object ‚Üí SupportTicket instance with validated data ‚úÖ\n",
        "```\n",
        "\n",
        "**If ANY step fails ‚Üí `ValidationError` with detailed message**\n",
        "\n",
        "Let's take a look at the result:\n",
        "The LLM successfully extracted all the ticket information and returned valid JSON with all 7 required fields. However, **Pydantic validation failed** because the status field contained \"open\" (lowercase) instead of the expected \"OPEN\" (uppercase).\n",
        "\n",
        "This is exactly the kind of\n",
        "  subtle error Pydantic is designed to catch: the data looks correct to a\n",
        "  human, the JSON is valid, and all fields are present, but the value doesn't\n",
        "  meet the strict constraints your database or API likely requires. Without\n",
        "  Pydantic, this lowercase \"open\" would pass through your tests and cause a\n",
        "  database constraint violation or API rejection in production."
      ],
      "metadata": {
        "id": "UVpODNXNTA8i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RHqsGFKvcdZ"
      },
      "source": [
        "## 6. Generating Test Reports\n",
        "\n",
        "Pytest can generate beautiful HTML reports showing all test results.\n",
        "\n",
        "Let's run all our tests and create a report:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHnvNi4Jvcda"
      },
      "outputs": [],
      "source": [
        "# Run all tests and generate an HTML report\n",
        "!pytest test_it_support.py test_parameterized.py -v --html=report.html --self-contained-html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsFx6g03vcda"
      },
      "source": [
        "The report is saved as `report.html`. You can download it from Colab and open it in your browser to see:\n",
        "- Summary of passed/failed tests\n",
        "- Execution time for each test\n",
        "- Detailed error messages for failures\n",
        "- Environment information\n",
        "\n",
        "This is perfect for sharing test results with your team! üìä"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WYaBxA_vcda"
      },
      "source": [
        "## 7. Exercises üéì\n",
        "\n",
        "Now it's your turn! Complete these exercises to practice what you've learned.\n",
        "\n",
        "### Exercise 1: Convert Manual Tests to Automated Tests\n",
        "\n",
        "Think of 3 questions you manually tested in ChatGPT. Convert them to automated pytest functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB0aUybBvcda"
      },
      "outputs": [],
      "source": [
        "# Exercise 1: Write your 3 tests here\n",
        "\n",
        "def test_your_question_1():\n",
        "    \"\"\"\n",
        "    TODO: Test your first IT support question\n",
        "    \"\"\"\n",
        "    pass  # Replace with your test code\n",
        "\n",
        "def test_your_question_2():\n",
        "    \"\"\"\n",
        "    TODO: Test your second IT support question\n",
        "    \"\"\"\n",
        "    pass  # Replace with your test code\n",
        "\n",
        "def test_your_question_3():\n",
        "    \"\"\"\n",
        "    TODO: Test your third IT support question\n",
        "    \"\"\"\n",
        "    pass  # Replace with your test code\n",
        "\n",
        "# Run your tests\n",
        "# !pytest -v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSULxMPnvcda"
      },
      "source": [
        "### Exercise 2: Parameterized Test for Common IT Issues\n",
        "\n",
        "Create a parameterized test that checks if the LLM provides appropriate solutions for 5 common IT problems.\n",
        "\n",
        "**Requirements:**\n",
        "- Use `@pytest.mark.parametrize`\n",
        "- Test at least 5 different IT issues\n",
        "- Check that solutions contain relevant keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmrBm5lGvcda"
      },
      "outputs": [],
      "source": [
        "#%%writefile exercise_2.py\n",
        "# Exercise 2: Your parameterized test here\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pytest\n",
        "\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "def ask_llm(prompt: str) -> str:\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-5-nano\",\n",
        "        input=prompt\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "# TODO: Write your parameterized test here\n",
        "# @pytest.mark.parametrize(...)\n",
        "# def test_it_solutions(...):\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1RGvlnbvcda"
      },
      "outputs": [],
      "source": [
        "# Run your parameterized test\n",
        "# !pytest exercise_2.py -v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdQ--Tltvcda"
      },
      "source": [
        "### Exercise 3: Pydantic Model for Incident Report\n",
        "\n",
        "Create a Pydantic model for an `IncidentReport` and test if the LLM can extract it correctly.\n",
        "\n",
        "**Required fields:**\n",
        "- `incident_id`: string\n",
        "- `reporter`: string\n",
        "- `timestamp`: string\n",
        "- `severity`: \"MINOR\" | \"MAJOR\" | \"CRITICAL\"\n",
        "- `affected_systems`: list of strings\n",
        "- `description`: string\n",
        "- `resolution_time_estimate`: string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvVaDYH0vcda"
      },
      "outputs": [],
      "source": [
        "# Exercise 3: Define your Pydantic model and test\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Literal\n",
        "import json\n",
        "\n",
        "# TODO: Define IncidentReport model\n",
        "class IncidentReport(BaseModel):\n",
        "    pass  # Add your fields here\n",
        "\n",
        "# TODO: Write a test that:\n",
        "# 1. Provides incident text to the LLM\n",
        "# 2. Asks LLM to extract as JSON\n",
        "# 3. Validates with your IncidentReport model\n",
        "\n",
        "def test_incident_extraction():\n",
        "    pass  # Your test code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB-v34xXvcda"
      },
      "source": [
        "### Exercise 4: Consistency Testing\n",
        "\n",
        "Test if the LLM gives consistent troubleshooting advice across 3 runs.\n",
        "\n",
        "**Requirements:**\n",
        "- Pick an IT issue (e.g., \"Computer won't start\")\n",
        "- Ask for troubleshooting steps 3 times\n",
        "- Check that all 3 responses mention the same key concepts\n",
        "- Hint: Define a set of expected keywords and check they appear in all responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUzwuqg-vcda"
      },
      "outputs": [],
      "source": [
        "# Exercise 4: Consistency test\n",
        "\n",
        "def test_troubleshooting_consistency():\n",
        "    \"\"\"\n",
        "    TODO: Test consistency of troubleshooting advice\n",
        "    \"\"\"\n",
        "    pass  # Your test code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3eT94mavcda"
      },
      "source": [
        "### Exercise 5: Intentionally Failing Tests\n",
        "\n",
        "Write 3 tests that will FAIL because they test edge cases or difficult scenarios.\n",
        "\n",
        "**Example edge cases:**\n",
        "- Ambiguous ticket descriptions\n",
        "- Mixed priority signals in text\n",
        "- Tickets with missing information\n",
        "- Very technical jargon\n",
        "\n",
        "Then, improve your prompts to make the tests pass!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baohSWTMvcda"
      },
      "outputs": [],
      "source": [
        "# Exercise 5: Write failing tests, then fix them\n",
        "\n",
        "def test_edge_case_1():\n",
        "    \"\"\"\n",
        "    TODO: Test an edge case that initially fails\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def test_edge_case_2():\n",
        "    \"\"\"\n",
        "    TODO: Test another edge case\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def test_edge_case_3():\n",
        "    \"\"\"\n",
        "    TODO: Test a third edge case\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# Step 1: Run and watch them fail\n",
        "# Step 2: Improve your prompts\n",
        "# Step 3: Run again and see them pass!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7m9O5xCvcdb"
      },
      "source": [
        "## 9. Best Practices for LLM Testing\n",
        "\n",
        "### ‚úÖ DO:\n",
        "\n",
        "1. **Write clear, specific prompts** - Reduces ambiguity\n",
        "2. **Test one thing at a time** - Easier to debug failures\n",
        "3. **Use descriptive test names** - `test_ticket_severity()` not `test_1()`\n",
        "4. **Add helpful assertion messages** - Explain what you expected\n",
        "5. **Test edge cases** - Empty inputs, very long inputs, ambiguous cases\n",
        "6. **Use Pydantic for structured outputs** - Automatic validation\n",
        "7. **Group related tests** - Use separate test files for different features\n",
        "8. **Check for key concepts, not exact strings** - LLMs may phrase answers differently\n",
        "\n",
        "### ‚ùå DON'T:\n",
        "\n",
        "1. **Don't expect exact string matches** - LLMs vary in phrasing\n",
        "2. **Don't test creative tasks too strictly** - Allow for variation\n",
        "3. **Don't ignore flaky tests** - Investigate and fix them\n",
        "4. **Don't test too many things in one test** - Keep tests focused\n",
        "5. **Don't forget to test error cases** - What if API fails?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qvLaTabvcdb"
      },
      "source": [
        "## 10. Key Takeaways & Next Steps\n",
        "\n",
        "### üéâ What You've Learned\n",
        "\n",
        "1. **Manual ‚Üí Automated**: You transformed your ChatGPT testing workflow into automated Python tests\n",
        "2. **Pytest basics**: Writing tests, running test suites, reading output\n",
        "3. **Parameterization**: Testing multiple scenarios with minimal code\n",
        "4. **Structured testing**: Using Pydantic to validate LLM outputs\n",
        "5. **Best practices**: How to write maintainable, effective LLM tests\n",
        "\n",
        "\n",
        "### üí° Pro Tips\n",
        "\n",
        "1. **Start small**: Begin with a few critical tests, then expand\n",
        "2. **Run tests often**: Integrate into your development workflow\n",
        "3. **Track test coverage**: Aim for 80%+ coverage of critical paths\n",
        "4. **Share reports**: Use HTML reports to communicate with non-technical team members\n",
        "5. **Iterate on prompts**: Use failing tests to improve your prompts\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Additional Resources\n",
        "\n",
        "- [Pytest Documentation](https://docs.pytest.org/)\n",
        "- [Pydantic Documentation](https://docs.pydantic.dev/)\n",
        "- [OpenAI API Documentation](https://platform.openai.com/docs/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfBc1hnkymIt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}