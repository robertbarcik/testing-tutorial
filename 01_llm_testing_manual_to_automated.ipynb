{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpylwiSTvcdV"
   },
   "source": [
    "# LLM Testing: From Manual to Automated\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand why automated testing is essential for LLM applications\n",
    "2. Convert manual tests (like those you did in ChatGPT) into automated Python tests\n",
    "3. Use pytest to organize and run test suites\n",
    "4. Write parameterized tests to test multiple scenarios efficiently\n",
    "5. Validate structured outputs using Pydantic models\n",
    "6. Interpret test results and debug failures\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Context: What You've Done Before (Manual Testing)\n",
    "\n",
    "Previously, you manually tested LLMs in ChatGPT by:\n",
    "- Asking factual questions and checking answers\n",
    "- Testing prompt sensitivity by rephrasing questions\n",
    "- Checking for bias in responses\n",
    "- Evaluating consistency across multiple queries\n",
    "\n",
    "### Why This Doesn't Scale\n",
    "\n",
    "Imagine you're building an **IT Support Chatbot** for a company helpdesk. You need to test:\n",
    "- ‚úÖ Does it correctly identify ticket severity?\n",
    "- ‚úÖ Does it provide accurate troubleshooting steps?\n",
    "- ‚úÖ Does it extract ticket information correctly?\n",
    "- ‚úÖ Is it consistent across similar queries?\n",
    "\n",
    "**Problem:** Testing these manually every time you update your prompt or model is:\n",
    "- ‚è∞ Time-consuming\n",
    "- üêõ Error-prone\n",
    "- üìà Not scalable (what about 100 test cases?)\n",
    "- üîÑ Hard to reproduce\n",
    "\n",
    "### The Solution: Automated Testing\n",
    "\n",
    "**Same tests you did in ChatGPT, now in code!**\n",
    "\n",
    "Benefits:\n",
    "- üöÄ Run hundreds of tests in seconds\n",
    "- üîÅ Reproducible results\n",
    "- ü§ñ Integrate into CI/CD pipelines\n",
    "- üìä Generate test reports automatically\n",
    "- üõ°Ô∏è Catch regressions when you change prompts or models\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SH1nSnzvcdW"
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll install the required packages and set up our OpenAI API access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAFKccVVvcdX"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# This may take a minute - you'll see progress bars\n",
    "!pip install openai pytest==8.3.4 pytest-html==4.1.1 pydantic>=2.11.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LD3vNVKTvcdX",
    "outputId": "01567527-cd99-49ce-a27b-a7e6c1bd9ca3"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pytest\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "from typing import List, Optional\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9AQgUETvcdX"
   },
   "source": [
    "### API Key Setup\n",
    "\n",
    "You'll need an OpenAI API key to run these tests.\n",
    "\n",
    "**How to get your API key:**\n",
    "1. Go to [platform.openai.com](https://platform.openai.com)\n",
    "2. Sign in or create an account\n",
    "3. Navigate to API Keys section\n",
    "4. Create a new secret key\n",
    "5. Copy it and use it below\n",
    "\n",
    "**Two ways to provide your API key:**\n",
    "\n",
    "**Option 1: Colab Secrets (Recommended - More Secure)**\n",
    "- Click the üîë key icon in the left sidebar\n",
    "- Add a new secret with name: `OPENAI_API_KEY`\n",
    "- Paste your API key as the value\n",
    "- Enable \"Notebook access\" toggle\n",
    "- Run the cell below - it will automatically load from secrets\n",
    "\n",
    "**Option 2: Enter when prompted**\n",
    "- Just run the cell below\n",
    "- You'll be prompted to enter your API key\n",
    "- The key will be hidden as you type\n",
    "\n",
    "**üí∞ Cost Note:** We'll use the `gpt-5-nano` model, which is very cost-effective for testing. These examples will cost less than $0.01 to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w75U7cnRvcdX",
    "outputId": "26a605c1-29bb-4bba-e66b-775584b2a34c"
   },
   "outputs": [],
   "source": [
    "# Configure OpenAI API key\n",
    "# Method 1: Try to get API key from Colab secrets (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "    print(\"‚úÖ API key loaded from Colab secrets\")\n",
    "except:\n",
    "    # Method 2: Manual input (fallback)\n",
    "    from getpass import getpass\n",
    "    print(\"üí° To use Colab secrets: Go to üîë (left sidebar) ‚Üí Add new secret ‚Üí Name: OPENAI_API_KEY\")\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Validate that the API key is set\n",
    "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
    "    raise ValueError(\"‚ùå ERROR: No API key provided!\")\n",
    "\n",
    "print(\"‚úÖ Authentication configured!\")\n",
    "\n",
    "# Configure which OpenAI model to use\n",
    "OPENAI_MODEL = \"gpt-5-nano\"  # Using gpt-5-nano for cost efficiency\n",
    "print(f\"ü§ñ Selected Model: {OPENAI_MODEL}\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPd7UhilvcdY"
   },
   "source": [
    "## 2. Your First Automated Test\n",
    "\n",
    "Let's start simple. Remember when you manually tested IT knowledge in ChatGPT?\n",
    "\n",
    "**Manual Test (What you did before):**\n",
    "1. Open ChatGPT\n",
    "2. Type: \"What port does HTTPS use?\"\n",
    "3. Read response\n",
    "4. Check if it says \"443\"\n",
    "5. ‚úÖ or ‚ùå\n",
    "\n",
    "**Automated Test (What we'll do now):**\n",
    "Same thing, but in code!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBBKV_v2vcdY",
    "outputId": "295b5e2f-16b6-46f7-88e6-06d4d9e9bf44"
   },
   "outputs": [],
   "source": [
    "# Helper function to call the LLM\n",
    "def ask_llm(prompt: str, model: str = \"gpt-5-nano\") -> str:\n",
    "    \"\"\"\n",
    "    Send a prompt to the LLM and return the response.\n",
    "\n",
    "    Args:\n",
    "        prompt: The question or instruction to send to the LLM\n",
    "        model: The model to use (default: gpt-5-nano)\n",
    "\n",
    "    Returns:\n",
    "        The LLM's response as a string\n",
    "    \"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=prompt\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "# Test it out!\n",
    "response = ask_llm(\"What port does HTTPS use? Answer with just the number.\")\n",
    "print(f\"LLM Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBkEU3NcvcdY"
   },
   "source": [
    "### Plain Python Test Function\n",
    "\n",
    "Here's our first test as a simple Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zNf5BRKvcdY",
    "outputId": "c2d7a54e-e23b-42c5-9fdc-5e78bf05cb40"
   },
   "outputs": [],
   "source": [
    "def test_https_port_knowledge():\n",
    "    \"\"\"\n",
    "    Test: Does the LLM know what port HTTPS uses?\n",
    "    Expected: Response should contain \"443\"\n",
    "    \"\"\"\n",
    "    # Arrange: Set up the test data\n",
    "    question = \"What port does HTTPS use? Answer with just the number.\"\n",
    "\n",
    "    # Act: Call the LLM\n",
    "    response = ask_llm(question)\n",
    "\n",
    "    # Assert: Check if the response is correct\n",
    "    assert \"443\" in response, f\"Expected '443' in response, but got: {response}\"\n",
    "\n",
    "    print(\"‚úÖ Test passed! LLM correctly identified HTTPS port as 443\")\n",
    "\n",
    "# Run the test\n",
    "test_https_port_knowledge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q17HY4ppvcdY"
   },
   "source": [
    "### Understanding the Test Structure\n",
    "\n",
    "Every good test follows the **AAA pattern**:\n",
    "\n",
    "1. **Arrange:** Set up your test data (the question)\n",
    "2. **Act:** Perform the action (call the LLM)\n",
    "3. **Assert:** Check if the result matches expectations\n",
    "\n",
    "**Key concept:** The `assert` statement is how we check correctness:\n",
    "- If the condition is `True` ‚Üí Test passes ‚úÖ\n",
    "- If the condition is `False` ‚Üí Test fails ‚ùå and shows error message\n",
    "\n",
    "**Note about consistency:** Since gpt-5-nano doesn't support temperature control, responses may vary slightly between runs. We design our tests to check for key concepts rather than exact string matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niH1_MznvcdY"
   },
   "source": [
    "## 3. Building a Test Suite with Pytest\n",
    "\n",
    "Now let's organize multiple tests using **pytest**, a professional testing framework.\n",
    "\n",
    "Pytest automatically:\n",
    "- Discovers all functions starting with `test_`\n",
    "- Runs them in isolation\n",
    "- Reports which passed or failed\n",
    "- Shows detailed error messages\n",
    "\n",
    "Let's create a test file with multiple IT support tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKqMoJGVvcdY",
    "outputId": "64d8520c-be34-45df-bba7-7883ffda6d47"
   },
   "outputs": [],
   "source": [
    "%%writefile test_it_support.py\n",
    "# test_it_support.py\n",
    "# This file contains automated tests for our IT Support LLM\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pytest\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def ask_llm(prompt: str, model: str = \"gpt-5-nano\") -> str:\n",
    "    \"\"\"Send a prompt to the LLM and return the response.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=prompt\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "\n",
    "# Test 1: Technical Fact Testing\n",
    "def test_technical_fact():\n",
    "    \"\"\"\n",
    "    Test: Does the LLM know basic IT facts?\n",
    "    This is like asking factual questions in ChatGPT and checking the answer.\n",
    "    \"\"\"\n",
    "    question = \"What is the default SSH port? Answer with just the number.\"\n",
    "    response = ask_llm(question)\n",
    "\n",
    "    # Check if response contains the correct port\n",
    "    assert \"22\" in response, f\"Expected SSH port 22, but got: {response}\"\n",
    "\n",
    "\n",
    "# Test 2: Ticket Severity Assessment\n",
    "def test_ticket_severity_assessment():\n",
    "    \"\"\"\n",
    "    Test: Can the LLM correctly assess ticket severity?\n",
    "    Critical issues should be flagged as HIGH priority.\n",
    "    \"\"\"\n",
    "    ticket_description = \"\"\"\n",
    "    Ticket: Database server is completely down. All employees cannot access\n",
    "    customer data. Production environment is affected.\n",
    "\n",
    "    Classify this ticket's severity as: LOW, MEDIUM, or HIGH.\n",
    "    Answer with just one word.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ask_llm(ticket_description)\n",
    "\n",
    "    # A complete database outage should be HIGH severity\n",
    "    assert \"HIGH\" in response.upper(), f\"Expected HIGH severity, but got: {response}\"\n",
    "\n",
    "\n",
    "# Test 3: Consistency Testing\n",
    "def test_consistency():\n",
    "    \"\"\"\n",
    "    Test: Does the LLM give consistent answers to the same question?\n",
    "    This is like asking the same question multiple times in ChatGPT.\n",
    "    \"\"\"\n",
    "    question = \"What is the purpose of a firewall in network security? Answer in one sentence.\"\n",
    "\n",
    "    # Ask the same question twice\n",
    "    response1 = ask_llm(question)\n",
    "    response2 = ask_llm(question)\n",
    "\n",
    "    # Check that both responses mention key concepts about firewalls\n",
    "    assert \"firewall\" in response1.lower(), f\"Response 1 missing 'firewall': {response1}\"\n",
    "    assert \"firewall\" in response2.lower(), f\"Response 2 missing 'firewall': {response2}\"\n",
    "\n",
    "    # Check semantic similarity (both should mention blocking/filtering/protecting)\n",
    "    keywords = [\"block\", \"filter\", \"control\", \"protect\", \"monitor\"]\n",
    "    assert any(kw in response1.lower() for kw in keywords), f\"Response 1 missing key concepts: {response1}\"\n",
    "    assert any(kw in response2.lower() for kw in keywords), f\"Response 2 missing key concepts: {response2}\"\n",
    "\n",
    "\n",
    "# Test 4: Structured Output (JSON)\n",
    "def test_structured_output_json():\n",
    "    \"\"\"\n",
    "    Test: Can the LLM extract structured information from a ticket?\n",
    "    This tests if the LLM can parse unstructured text into structured data.\n",
    "    \"\"\"\n",
    "    ticket_text = \"\"\"\n",
    "    Ticket #7823: User Sarah Chen reports that she cannot print from her laptop.\n",
    "    The printer shows as offline. Priority: Medium. Category: Hardware.\n",
    "\n",
    "    Extract the following information as JSON:\n",
    "    - ticket_id\n",
    "    - user_name\n",
    "    - issue_summary\n",
    "    - priority\n",
    "    - category\n",
    "\n",
    "    Return ONLY valid JSON, no other text.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ask_llm(ticket_text)\n",
    "\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        data = json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        pytest.fail(f\"Response is not valid JSON: {response}\")\n",
    "\n",
    "    # Verify all required fields are present\n",
    "    required_fields = [\"ticket_id\", \"user_name\", \"issue_summary\", \"priority\", \"category\"]\n",
    "    for field in required_fields:\n",
    "        assert field in data, f\"Missing required field: {field}\"\n",
    "\n",
    "    # Verify correctness of extracted data\n",
    "    assert \"7823\" in str(data[\"ticket_id\"]), f\"Wrong ticket_id: {data['ticket_id']}\"\n",
    "    assert \"Sarah\" in data[\"user_name\"] or \"Chen\" in data[\"user_name\"], f\"Wrong user: {data['user_name']}\"\n",
    "\n",
    "\n",
    "# Test 5: Structured Output with Pydantic Validation\n",
    "class SupportTicket(BaseModel):\n",
    "    \"\"\"Pydantic model for a support ticket.\"\"\"\n",
    "    ticket_id: str = Field(..., description=\"Unique ticket identifier\")\n",
    "    user: str = Field(..., description=\"Name of the user who reported the issue\")\n",
    "    issue: str = Field(..., description=\"Brief description of the issue\")\n",
    "    priority: str = Field(..., description=\"Priority level: LOW, MEDIUM, or HIGH\")\n",
    "    category: str = Field(..., description=\"Category of the issue\")\n",
    "\n",
    "def test_structured_output_validation():\n",
    "    \"\"\"\n",
    "    Test: Can the LLM produce output that passes strict validation?\n",
    "    We use Pydantic to ensure the output has the right structure AND data types.\n",
    "    \"\"\"\n",
    "    ticket_text = \"\"\"\n",
    "    Ticket #9234: John Smith cannot access his email. Getting \"authentication failed\" error.\n",
    "    This is blocking his work. Priority: High. Category: Email.\n",
    "\n",
    "    Extract the information as JSON with these exact fields:\n",
    "    - ticket_id (string)\n",
    "    - user (string)\n",
    "    - issue (string)\n",
    "    - priority (string: LOW, MEDIUM, or HIGH)\n",
    "    - category (string)\n",
    "\n",
    "    Return ONLY valid JSON, no other text.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ask_llm(ticket_text)\n",
    "\n",
    "    # Parse JSON\n",
    "    try:\n",
    "        data = json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        pytest.fail(f\"Response is not valid JSON: {response}\")\n",
    "\n",
    "    # Validate with Pydantic model\n",
    "    try:\n",
    "        ticket = SupportTicket(**data)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Response doesn't match SupportTicket model: {e}\\nData: {data}\")\n",
    "\n",
    "    # Verify semantic correctness\n",
    "    assert \"9234\" in ticket.ticket_id, f\"Wrong ticket_id: {ticket.ticket_id}\"\n",
    "    assert \"John\" in ticket.user or \"Smith\" in ticket.user, f\"Wrong user: {ticket.user}\"\n",
    "    assert ticket.priority.upper() in [\"LOW\", \"MEDIUM\", \"HIGH\"], f\"Invalid priority: {ticket.priority}\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Test file created: test_it_support.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHSsIJEwvcdZ"
   },
   "source": [
    "### Run the Test Suite\n",
    "\n",
    "Now let's run all 5 tests with pytest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMcuqfDZvcdZ",
    "outputId": "576d87fc-e88c-4cc8-948e-33c0399efd4d"
   },
   "outputs": [],
   "source": [
    "# Run pytest with verbose output\n",
    "# -v = verbose (show each test name)\n",
    "# -s = show print statements\n",
    "!pytest test_it_support.py -v -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_oFyjOFvcdZ"
   },
   "source": [
    "### üéâ Congratulations!\n",
    "\n",
    "You just ran 5 automated tests! Notice how pytest:\n",
    "- Discovered all functions starting with `test_`\n",
    "- Ran each test independently\n",
    "- Showed which passed (‚úì) or failed (‚úó)\n",
    "- Displayed helpful error messages for failures\n",
    "\n",
    "**This is the same testing you did manually in ChatGPT, but now:**\n",
    "- ‚ö° Takes seconds instead of minutes\n",
    "- üîÅ Perfectly reproducible\n",
    "- üìä Generates reports automatically\n",
    "- ü§ñ Can run in CI/CD pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFuenFFdvcdZ"
   },
   "source": [
    "## 4. Parameterized Testing\n",
    "\n",
    "What if you want to test multiple similar scenarios? You could write 10 separate test functions... or use **parameterized tests**!\n",
    "\n",
    "**Parameterized tests** let you run the same test logic with different input data.\n",
    "\n",
    "### Example: Testing Multiple IT Knowledge Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h60j0z8nvcdZ",
    "outputId": "72a84b84-dbd9-4659-dac2-aef607c99a3e"
   },
   "outputs": [],
   "source": [
    "%%writefile test_parameterized.py\n",
    "# test_parameterized.py\n",
    "# Demonstrates parameterized testing with pytest\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pytest\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def ask_llm(prompt: str, model: str = \"gpt-5-nano\") -> str:\n",
    "    \"\"\"Send a prompt to the LLM and return the response.\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=prompt\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "\n",
    "# Parameterized test for IT knowledge\n",
    "@pytest.mark.parametrize(\"question,expected_answer\", [\n",
    "    (\"What port does HTTP use? Answer with just the number.\", \"80\"),\n",
    "    (\"What port does HTTPS use? Answer with just the number.\", \"443\"),\n",
    "    (\"What port does SSH use? Answer with just the number.\", \"22\"),\n",
    "    (\"What port does FTP use? Answer with just the number.\", \"21\"),\n",
    "    (\"What port does SMTP use? Answer with just the number.\", \"25\"),\n",
    "])\n",
    "def test_port_knowledge(question, expected_answer):\n",
    "    \"\"\"\n",
    "    Test: Does the LLM know standard network ports?\n",
    "    This ONE test function runs 5 times with different data!\n",
    "    \"\"\"\n",
    "    response = ask_llm(question)\n",
    "    assert expected_answer in response, f\"Expected '{expected_answer}' in response, got: {response}\"\n",
    "\n",
    "\n",
    "# Parameterized test for ticket classification\n",
    "@pytest.mark.parametrize(\"issue_description,expected_category\", [\n",
    "    (\"My laptop screen is cracked and won't display anything.\", \"Hardware\"),\n",
    "    (\"I forgot my password and can't log into the system.\", \"Access\"),\n",
    "    (\"The company website is loading very slowly for all users.\", \"Network\"),\n",
    "    (\"Excel keeps crashing when I try to open large files.\", \"Software\"),\n",
    "    (\"I need permission to access the shared finance folder.\", \"Access\"),\n",
    "])\n",
    "def test_ticket_classification(issue_description, expected_category):\n",
    "    \"\"\"\n",
    "    Test: Can the LLM correctly categorize different types of IT issues?\n",
    "    Categories: Hardware, Software, Network, Access\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Categorize this IT support issue into ONE of these categories:\n",
    "    Hardware, Software, Network, Access\n",
    "\n",
    "    Issue: {issue_description}\n",
    "\n",
    "    Answer with just the category name.\n",
    "    \"\"\"\n",
    "\n",
    "    response = ask_llm(prompt)\n",
    "    assert expected_category.lower() in response.lower(), \\\n",
    "        f\"Expected category '{expected_category}', but got: {response}\"\n",
    "\n",
    "\n",
    "# Parameterized test for troubleshooting advice\n",
    "# NOTE: We use multiple possible keywords since LLMs may phrase advice differently\n",
    "@pytest.mark.parametrize(\"problem,expected_keywords\", [\n",
    "    (\"User can't connect to WiFi\", [\"wifi\", \"wireless\", \"router\", \"modem\", \"connection\"]),\n",
    "    (\"Printer is offline\", [\"printer\", \"print\", \"device\", \"cable\", \"driver\"]),\n",
    "    (\"Computer is very slow\", [\"slow\", \"memory\", \"cpu\", \"task\", \"process\", \"resource\"]),\n",
    "    (\"Email won't send\", [\"email\", \"smtp\", \"server\", \"account\", \"credential\"]),\n",
    "    (\"Can't install software\", [\"install\", \"permission\", \"administrator\", \"compatibility\", \"space\"]),\n",
    "])\n",
    "def test_troubleshooting_advice(problem, expected_keywords):\n",
    "    \"\"\"\n",
    "    Test: Does the LLM provide relevant troubleshooting advice?\n",
    "    We check if the advice mentions at least ONE relevant keyword.\n",
    "    This is more flexible than checking for exact keywords!\n",
    "    \"\"\"\n",
    "    prompt = f\"Provide one troubleshooting step for this issue: {problem}\"\n",
    "    response = ask_llm(prompt)\n",
    "\n",
    "    response_lower = response.lower()\n",
    "\n",
    "    # Check if ANY of the expected keywords appear in the response\n",
    "    found_keywords = [kw for kw in expected_keywords if kw in response_lower]\n",
    "\n",
    "    assert len(found_keywords) > 0, \\\n",
    "        f\"Expected advice to mention one of {expected_keywords}, but got: {response}\"\n",
    "\n",
    "    # Also check that we got actual advice (not just echoing the problem)\n",
    "    assert len(response) > 20, f\"Response too short to be useful advice: {response}\"\n",
    "\n",
    "\n",
    "print(\"‚úÖ Parameterized test file created: test_parameterized.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QML2Lo4uvcdZ",
    "outputId": "d5e772af-efb8-407a-f3cd-684736ce4447"
   },
   "outputs": [],
   "source": [
    "# Run the parameterized tests\n",
    "# Notice how ONE test function becomes MANY test cases!\n",
    "!pytest test_parameterized.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG4ESG1PvcdZ"
   },
   "source": [
    "### üöÄ Power of Parameterization\n",
    "\n",
    "Look at what just happened:\n",
    "- We wrote **3 test functions**\n",
    "- But pytest ran **15 test cases** (5 + 5 + 5)\n",
    "- Each test case appears separately in the report\n",
    "- If one case fails, others still run\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úçÔ∏è Less code duplication\n",
    "- üìù Easier to add new test cases (just add to the list)\n",
    "- üîç Clearer which specific inputs failed\n",
    "- üéØ More comprehensive coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0RC-1zpvcdZ"
   },
   "source": [
    "## 5. Testing Structured Outputs with Pydantic\n",
    "\n",
    "In real applications, you often need LLMs to return structured data (JSON). But how do you test that the structure is correct?\n",
    "\n",
    "**Pydantic** helps you:\n",
    "1. Define the expected structure (schema)\n",
    "2. Validate that the LLM output matches the schema\n",
    "3. Get type checking and helpful error messages\n",
    "\n",
    "### Example: Support Ticket Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pAAx1dWxvcdZ",
    "outputId": "552f8ff8-e330-4d08-cb81-ecf27f18d062"
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import Literal\n",
    "import json\n",
    "\n",
    "# Define the structure we expect from the LLM\n",
    "class SupportTicket(BaseModel):\n",
    "    \"\"\"A structured support ticket with all required fields.\"\"\"\n",
    "    ticket_id: str = Field(..., description=\"Ticket ID (e.g., 'TICK-1234')\")\n",
    "    user: str = Field(..., description=\"Full name of the user\")\n",
    "    email: str = Field(..., description=\"User's email address\")\n",
    "    issue: str = Field(..., description=\"Brief description of the issue\")\n",
    "    priority: Literal[\"LOW\", \"MEDIUM\", \"HIGH\"] = Field(..., description=\"Ticket priority\")\n",
    "    category: str = Field(..., description=\"Issue category\")\n",
    "    status: Literal[\"OPEN\", \"IN_PROGRESS\", \"RESOLVED\", \"CLOSED\"] = Field(default=\"OPEN\")\n",
    "\n",
    "# Example: Let's test extraction\n",
    "ticket_text = \"\"\"\n",
    "Ticket ID: TICK-5678\n",
    "From: Alice Johnson (alice.johnson@company.com)\n",
    "Issue: Cannot access VPN from home. Getting \"connection timeout\" error.\n",
    "Priority: HIGH\n",
    "Category: Network/VPN\n",
    "\n",
    "Extract this ticket information as JSON with fields: ticket_id, user, email, issue, priority, category, status.\n",
    "Return ONLY valid JSON, no other text.\n",
    "\"\"\"\n",
    "\n",
    "response = ask_llm(ticket_text)\n",
    "print(\"LLM Response:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Parse and validate\n",
    "try:\n",
    "    data = json.loads(response)\n",
    "    ticket = SupportTicket(**data)\n",
    "    print(\"‚úÖ Valid ticket structure!\")\n",
    "    print(f\"\\nTicket Details:\")\n",
    "    print(f\"  ID: {ticket.ticket_id}\")\n",
    "    print(f\"  User: {ticket.user}\")\n",
    "    print(f\"  Email: {ticket.email}\")\n",
    "    print(f\"  Issue: {ticket.issue}\")\n",
    "    print(f\"  Priority: {ticket.priority}\")\n",
    "    print(f\"  Category: {ticket.category}\")\n",
    "    print(f\"  Status: {ticket.status}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"‚ùå Invalid JSON: {e}\")\n",
    "except ValidationError as e:\n",
    "    print(f\"‚ùå Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb3prztJvcdZ"
   },
   "source": [
    "### Why Pydantic is Powerful for Testing\n",
    "\n",
    "Pydantic automatically checks:\n",
    "- ‚úÖ All required fields are present\n",
    "- ‚úÖ Field types are correct (string, int, etc.)\n",
    "- ‚úÖ Values match constraints (e.g., priority must be LOW/MEDIUM/HIGH)\n",
    "- ‚úÖ Provides clear error messages when validation fails\n",
    "\n",
    "**Without Pydantic:** You'd need to manually write checks for each field.  \n",
    "**With Pydantic:** One line (`SupportTicket(**data)`) does it all! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RHqsGFKvcdZ"
   },
   "source": [
    "## 6. Generating Test Reports\n",
    "\n",
    "Pytest can generate beautiful HTML reports showing all test results.\n",
    "\n",
    "Let's run all our tests and create a report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHnvNi4Jvcda",
    "outputId": "5ec37861-4131-4168-ff77-dcb451cec791"
   },
   "outputs": [],
   "source": [
    "# Run all tests and generate an HTML report\n",
    "!pytest test_it_support.py test_parameterized.py -v --html=report.html --self-contained-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsFx6g03vcda"
   },
   "source": [
    "The report is saved as `report.html`. You can download it from Colab and open it in your browser to see:\n",
    "- Summary of passed/failed tests\n",
    "- Execution time for each test\n",
    "- Detailed error messages for failures\n",
    "- Environment information\n",
    "\n",
    "This is perfect for sharing test results with your team! üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WYaBxA_vcda"
   },
   "source": [
    "## 7. Exercises üéì\n",
    "\n",
    "Now it's your turn! Complete these exercises to practice what you've learned.\n",
    "\n",
    "### Exercise 1: Convert Manual Tests to Automated Tests\n",
    "\n",
    "Think of 3 IT support questions you manually tested in ChatGPT. Convert them to automated pytest functions.\n",
    "\n",
    "**Example topics:**\n",
    "- What causes a \"DNS not found\" error?\n",
    "- How do you reset a Windows password?\n",
    "- What's the difference between RAM and storage?\n",
    "\n",
    "Write your tests below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GB0aUybBvcda"
   },
   "outputs": [],
   "source": [
    "# Exercise 1: Write your 3 tests here\n",
    "\n",
    "def test_your_question_1():\n",
    "    \"\"\"\n",
    "    TODO: Test your first IT support question\n",
    "    \"\"\"\n",
    "    pass  # Replace with your test code\n",
    "\n",
    "def test_your_question_2():\n",
    "    \"\"\"\n",
    "    TODO: Test your second IT support question\n",
    "    \"\"\"\n",
    "    pass  # Replace with your test code\n",
    "\n",
    "def test_your_question_3():\n",
    "    \"\"\"\n",
    "    TODO: Test your third IT support question\n",
    "    \"\"\"\n",
    "    pass  # Replace with your test code\n",
    "\n",
    "# Run your tests\n",
    "# !pytest -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSULxMPnvcda"
   },
   "source": [
    "### Exercise 2: Parameterized Test for Common IT Issues\n",
    "\n",
    "Create a parameterized test that checks if the LLM provides appropriate solutions for 5 common IT problems.\n",
    "\n",
    "**Requirements:**\n",
    "- Use `@pytest.mark.parametrize`\n",
    "- Test at least 5 different IT issues\n",
    "- Check that solutions contain relevant keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmrBm5lGvcda",
    "outputId": "7d92a85d-39d8-4160-9953-e9b87a975b02"
   },
   "outputs": [],
   "source": [
    "%%writefile exercise_2.py\n",
    "# Exercise 2: Your parameterized test here\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pytest\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def ask_llm(prompt: str) -> str:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        input=prompt\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "# TODO: Write your parameterized test here\n",
    "# @pytest.mark.parametrize(...)\n",
    "# def test_it_solutions(...):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1RGvlnbvcda"
   },
   "outputs": [],
   "source": [
    "# Run your parameterized test\n",
    "# !pytest exercise_2.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdQ--Tltvcda"
   },
   "source": [
    "### Exercise 3: Pydantic Model for Incident Report\n",
    "\n",
    "Create a Pydantic model for an `IncidentReport` and test if the LLM can extract it correctly.\n",
    "\n",
    "**Required fields:**\n",
    "- `incident_id`: string\n",
    "- `reporter`: string\n",
    "- `timestamp`: string\n",
    "- `severity`: \"MINOR\" | \"MAJOR\" | \"CRITICAL\"\n",
    "- `affected_systems`: list of strings\n",
    "- `description`: string\n",
    "- `resolution_time_estimate`: string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvVaDYH0vcda"
   },
   "outputs": [],
   "source": [
    "# Exercise 3: Define your Pydantic model and test\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "import json\n",
    "\n",
    "# TODO: Define IncidentReport model\n",
    "class IncidentReport(BaseModel):\n",
    "    pass  # Add your fields here\n",
    "\n",
    "# TODO: Write a test that:\n",
    "# 1. Provides incident text to the LLM\n",
    "# 2. Asks LLM to extract as JSON\n",
    "# 3. Validates with your IncidentReport model\n",
    "\n",
    "def test_incident_extraction():\n",
    "    pass  # Your test code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qB-v34xXvcda"
   },
   "source": [
    "### Exercise 4: Consistency Testing\n",
    "\n",
    "Test if the LLM gives consistent troubleshooting advice across 3 runs.\n",
    "\n",
    "**Requirements:**\n",
    "- Pick an IT issue (e.g., \"Computer won't start\")\n",
    "- Ask for troubleshooting steps 3 times\n",
    "- Check that all 3 responses mention the same key concepts\n",
    "- Hint: Define a set of expected keywords and check they appear in all responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUzwuqg-vcda"
   },
   "outputs": [],
   "source": [
    "# Exercise 4: Consistency test\n",
    "\n",
    "def test_troubleshooting_consistency():\n",
    "    \"\"\"\n",
    "    TODO: Test consistency of troubleshooting advice\n",
    "    \"\"\"\n",
    "    pass  # Your test code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3eT94mavcda"
   },
   "source": [
    "### Exercise 5: Intentionally Failing Tests\n",
    "\n",
    "Write 3 tests that will FAIL because they test edge cases or difficult scenarios.\n",
    "\n",
    "**Example edge cases:**\n",
    "- Ambiguous ticket descriptions\n",
    "- Mixed priority signals in text\n",
    "- Tickets with missing information\n",
    "- Very technical jargon\n",
    "\n",
    "Then, improve your prompts to make the tests pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baohSWTMvcda"
   },
   "outputs": [],
   "source": [
    "# Exercise 5: Write failing tests, then fix them\n",
    "\n",
    "def test_edge_case_1():\n",
    "    \"\"\"\n",
    "    TODO: Test an edge case that initially fails\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def test_edge_case_2():\n",
    "    \"\"\"\n",
    "    TODO: Test another edge case\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def test_edge_case_3():\n",
    "    \"\"\"\n",
    "    TODO: Test a third edge case\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Step 1: Run and watch them fail\n",
    "# Step 2: Improve your prompts\n",
    "# Step 3: Run again and see them pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7m9O5xCvcdb"
   },
   "source": [
    "## 9. Best Practices for LLM Testing\n",
    "\n",
    "### ‚úÖ DO:\n",
    "\n",
    "1. **Write clear, specific prompts** - Reduces ambiguity\n",
    "2. **Test one thing at a time** - Easier to debug failures\n",
    "3. **Use descriptive test names** - `test_ticket_severity()` not `test_1()`\n",
    "4. **Add helpful assertion messages** - Explain what you expected\n",
    "5. **Test edge cases** - Empty inputs, very long inputs, ambiguous cases\n",
    "6. **Use Pydantic for structured outputs** - Automatic validation\n",
    "7. **Group related tests** - Use separate test files for different features\n",
    "8. **Check for key concepts, not exact strings** - LLMs may phrase answers differently\n",
    "\n",
    "### ‚ùå DON'T:\n",
    "\n",
    "1. **Don't expect exact string matches** - LLMs vary in phrasing\n",
    "2. **Don't test creative tasks too strictly** - Allow for variation\n",
    "3. **Don't ignore flaky tests** - Investigate and fix them\n",
    "4. **Don't test too many things in one test** - Keep tests focused\n",
    "5. **Don't forget to test error cases** - What if API fails?\n",
    "\n",
    "### Testing Strategy\n",
    "\n",
    "For each LLM feature, test:\n",
    "1. **Happy path** - Normal, expected inputs\n",
    "2. **Edge cases** - Unusual but valid inputs\n",
    "3. **Error cases** - Invalid inputs\n",
    "4. **Consistency** - Check for key concepts across multiple runs\n",
    "5. **Performance** - Response time (if critical)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qvLaTabvcdb"
   },
   "source": [
    "## 10. Key Takeaways & Next Steps\n",
    "\n",
    "### üéâ What You've Learned\n",
    "\n",
    "1. **Manual ‚Üí Automated**: You transformed your ChatGPT testing workflow into automated Python tests\n",
    "2. **Pytest basics**: Writing tests, running test suites, reading output\n",
    "3. **Parameterization**: Testing multiple scenarios with minimal code\n",
    "4. **Structured testing**: Using Pydantic to validate LLM outputs\n",
    "5. **Best practices**: How to write maintainable, effective LLM tests\n",
    "\n",
    "\n",
    "### üí° Pro Tips\n",
    "\n",
    "1. **Start small**: Begin with a few critical tests, then expand\n",
    "2. **Run tests often**: Integrate into your development workflow\n",
    "3. **Track test coverage**: Aim for 80%+ coverage of critical paths\n",
    "4. **Share reports**: Use HTML reports to communicate with non-technical team members\n",
    "5. **Iterate on prompts**: Use failing tests to improve your prompts\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Additional Resources\n",
    "\n",
    "- [Pytest Documentation](https://docs.pytest.org/)\n",
    "- [Pydantic Documentation](https://docs.pydantic.dev/)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfBc1hnkymIt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
