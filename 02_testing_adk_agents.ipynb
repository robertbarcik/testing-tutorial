{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y60EZMJCy-Bl"
      },
      "source": [
        "# Testing ADK Agents: From Simple LLMs to Agentic Behavior\n",
        "\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "1. Build testable agents using Google's ADK (Agent Development Kit)\n",
        "2. Test agent tool selection (which tools the agent chooses to call)\n",
        "3. Test tool parameters (accuracy of extracted information)\n",
        "4. Test multi-step reasoning (tool call sequences)\n",
        "5. Test edge cases (invalid inputs, ambiguous requests)\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Context: Why Agent Testing is Different\n",
        "\n",
        "### Quick Recap: Notebook 01\n",
        "\n",
        "In the previous lesson, you learned to:\n",
        "- ✅ Write automated tests with pytest\n",
        "- ✅ Test LLM text outputs for factual correctness\n",
        "- ✅ Validate structured outputs with Pydantic\n",
        "- ✅ Use parameterized tests\n",
        "\n",
        "### What's Different with Agents?\n",
        "\n",
        "**Simple LLM (Notebook 01):**\n",
        "```\n",
        "User: \"What port does SSH use?\"\n",
        "LLM: \"Port 22\"\n",
        "Test: Check if response contains \"22\" ✅\n",
        "```\n",
        "\n",
        "**Agent with Tools (This Notebook):**\n",
        "```\n",
        "User: \"What's the status of ticket #5678?\"\n",
        "Agent: Thinks... I need to look up this ticket\n",
        "       Calls: lookup_ticket(\"5678\")\n",
        "       Tool returns: {ticket_id: 5678, status: \"In Progress\", ...}\n",
        "       Agent: \"Ticket #5678 is currently In Progress...\"\n",
        "       \n",
        "Test: Did agent call the right tool? ✅\n",
        "Test: Did agent extract correct ticket ID? ✅\n",
        "Test: Did agent use the tool results properly? ✅\n",
        "```\n",
        "\n",
        "### Key Differences\n",
        "\n",
        "| Simple LLM Testing | Agent Testing |\n",
        "|-------------------|---------------|\n",
        "| Test final text output | Test tool selection & parameters |\n",
        "| Single-step response | Multi-step reasoning |\n",
        "| Stateless | Stateful (tool results affect next steps) |\n",
        "| Straightforward assertions | Test tool call sequences |\n",
        "\n",
        "### What We're Building Today\n",
        "\n",
        "An **IT Support Agent** with these tools:\n",
        "- 🎫 `lookup_ticket(ticket_id)` - Retrieve ticket details\n",
        "- 📚 `search_knowledge_base(query)` - Find help articles\n",
        "- 🔍 `check_system_status(service)` - Check if systems are up\n",
        "\n",
        "**Testing scenarios:**\n",
        "- User asks about a ticket → Agent calls `lookup_ticket` with correct ID\n",
        "- User has a problem → Agent searches KB for solution\n",
        "- User asks multi-step question → Agent calls multiple tools in order\n",
        "\n",
        "---\n",
        "\n",
        "Let's get started! 🚀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjIz-9usy-Bn"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, we'll install the required packages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q \\\n",
        "  google-adk==1.17.0 \\\n",
        "  litellm==1.79.0 \\\n",
        "  openai==1.109.1 \\\n",
        "  python-dotenv==1.1.1 \\\n",
        "  nest-asyncio==1.6.0 \\\n",
        "  deprecated==1.3.1 \\\n",
        "  google-genai==1.46.0 \\\n",
        "  pydantic==2.11.10"
      ],
      "metadata": {
        "id": "YUuXHmXFYAPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpea1Pfey-Bo"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "import json\n",
        "import asyncio\n",
        "from typing import List, Optional, Dict, Any\n",
        "import nest_asyncio\n",
        "\n",
        "# Enable nested event loops (required for Colab)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Core ADK imports\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.models.lite_llm import LiteLlm\n",
        "from google.genai import types\n",
        "\n",
        "print(\"✅ All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJrB7p-dy-Bo"
      },
      "source": [
        "### API Key Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv81ekemy-Bp"
      },
      "outputs": [],
      "source": [
        "# Configure OpenAI API key\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    print(\"✅ API key loaded from Colab secrets\")\n",
        "except:\n",
        "    from getpass import getpass\n",
        "    print(\"💡 To use Colab secrets: Go to 🔑 (left sidebar) → Add new secret → Name: OPENAI_API_KEY\")\n",
        "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "if not OPENAI_API_KEY or OPENAI_API_KEY.strip() == \"\":\n",
        "    raise ValueError(\"❌ ERROR: No API key provided!\")\n",
        "\n",
        "print(\"✅ Authentication configured!\")\n",
        "\n",
        "# Model configuration\n",
        "OPENAI_MODEL = \"gpt-5-nano\"\n",
        "print(f'🤖 Selected Model: {OPENAI_MODEL}')\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phL_om-6y-Bp"
      },
      "source": [
        "## 2. Creating a Testable IT Support Agent\n",
        "\n",
        "Let's build a simple IT support agent with three tools that handle common helpdesk tasks:\n",
        "\n",
        "  1. `lookup_ticket(ticket_id: str)` - **Ticket Database Lookup**\n",
        "  - Purpose: Retrieves ticket information from a mock database\n",
        "  - What it returns: Ticket details including ID, user, issue description,\n",
        "  status, and priority\n",
        "  - Test database: Contains 3 pre-defined tickets (TICKET-001, TICKET-002,\n",
        "  TICKET-003) with different scenarios\n",
        " - ✅ We will verify if the agent can correctly extract ticket IDs from\n",
        "  user queries and call the right tool\n",
        "\n",
        "  2. `search_knowledge_base(query: str)` - **Knowledge Base Search**\n",
        "  - Purpose: Searches IT documentation for troubleshooting articles\n",
        "  - What it returns: Relevant help articles based on keywords in the query\n",
        "  - Mock data: Contains 3 articles covering VPN issues, password resets, and\n",
        "  printer problems\n",
        "  - ✅ We will checks if the agent can identify the user's problem type\n",
        "  and search with appropriate keywords\n",
        "\n",
        "  3. `escalate_to_human(ticket_id: str, reason: str)` - **Escalation to Human Agent**\n",
        "  - Purpose: Transfers complex issues to a human support agent\n",
        "  - What it returns: Confirmation message with ticket ID and escalation reason\n",
        "  - Mock behavior: Simply logs the escalation (doesn't actually contact anyone)\n",
        "  - ✅ We will test if the agent knows when it can't solve a problem and\n",
        "   properly escalates\n",
        "\n",
        "⭐ Key point: These are mock implementations - they return hardcoded data\n",
        "  instead of querying real systems. This makes our tests fast, reliable, and\n",
        "  safe to run repeatedly without worrying about costs or side effects.\n",
        "\n",
        "\n",
        "Let's implement them:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmb7wwcWy-Bp"
      },
      "outputs": [],
      "source": [
        "# Tool 1: Lookup Ticket\n",
        "def lookup_ticket(ticket_id: str) -> dict:\n",
        "    \"\"\"\n",
        "    Look up details for a support ticket.\n",
        "\n",
        "    Args:\n",
        "        ticket_id: The ticket ID to look up (e.g., \"5678\")\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with ticket details\n",
        "    \"\"\"\n",
        "    mock_tickets = {\n",
        "        \"5678\": {\n",
        "            \"ticket_id\": \"5678\",\n",
        "            \"status\": \"In Progress\",\n",
        "            \"priority\": \"High\",\n",
        "            \"user\": \"Alice Johnson\",\n",
        "            \"issue\": \"Cannot access email\",\n",
        "            \"assigned_to\": \"Tech Support Team\"\n",
        "        },\n",
        "        \"1234\": {\n",
        "            \"ticket_id\": \"1234\",\n",
        "            \"status\": \"Resolved\",\n",
        "            \"priority\": \"Medium\",\n",
        "            \"user\": \"Bob Smith\",\n",
        "            \"issue\": \"Printer not working\",\n",
        "            \"assigned_to\": \"Hardware Team\"\n",
        "        },\n",
        "        \"9999\": {\n",
        "            \"ticket_id\": \"9999\",\n",
        "            \"status\": \"Open\",\n",
        "            \"priority\": \"Critical\",\n",
        "            \"user\": \"Charlie Brown\",\n",
        "            \"issue\": \"Server down\",\n",
        "            \"assigned_to\": \"Infrastructure Team\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if ticket_id in mock_tickets:\n",
        "        return mock_tickets[ticket_id]\n",
        "    else:\n",
        "        return {\"error\": f\"Ticket {ticket_id} not found\"}\n",
        "\n",
        "\n",
        "# Tool 2: Search Knowledge Base\n",
        "def search_knowledge_base(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Search the IT knowledge base for help articles.\n",
        "\n",
        "    Args:\n",
        "        query: Search query (e.g., \"how to reset password\")\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with search results\n",
        "    \"\"\"\n",
        "    mock_kb = {\n",
        "        \"password\": [\n",
        "            {\"title\": \"How to Reset Your Password\", \"article_id\": \"KB001\"},\n",
        "            {\"title\": \"Password Requirements\", \"article_id\": \"KB002\"}\n",
        "        ],\n",
        "        \"email\": [\n",
        "            {\"title\": \"Troubleshooting Email Access\", \"article_id\": \"KB010\"},\n",
        "            {\"title\": \"Email Configuration Guide\", \"article_id\": \"KB011\"}\n",
        "        ],\n",
        "        \"vpn\": [\n",
        "            {\"title\": \"VPN Setup Instructions\", \"article_id\": \"KB020\"},\n",
        "            {\"title\": \"VPN Connection Issues\", \"article_id\": \"KB021\"}\n",
        "        ],\n",
        "        \"printer\": [\n",
        "            {\"title\": \"Printer Offline Solutions\", \"article_id\": \"KB030\"},\n",
        "            {\"title\": \"How to Install Printer Drivers\", \"article_id\": \"KB031\"}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    query_lower = query.lower()\n",
        "    results = []\n",
        "\n",
        "    for keyword, articles in mock_kb.items():\n",
        "        if keyword in query_lower:\n",
        "            results.extend(articles)\n",
        "\n",
        "    if results:\n",
        "        return {\"query\": query, \"results\": results, \"count\": len(results)}\n",
        "    else:\n",
        "        return {\"query\": query, \"results\": [], \"count\": 0}\n",
        "\n",
        "\n",
        "# Tool 3: Check System Status\n",
        "def check_system_status(service_name: str) -> dict:\n",
        "    \"\"\"\n",
        "    Check the operational status of a service or system.\n",
        "\n",
        "    Args:\n",
        "        service_name: Name of the service (e.g., \"email\", \"vpn\", \"database\")\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with service status\n",
        "    \"\"\"\n",
        "    mock_status = {\n",
        "        \"email\": {\"service\": \"email\", \"status\": \"operational\", \"uptime\": \"99.9%\"},\n",
        "        \"vpn\": {\"service\": \"vpn\", \"status\": \"operational\", \"uptime\": \"100%\"},\n",
        "        \"database\": {\"service\": \"database\", \"status\": \"degraded\", \"uptime\": \"95.2%\"},\n",
        "        \"file_server\": {\"service\": \"file_server\", \"status\": \"down\", \"uptime\": \"0%\"},\n",
        "        \"web_portal\": {\"service\": \"web_portal\", \"status\": \"operational\", \"uptime\": \"99.5%\"}\n",
        "    }\n",
        "\n",
        "    service_lower = service_name.lower().replace(\" \", \"_\")\n",
        "\n",
        "    if service_lower in mock_status:\n",
        "        return mock_status[service_lower]\n",
        "    else:\n",
        "        return {\"service\": service_name, \"status\": \"unknown\"}\n",
        "\n",
        "\n",
        "print(\"✅ Tools defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0MA2wN6y-Bp"
      },
      "source": [
        "### Creating the ADK Agent\n",
        "This code creates and configures an ADK agent with three key parts:\n",
        "\n",
        "  1. The LLM Model Configuration: connects ADK to OpenAI's GPT model through LiteLLM\n",
        "  2. The Agent Definition\n",
        "  3. The Instruction Prompt: here we teach the agent when and how to use tools\n",
        "\n",
        "- The instructions are detailed:\n",
        "  - Agent behavior is controlled primarily by the instruction prompt\n",
        "  - Clear instructions = predictable behavior = easier testing\n",
        "  - In testing, we verify the agent follows these rules\n",
        "\n",
        "- We include negative examples as well (when NOT to use tools):\n",
        "  - Without them, agents tend to over-use tools (calling tools for every\n",
        "  question)\n",
        "  - Clear boundaries make testing easier: \"Did the agent correctly decide NOT\n",
        "  to use a tool?\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ueq7Bt1y-Bp"
      },
      "outputs": [],
      "source": [
        "# Create the LiteLlm model instance for OpenAI\n",
        "llm_model = LiteLlm(\n",
        "    model=f\"openai/{OPENAI_MODEL}\",\n",
        "    api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "# Create the IT Support Agent\n",
        "it_support_agent = LlmAgent(\n",
        "    name=\"it_support_agent\",\n",
        "    model=llm_model,\n",
        "    description=\"An IT support agent that helps users with tickets, knowledge base searches, and system status checks\",\n",
        "    instruction=\"\"\"You are an IT support agent. You help users with IT issues by using the appropriate tools.\n",
        "\n",
        "WHEN TO USE TOOLS:\n",
        "1. Ticket questions (e.g., \"ticket 5678\", \"check ticket status\"): call lookup_ticket tool\n",
        "2. HOW-TO questions or help with specific problems: call search_knowledge_base tool\n",
        "3. Service status questions (e.g., \"is email working?\"): call check_system_status tool\n",
        "\n",
        "WHEN NOT TO USE TOOLS:\n",
        "- General questions about concepts (e.g., \"What is IT support?\", \"What is a firewall?\")\n",
        "- Greetings or small talk\n",
        "- Questions that don't require looking up specific information\n",
        "\n",
        "EXAMPLES OF TOOL USAGE:\n",
        "- \"What's ticket 5678 status?\" -> call lookup_ticket(\"5678\")\n",
        "- \"How do I fix VPN issues?\" -> call search_knowledge_base(\"VPN issues\")\n",
        "- \"Is email working?\" -> call check_system_status(\"email\")\n",
        "\n",
        "EXAMPLES OF NO TOOL NEEDED:\n",
        "- \"What is IT support?\" -> Answer directly without tools\n",
        "- \"Hello\" -> Respond without tools\n",
        "- \"What does a firewall do?\" -> Answer directly without tools\n",
        "\n",
        "For questions requiring specific current information (tickets, KB articles, system status), ALWAYS use the appropriate tool.\"\"\",\n",
        "    tools=[lookup_ticket, search_knowledge_base, check_system_status]\n",
        ")\n",
        "\n",
        "print(\"✅ IT Support Agent created!\")\n",
        "print(f\"Agent has {len(it_support_agent.tools)} tools available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62nzKZfny-Bq"
      },
      "source": [
        "## 3. Helper Function for Testing\n",
        "To test our agent, we need to run it with different user messages and inspect what tools it called. `run_agent_and_get_tools()` helper function handles all the ADK setup (creating sessions, runners, formatting messages) and extracts the key information we care about for testing:\n",
        "- which tools were called\n",
        "- what parameters were used\n",
        "- what the final response was\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DC0AuM8y-Bq"
      },
      "outputs": [],
      "source": [
        "async def run_agent_and_get_tools(user_message: str, session_id: str = \"test_session\"):\n",
        "    \"\"\"\n",
        "    Run the agent and capture tool calls.\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            'tool_calls': list of {name, parameters},\n",
        "            'tool_count': int,\n",
        "            'response': str\n",
        "        }\n",
        "    \"\"\"\n",
        "    # Create session\n",
        "    session_service = InMemorySessionService()\n",
        "    user_id = \"test_user\"\n",
        "\n",
        "    await session_service.create_session(\n",
        "        app_name=\"it_support_test\",\n",
        "        user_id=user_id,\n",
        "        session_id=session_id,\n",
        "        state={}\n",
        "    )\n",
        "\n",
        "    # Create runner\n",
        "    runner = Runner(\n",
        "        app_name=\"it_support_test\",\n",
        "        agent=it_support_agent,\n",
        "        session_service=session_service\n",
        "    )\n",
        "\n",
        "    # Format message and run\n",
        "    content = types.Content(role='user', parts=[types.Part(text=user_message)])\n",
        "    events = runner.run_async(user_id=user_id, session_id=session_id, new_message=content)\n",
        "\n",
        "    # Collect tool calls and response\n",
        "    tool_calls = []\n",
        "    final_response = \"\"\n",
        "\n",
        "    async for event in events:\n",
        "        # Use get_function_calls() to extract tool calls\n",
        "        if hasattr(event, 'get_function_calls'):\n",
        "            function_calls = event.get_function_calls()\n",
        "            if function_calls:\n",
        "                for func_call in function_calls:\n",
        "                    tool_calls.append({\n",
        "                        'name': func_call.name,\n",
        "                        'parameters': func_call.args\n",
        "                    })\n",
        "\n",
        "        # Get final response\n",
        "        if event.is_final_response():\n",
        "            final_response = event.content.parts[0].text\n",
        "\n",
        "    return {\n",
        "        'tool_calls': tool_calls,\n",
        "        'tool_count': len(tool_calls),\n",
        "        'response': final_response\n",
        "    }\n",
        "\n",
        "print(\"✅ Helper function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZoxqJawy-Bq"
      },
      "source": [
        "## 4. Testing Tool Selection\n",
        "  \n",
        "  The first and most fundamental test: **Does the agent choose the right tool?**\n",
        "\n",
        "  This is critical because everything else depends on it. If the agent picks\n",
        "  the wrong tool (searching the knowledge base when it should look up a\n",
        "  ticket), nothing else matters—the user gets the wrong help, even if the\n",
        "  parameters are perfect.\n",
        "\n",
        "### Pattern: Test Tool Selection\n",
        "\n",
        "```python\n",
        "1. Give agent a task requiring a specific tool (e.g., \"Check ticket 5678\")\n",
        "2. Run the agent with that message\n",
        "3. Assert that the expected tool was called\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG2JFrKKy-Bq"
      },
      "source": [
        "### Test 1: Agent calls lookup_ticket tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe7Mkh2jy-Bq"
      },
      "outputs": [],
      "source": [
        "# Test: When user asks about a ticket, agent should call lookup_ticket\n",
        "user_message = \"Can you check the status of ticket 5678?\"\n",
        "result = await run_agent_and_get_tools(user_message, session_id=\"test_1\")\n",
        "\n",
        "print(f\"User: {user_message}\")\n",
        "print(f\"\\nTools called: {result['tool_count']}\")\n",
        "print(f\"Tool names: {[tc['name'] for tc in result['tool_calls']]}\")\n",
        "print(f\"\\nAgent response: {result['response'][:200]}...\\n\")\n",
        "\n",
        "# Assertions\n",
        "assert result['tool_count'] > 0, \"❌ Agent should have called at least one tool\"\n",
        "tool_names = [tc['name'] for tc in result['tool_calls']]\n",
        "assert 'lookup_ticket' in tool_names, f\"❌ Expected 'lookup_ticket', got: {tool_names}\"\n",
        "\n",
        "print(\"✅ TEST PASSED: Agent correctly called lookup_ticket\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oNAHw-Hy-Br"
      },
      "source": [
        "### Test 2: Agent calls knowledge base search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NeP6x1cy-Br"
      },
      "outputs": [],
      "source": [
        "# Test: When user has a problem, agent should search knowledge base\n",
        "user_message = \"How do I reset my password?\"\n",
        "result = await run_agent_and_get_tools(user_message, session_id=\"test_2\")\n",
        "\n",
        "print(f\"User: {user_message}\")\n",
        "print(f\"\\nTools called: {result['tool_count']}\")\n",
        "print(f\"Tool names: {[tc['name'] for tc in result['tool_calls']]}\")\n",
        "print(f\"\\nAgent response: {result['response'][:200]}...\\n\")\n",
        "\n",
        "# Assertions\n",
        "tool_names = [tc['name'] for tc in result['tool_calls']]\n",
        "assert 'search_knowledge_base' in tool_names, f\"❌ Expected 'search_knowledge_base', got: {tool_names}\"\n",
        "\n",
        "print(\"✅ TEST PASSED: Agent correctly called search_knowledge_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOp7YLbIy-Br"
      },
      "source": [
        "### Test 3: Your Turn - Test System Status Check\n",
        "\n",
        "Now it's your turn to write a test!\n",
        "\n",
        "Your task:\n",
        "  - Write a test that verifies the agent calls `check_system_status` when the\n",
        "  user asks about service availability\n",
        "  - Use the pattern from Tests 1 and 2 above\n",
        "  - Test with a message like: \"Is the email service working?\"\n",
        "\n",
        "  What to assert:\n",
        "  1. The agent called at least one tool\n",
        "  2. The tool called was `check_system_status`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN--p3coy-Br"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPxTKamly-Br"
      },
      "source": [
        "### Key Insight: Testing Behavior, Not Text\n",
        "\n",
        "Notice what we're testing:\n",
        "- ✅ **We test**: Which tool was called\n",
        "- ❌ **We don't test**: The exact text of the response\n",
        "\n",
        "Why? Because:\n",
        "1. Tool calls are **deterministic** (agent logic)\n",
        "2. Text responses are **variable** (natural language)\n",
        "3. Tool calls prove the agent **understood** the task\n",
        "4. Tool calls are what **actually matter** for functionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlq1mdtly-Bs"
      },
      "source": [
        "## 5. Testing Tool Parameters\n",
        "\n",
        "Now let's test whether the agent extract the correct parameters from the user's message.\n",
        "\n",
        "**Example:**\n",
        "- User: \"Check ticket 5678\"\n",
        "- Agent calls: `lookup_ticket(\"5678\")` ✅\n",
        "- Agent calls: `lookup_ticket(\"1234\")` ❌ Wrong ticket!\n",
        "\n",
        "Both scenarios call the right tool (lookup_ticket), but only the first one is actually useful. Parameter accuracy is essential for correct functionality.\n",
        "\n",
        "  Agents must handle:\n",
        "  - 🔢 Extracting IDs from natural language (\"ticket 5678\" → \"5678\")\n",
        "  - 🔍 Formulating search queries (\"How to reset password?\" → \"reset password\"\n",
        "  or \"password reset\")\n",
        "  - 🏷️ Identifying service names (\"Is email working?\" → \"email\")\n",
        "  - 🗣️ Noisy input (\"Hey so like check ticket 5678 please?\" → \"5678\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCumYvpfy-Bs"
      },
      "source": [
        "### Test 4: Agent extracts ticket ID correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zSVm6EJy-Bs"
      },
      "outputs": [],
      "source": [
        "# Test: Agent correctly extracts ticket ID from user message\n",
        "user_message = \"What's the status of ticket 1234?\"\n",
        "result = await run_agent_and_get_tools(user_message, session_id=\"test_4\")\n",
        "\n",
        "print(f\"User: {user_message}\")\n",
        "print(f\"\\nTools called: {[tc['name'] for tc in result['tool_calls']]}\")\n",
        "\n",
        "# Find the lookup_ticket call\n",
        "ticket_calls = [tc for tc in result['tool_calls'] if tc['name'] == 'lookup_ticket']\n",
        "assert len(ticket_calls) > 0, \"❌ Agent should have called lookup_ticket\"\n",
        "\n",
        "# Check the ticket_id parameter\n",
        "ticket_id = ticket_calls[0]['parameters'].get('ticket_id')\n",
        "print(f\"Extracted ticket_id: {ticket_id}\\n\")\n",
        "\n",
        "assert ticket_id == \"1234\", f\"❌ Expected ticket_id '1234', but got: {ticket_id}\"\n",
        "\n",
        "print(\"✅ TEST PASSED: Agent correctly extracted ticket ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5pI3RvJy-Bs"
      },
      "source": [
        "### Test 5: Agent extracts search query correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8djR06Tzy-Bs"
      },
      "outputs": [],
      "source": [
        "# Test: Agent correctly extracts and formats search query\n",
        "user_message = \"How do I fix VPN connection issues?\"\n",
        "result = await run_agent_and_get_tools(user_message, session_id=\"test_5\")\n",
        "\n",
        "print(f\"User: {user_message}\")\n",
        "print(f\"\\nTools called: {[tc['name'] for tc in result['tool_calls']]}\")\n",
        "\n",
        "# Find the KB search call\n",
        "kb_calls = [tc for tc in result['tool_calls'] if tc['name'] == 'search_knowledge_base']\n",
        "assert len(kb_calls) > 0, \"❌ Agent should have called search_knowledge_base\"\n",
        "\n",
        "# Check that query contains relevant keywords\n",
        "query = kb_calls[0]['parameters'].get('query', '').lower()\n",
        "print(f\"Search query: {query}\\n\")\n",
        "\n",
        "assert 'vpn' in query, f\"❌ Query should contain 'vpn', but got: {query}\"\n",
        "\n",
        "print(\"✅ TEST PASSED: Agent correctly extracted search query\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKs2L3hay-Bt"
      },
      "source": [
        "### Test 6:  Your Turn - Test Parameter Extraction for Service Names\n",
        "\n",
        "Now test if the agent can correctly extract service names from user queries!\n",
        "\n",
        "  Your task:\n",
        "  - Write a test that verifies the agent extracts the correct service name when\n",
        "   asking about system status\n",
        "  - Use a message like: \"Can you tell me if the VPN is up and running?\"\n",
        "\n",
        "  What to assert:\n",
        "  1. The agent called check_system_status\n",
        "  2. The service_name parameter contains \"vpn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAhjTAeiy-Bt"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT-k20wry-Bt"
      },
      "source": [
        "## 6. Testing Multi-Step Reasoning\n",
        "\n",
        "So far, we've tested agents that use one tool at a time. But real-world tasks often require multiple steps where the agent must:\n",
        "  1. Break down a complex request into smaller steps\n",
        "  2. Call multiple tools in a logical sequence\n",
        "  3. Use information from one tool call to inform the next\n",
        "\n",
        "In production, users don't always ask simple, single-step questions. They ask things like:\n",
        "  - \"Is the database down? If so, find affected tickets\" (status check → ticket\n",
        "   search)\n",
        "  - \"Help me troubleshoot my VPN, starting with my open tickets\" (lookup →\n",
        "  search → status)\n",
        "\n",
        "\n",
        " What makes multi-step reasoning challenging:\n",
        "\n",
        "| Challenge            | Why It's Hard |\n",
        "|----------------------|-------------------------------------------------------------|\n",
        "| Task decomposition   | Agent must understand that one question requires multiple actions |\n",
        "| Correct sequencing   | Tools must be called in the right order (can't search before knowing what to search for) |\n",
        "| Context maintenance  | Agent must remember results from step 1 when executing step 2 |\n",
        "| Knowing when to stop | Agent needs to recognize when enough tools have been called |\n",
        "\n",
        "**Example:**\n",
        "Imagine a user asks \"Check ticket 5678 and find solutions for the issue\"\n",
        "\n",
        "```\n",
        "User: \"Check ticket 5678 and find solutions for the issue\"\n",
        "\n",
        "Step 1: Agent calls lookup_ticket(\"5678\")\n",
        "        Returns: {issue: \"Cannot access email\"}\n",
        "        \n",
        "Step 2: Agent calls search_knowledge_base(\"email access\")\n",
        "        Returns: [KB articles about email]\n",
        "        \n",
        "Step 3: Agent synthesizes both results into helpful response\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOSlBKt8y-Bt"
      },
      "source": [
        "### Test 7: Multi-step reasoning (ticket then KB)\n",
        "\n",
        "\n",
        "Let's create a test that verify whether the agent can break down a complex request into a logical sequence:\n",
        "- first looking up the ticket to understand the\n",
        "   problem\n",
        "- then searching for solutions based on that information\n",
        "\n",
        "Before we run the test, let's see what ticket 5678 contains so we understand\n",
        "  what the agent is working with:\n",
        "\n",
        "```\n",
        " \"5678\": {\n",
        "      \"ticket_id\": \"5678\",\n",
        "      \"status\": \"In Progress\",\n",
        "      \"priority\": \"High\",\n",
        "      \"user\": \"Alice Johnson\",\n",
        "      \"issue\": \"Cannot access email\",\n",
        "      \"assigned_to\": \"Tech Support Team\"\n",
        "  }\n",
        "```\n",
        "\n",
        "Now let's test the agent's multi-step reasoning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7FG9CH6y-Bt"
      },
      "outputs": [],
      "source": [
        "# Test: Agent performs multi-step reasoning\n",
        "# 1. Looks up ticket to understand the issue\n",
        "# 2. Searches KB for solutions\n",
        "user_message = \"Check ticket 5678 and help me find solutions for the issue\"\n",
        "result = await run_agent_and_get_tools(user_message, session_id=\"test_7\")\n",
        "\n",
        "print(f\"User: {user_message}\")\n",
        "print(f\"\\nTools called: {result['tool_count']}\")\n",
        "tool_sequence = [tc['name'] for tc in result['tool_calls']]\n",
        "print(f\"Tool sequence: {tool_sequence}\\n\")\n",
        "\n",
        "# Assert multiple tools were called\n",
        "assert result['tool_count'] >= 2, f\"❌ Expected at least 2 tool calls, but got: {result['tool_count']}\"\n",
        "\n",
        "# Assert that both tools were called\n",
        "assert 'lookup_ticket' in tool_sequence, \"❌ Agent should look up the ticket\"\n",
        "assert 'search_knowledge_base' in tool_sequence, \"❌ Agent should search KB for solutions\"\n",
        "\n",
        "# Assert correct order: ticket lookup should come before KB search\n",
        "ticket_index = tool_sequence.index('lookup_ticket')\n",
        "kb_index = tool_sequence.index('search_knowledge_base')\n",
        "assert ticket_index < kb_index, f\"❌ Agent should look up ticket before searching KB, but order was: {tool_sequence}\"\n",
        "\n",
        "print(\"✅ TEST PASSED: Agent correctly performed multi-step reasoning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's observe the result.  The agent called 3 tools in this sequence: `lookup_ticket` → `search_knowledge_base` → `check_system_status`\n",
        "\n",
        "The agent showed **proactive reasoning**! After discovering the ticket is  about email access issues, it went a step further to check whether the email  service itself is down. This is actually intelligent troubleshooting — determining if it's a user-specific issue or a system-wide problem.\n",
        "\n",
        "\n",
        "  Agents can exhibit **emergent behavior** beyond the minimum required steps.\n",
        "  This can be:\n",
        "  - ✅ **Beneficial:** Smarter, more thorough assistance\n",
        "  - ⚠️ **Unexpected:** More API calls, higher costs, potential for errors\n",
        "\n",
        "  As testers, you need to:\n",
        "  - ✅ Test for **minimum expected behavior** (the 2 core tools)\n",
        "  - ✅ Be aware of **additional tool calls** and understand why they happen\n",
        "  - ✅ Decide if extra steps are desirable or need to be constrained\n",
        "\n"
      ],
      "metadata": {
        "id": "ITxCmvRAuvOZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWa0ybrYy-Bt"
      },
      "source": [
        "### Test 8: Your Turn - Test Multi-Step Reasoning with Status Check\n",
        "\n",
        "Now test if the agent can chain together a status check with knowledge base\n",
        "  search!\n",
        "\n",
        "  Your task:\n",
        "  - Write a test where the user says: \"The database seems slow. Check its\n",
        "  status and find troubleshooting guides.\"\n",
        "  - The agent should perform two steps:\n",
        "    1. Call check_system_status to check database status\n",
        "    2. Call search_knowledge_base to find troubleshooting articles\n",
        "\n",
        "  What to assert:\n",
        "  1. At least 2 tools were called\n",
        "  2. Both check_system_status and search_knowledge_base were called\n",
        "  3. The status check happens before the KB search (logical order)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "\n"
      ],
      "metadata": {
        "id": "FFvEE2nzve-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCkh3RNny-Bt"
      },
      "source": [
        "## 7. Testing Edge Cases\n",
        "\n",
        "  So far, we've tested the happy path—scenarios where users provide clear,\n",
        "  well-formed requests with all necessary information. However, in production, users will provide **incomplete information, use ambiguous language, make typos and errors, ask questions that don't require any tools or provide invalid IDs or references that don't exist**.\n",
        "\n",
        "Therefore we need to test **edge cases**. These are scenarios that test the boundaries and limits of your system such as:\n",
        "\n",
        "\n",
        "  - ❓ Ambiguous requests - \"Help me with my problem\" (what problem?)\n",
        "  - ❌ Invalid data - \"Check ticket ABC123\" (malformed ticket ID)\n",
        "  - 🤷 No tool needed - \"What is IT support?\" (general knowledge question)\n",
        "  - 🔀 Multiple interpretations - \"Check the email\" (ticket about email? or\n",
        "  email service status?)\n",
        "  - 🗣️ Noisy input - \"Hey so like, umm, could you maybe check ticket 5678?\"\n",
        "\n",
        "\n",
        "**Your agent needs to handle these gracefully!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3czF3_Fy-Bu"
      },
      "source": [
        "### Test 9: General question (no tool needed)\n",
        "\n",
        "  This test verifies that the agent knows when NOT to use tools. General\n",
        "  conceptual questions like \"What is IT support?\" don't require looking up data\n",
        "   or checking systems—they just need a direct answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_xqtjauy-Bu"
      },
      "outputs": [],
      "source": [
        "# Test: Agent handles general questions without calling tools\n",
        "user_message = \"What is IT support?\"\n",
        "result = await run_agent_and_get_tools(user_message, session_id=\"test_9\")\n",
        "\n",
        "print(f\"User: {user_message}\")\n",
        "print(f\"\\nTools called: {result['tool_count']}\")\n",
        "print(f\"\\nAgent response: {result['response'][:200]}...\\n\")\n",
        "\n",
        "# For a general question, agent shouldn't need tools\n",
        "assert result['tool_count'] == 0, f\"❌ General question shouldn't require tools, but {result['tool_count']} tools were called\"\n",
        "\n",
        "# Should still provide a response\n",
        "assert len(result['response']) > 0, \"❌ Agent should provide a response\"\n",
        "\n",
        "print(\"✅ TEST PASSED: Agent handled general question without tools\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vKZLTyiy-Bu"
      },
      "source": [
        "### Test 10: Invalid ticket ID\n",
        "\n",
        "\n",
        "Test 10 will check how the agent handles requests for data that doesn't exist. The agent should still attempt the lookup (because the request is valid), but then handle the error response gracefully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5biAjO--y-Bu"
      },
      "outputs": [],
      "source": [
        "# Test: Agent attempts to look up non-existent ticket\n",
        "# The tool will return an error, agent should handle it\n",
        "user_message = \"Check ticket 99999999\"\n",
        "result = await run_agent_and_get_tools(user_message, session_id=\"test_10\")\n",
        "\n",
        "print(f\"User: {user_message}\")\n",
        "print(f\"\\nTools called: {[tc['name'] for tc in result['tool_calls']]}\")\n",
        "print(f\"\\nAgent response: {result['response'][:200]}...\\n\")\n",
        "\n",
        "# Agent should still try to look up the ticket\n",
        "tool_names = [tc['name'] for tc in result['tool_calls']]\n",
        "assert 'lookup_ticket' in tool_names, \"❌ Agent should attempt ticket lookup\"\n",
        "\n",
        "# The final response should indicate the ticket wasn't found\n",
        "response_lower = result['response'].lower()\n",
        "assert ('not found' in response_lower or \"isn't found\" in response_lower or\n",
        "        \"doesn't exist\" in response_lower or 'invalid' in response_lower or\n",
        "        'error' in response_lower), \\\n",
        "       f\"❌ Response should indicate ticket not found\"\n",
        "\n",
        "print(\"✅ TEST PASSED: Agent handled invalid ticket ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3Vnp3W0y-Bu"
      },
      "source": [
        "### Test 11: Noisy input\n",
        "\n",
        "  Real users don't speak like documentation examples. They add filler words,\n",
        "  pleasantries, typos, and extra context such as:\n",
        "\n",
        "  - \"umm could u maybe check ticket 5678 thx\"\n",
        "  - \"Hey there! So I was wondering if you could help me with ticket number\n",
        "  5678? That would be great!\"\n",
        "  - \"ticket 5678 please asap!!!\"\n",
        "  \n",
        "  \n",
        "This test verifies the agent can extract key information from messy, conversational input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9zoU2luy-Bu"
      },
      "outputs": [],
      "source": [
        "# Test: Agent extracts ticket ID from messy/noisy user input\n",
        "user_message = \"Hey, so like, I was wondering, could you maybe check ticket 5678 for me? Thanks!\"\n",
        "result = await run_agent_and_get_tools(user_message, session_id=\"test_11\")\n",
        "\n",
        "print(f\"User: {user_message}\")\n",
        "print(f\"\\nTools called: {[tc['name'] for tc in result['tool_calls']]}\")\n",
        "\n",
        "# Agent should extract the ticket ID despite the noise\n",
        "ticket_calls = [tc for tc in result['tool_calls'] if tc['name'] == 'lookup_ticket']\n",
        "assert len(ticket_calls) > 0, \"❌ Agent should extract ticket ID from noisy input\"\n",
        "\n",
        "ticket_id = ticket_calls[0]['parameters'].get('ticket_id')\n",
        "print(f\"Extracted ticket_id: {ticket_id}\\n\")\n",
        "\n",
        "assert ticket_id == \"5678\", f\"❌ Expected ticket '5678', but got: {ticket_id}\"\n",
        "\n",
        "print(\"✅ TEST PASSED: Agent extracted info from noisy input\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UalQj4F6y-Bu"
      },
      "source": [
        "### Edge Case Testing Strategy\n",
        "\n",
        "  When testing edge cases, think about what could go wrong in production. Ask\n",
        "  yourself: \"How might users actually use this?\"\n",
        "\n",
        "**Categories of edge cases to test:**\n",
        "\n",
        "  1. **❌ Invalid inputs** - What if data is malformed or doesn't exist?\n",
        "     - Non-existent ticket IDs\n",
        "     - Misspelled service names\n",
        "     - Ticket IDs in wrong format (e.g., \"#5678\" vs \"5678\")\n",
        "\n",
        "  2. **❓ Missing information** - What if user doesn't provide required\n",
        "  details?\n",
        "     - \"Check my ticket\" (which ticket?)\n",
        "     - \"Is it working?\" (what service?)\n",
        "     - \"Find me some help\" (with what?)\n",
        "\n",
        "  3. **🔀 Ambiguity** - What if request could mean multiple things?\n",
        "     - \"Check the email\" (ticket about email? or email service status?)\n",
        "     - \"Database issue\" (lookup ticket? check status? search KB?)\n",
        "     - \"The printer\" (which printer? printer service?)\n",
        "\n",
        "  4. **⚠️ Error conditions** - What if tools fail or return errors?\n",
        "     - Tool times out\n",
        "     - API returns 500 error\n",
        "     - Network connection lost\n",
        "\n",
        "  5. **:❗Boundary conditions** - What about extreme values?\n",
        "     - Very long ticket IDs\n",
        "     - Special characters in queries\n",
        "     - Empty input strings\n",
        "     - Extremely long user messages\n",
        "\n",
        "  6. **💬 Noisy or conversational input** - Real users aren't robots!\n",
        "     - Extra filler words (\"um\", \"like\", \"maybe\")\n",
        "     - Politeness phrases (\"please\", \"thank you\", \"could you\")\n",
        "     - Typos and grammatical errors\n",
        "     - Multiple languages mixed together\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRUMpAbLy-Bu"
      },
      "source": [
        "## 8. Exercises 🎓\n",
        "\n",
        "Now it's your turn! Apply what you've learned about testing agents.\n",
        "\n",
        "### Exercise 1: Test Different Ticket IDs\n",
        "\n",
        "Write tests for ticket IDs: 1234, 9999. Verify the agent:\n",
        "1. Calls lookup_ticket\n",
        "2. Extracts the correct ticket ID\n",
        "3. Provides appropriate response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvWnTxBhy-Bu"
      },
      "outputs": [],
      "source": [
        "# Exercise 1: Your code here\n",
        "# Test ticket 1234\n",
        "\n",
        "# Test ticket 9999\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ToqaJpcy-Bv"
      },
      "source": [
        "### Exercise 2: Test Tool Disambiguation\n",
        "\n",
        "Write a test where the user says \"Check the database\". The agent could:\n",
        "- Call `lookup_ticket` if they think it's a ticket\n",
        "- Call `check_system_status` if they think it's the service\n",
        "\n",
        "Which one should the agent choose? Test your hypothesis!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whF20Tpcy-Bv"
      },
      "outputs": [],
      "source": [
        "# Exercise 2: Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvttXq7Oy-Bv"
      },
      "source": [
        "### Exercise 3: Test Multiple Tickets\n",
        "\n",
        "Write a test where the user says \"Compare tickets 5678 and 1234\".\n",
        "\n",
        "**Assert:**\n",
        "1. Agent looks up both tickets\n",
        "2. Both ticket IDs are correctly extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ACpq6jby-Bv"
      },
      "outputs": [],
      "source": [
        "# Exercise 3: Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr3VySjKy-Bv"
      },
      "source": [
        "### Exercise 4: Create a Failing Test\n",
        "\n",
        "Write a test that you EXPECT to fail. Then explain:\n",
        "1. Why it fails\n",
        "2. Is it a bug in the agent or a test problem?\n",
        "3. How would you fix it?\n",
        "\n",
        "**Example failing scenarios:**\n",
        "- Agent calls wrong tool\n",
        "- Agent extracts wrong parameter\n",
        "- Agent doesn't handle edge case\n",
        "- Test is too strict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz-5NC9Cy-Bv"
      },
      "outputs": [],
      "source": [
        "# Exercise 4: Your failing test here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmGpgETzy-Bv"
      },
      "source": [
        "## 9. Best Practices for Agent Testing\n",
        "\n",
        "### ✅ DO:\n",
        "\n",
        "1. **Test behavior, not text** - Focus on tool calls and parameters\n",
        "2. **Test tool selection first** - Ensure agent picks the right tool\n",
        "3. **Test parameter accuracy** - Verify extracted information is correct\n",
        "4. **Test tool call sequences** - Multi-step reasoning matters\n",
        "5. **Test edge cases** - Invalid input, missing data, ambiguity\n",
        "6. **Use mock data** - Fast tests with fake/mock tool responses\n",
        "7. **Test happy path AND failures** - Both success and error cases\n",
        "8. **Write descriptive test names** - Make it clear what you're testing\n",
        "\n",
        "### ❌ DON'T:\n",
        "\n",
        "1. **Don't test only final text** - Tool calls are more important\n",
        "2. **Don't expect exact tool sequences** - Some variation is OK\n",
        "3. **Don't skip edge cases** - That's where bugs hide\n",
        "4. **Don't use real external services** - Slow and unreliable\n",
        "5. **Don't test too many things in one test** - Keep tests focused\n",
        "\n",
        "### Testing Hierarchy\n",
        "\n",
        "**Priority 1: Critical Functionality**\n",
        "- Does agent call the right tool?\n",
        "- Does agent extract correct parameters?\n",
        "\n",
        "**Priority 2: Complex Behavior**\n",
        "- Multi-step reasoning\n",
        "- Tool call ordering\n",
        "\n",
        "**Priority 3: Edge Cases**\n",
        "- Invalid inputs\n",
        "- Error handling\n",
        "- Ambiguous requests\n",
        "\n",
        "**Priority 4: Output Quality**\n",
        "- Response helpfulness\n",
        "- Text clarity\n",
        "- (This is covered in the upcoming notebook: LLM-as-judge)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "inThj_yuVFhU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}